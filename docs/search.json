[
  {
    "objectID": "how-to-contribute.html",
    "href": "how-to-contribute.html",
    "title": "How to Contribute",
    "section": "",
    "text": "We hope R Works will attract other voices from around the R Community, including yourself! We’re always looking for high-quality, original work to demonstrate the power of R. If you would like to contribute R-related topics, news, commentary, or examples, please follow this process to submit a post for consideration."
  },
  {
    "objectID": "how-to-contribute.html#faqs",
    "href": "how-to-contribute.html#faqs",
    "title": "How to Contribute",
    "section": "FAQs",
    "text": "FAQs\n\nCan I cross-post my blog post?\n\nYes, you can cross-post your blog post as long as it is published at the same time as the one on R Works.\n\nDo you edit posts from the past?\n\nOnce published, we only edit posts from the past for typos or broken links."
  },
  {
    "objectID": "how-to-contribute.html#code-of-conduct",
    "href": "how-to-contribute.html#code-of-conduct",
    "title": "How to Contribute",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nR Works is released with a Contributor Code of Conduct. By contributing to this project, you agree to abide by its terms."
  },
  {
    "objectID": "how-to-contribute.html#license",
    "href": "how-to-contribute.html#license",
    "title": "How to Contribute",
    "section": "License",
    "text": "License\nUnless otherwise noted, content on R Works is licensed under the CC-BY license."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R Works",
    "section": "",
    "text": "Bachet’s Four Weights Problem\n\n\n\n\n\n\nPuzzle Corner\n\n\n\nLearn how to solve Bachet’s Four Weights Problem using R, with code and explanations to measure weights from 1 to 40 efficiently.\n\n\n\n\n\n2024-12-06\n\n\nNina Zumel\n\n\n\n\n\n\n\n\n\n\n\n\nExamining Meta-Analysis\n\n\n\n\n\nIn this post we would like to review the idea of meta-analysis and compare a traditional, frequentist style, random effects meta-analysis to Bayesian methods.\n\n\n\n\n\n2024-11-26\n\n\nJohn Mount, Joseph Rickert\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 2024: Top 40 New CRAN Packages\n\n\n\n\n\n\nTop 40\n\n\n\nOne hundred eighty-one new packages made CRAN’s final cut in October.\n\n\n\n\n\n2024-11-25\n\n\nJoseph Rickert\n\n\n\n\n\n\n\n\n\n\n\n\n100 Bushels of Corn, Revisited\n\n\n\n\n\n\nPuzzle Corner\n\n\n\nWe find more solutions to the 100 Bushels of Corn puzzle using the numbers R package.\n\n\n\n\n\n2024-11-22\n\n\nJohn Mount, Nina Zumel\n\n\n\n\n\n\n\n\n\n\n\n\n100 Bushels of Corn\n\n\n\n\n\n\nPuzzle Corner\n\n\n\n100 bushes of corn are distributed to 100 people such that every man receives 3 bushels, every woman 2 bushels, and every child 1/2 a bushel. How many men, women, and children are there? (Solved with R).\n\n\n\n\n\n2024-11-15\n\n\nNina Zumel\n\n\n\n\n\n\n\n\n\n\n\n\nManifold Learning\n\n\n\n\n\nManifold Learning reduces data dimensions to discover patterns for analysis and visualization. This post provides an overview of Manifold Learning and its algorithms, the tsne package, and other R tools and resources.\n\n\n\n\n\n2024-11-11\n\n\nJoseph Rickert\n\n\n\n\n\n\n\n\n\n\n\n\nR Work Opportunities\n\n\n\n\n\n\nR Community\n\n\n\nExplore new job opportunities that highlight R skills.\n\n\n\n\n\n2024-11-10\n\n\nIsabella Velásquez\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 2024: Top 40 New CRAN Packages\n\n\n\n\n\n\nTop 40\n\n\n\nTwo hundred thirty new packages made it to CRAN in September. Here are my “Top 40” selections in 17 categories.\n\n\n\n\n\n2024-10-31\n\n\nJoseph Rickert\n\n\n\n\n\n\n\n\n\n\n\n\nAugust 2024: Top 40 New CRAN Packages\n\n\n\n\n\n\nTop 40\n\n\n\n“Top 40” is back, broadcasting on the new R Works blog.\n\n\n\n\n\n2024-10-30\n\n\nJoseph Rickert\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to R Works!\n\n\n\n\n\n\nR Community\n\n\n\nWe hope that the R Works blog informs and inspires R users everywhere.\n\n\n\n\n\n2024-10-30\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/manifold-learning/index.html",
    "href": "posts/manifold-learning/index.html",
    "title": "Manifold Learning",
    "section": "",
    "text": "Many algorithms supporting AI and machine learning depend on the notion of embeddings. Data sets are mapped to or embedded in high-dimensional Euclidean vector spaces. Then, various mathematical strategies are employed to reduce data size by mapping these high dimensional points to structures in lower dimensional spaces in ways that preserve some important structural properties of the high dimensional data. Classic examples are the Word2Vec algorithm which maps similar words to nearby points in vector spaces, and Principal Components Analysis which maps multidimensional vector data to lower dimensional spaces while preserving the variance of the data points. The mathematical term for the structures sought to contain the data in the lower dimensional space is manifold. Manifolds are basically vector spaces with additional structure that enable notions such as connectedness and smoothness to make sense. Think of a sphere as a two-dimensional manifold in a three-dimensional space, or lines and circles as one-dimensional structures in a two-dimensional space.\nOver the past fifteen years or so, these kinds of geometric ideas about working with data have coalesced into the very mathematical field of Manifold Learning. In this post, I hope to provide a way for those of us who are not mathematicians, but willing to do some work to explore this incredibly interesting field. I’ll do this by pointing to some of the accessible literature, providing a couple of simple examples, and listing some R resources for exploration."
  },
  {
    "objectID": "posts/manifold-learning/index.html#an-overview-of-manifold-learning",
    "href": "posts/manifold-learning/index.html#an-overview-of-manifold-learning",
    "title": "Manifold Learning",
    "section": "An overview of Manifold Learning",
    "text": "An overview of Manifold Learning\nThis section comprises some notes on the marvelous review paper Manifold Learning: What, How, and Why by Marina Meilă and Hanyu Zhang. The paper is a comprehensive, clearly written, historical approach at a level suitable for beginners. It is an expert guide to the vast literature on the subject. The Annual Reviews version of the paper at the link above is a pleasure to work with because almost all of the essential papers are hyperlinked.\n\n\n\nThe Basic Problem\nThe basic problem motivating manifold learning is data reduction. Given a data set with D features or explanatory variables, how can we transform it into a smaller data set with fewer features in a way that retains all of the essential information and provides some insight about the structure of the data? The idea is similar to PCA. Here, we assume the data exist in a D-dimensional vector space but mostly lie in or near a k-dimensional subspace. PCA provides a linear mapping from \\(R^D\\) to \\(R^k\\).\n\n\nThe Manifold Assumption\nThe data are a sample from a probability distribution with support on, or near, a D-dimensional manifold embedded in \\(R^D\\).\n\n\nThree Paradigms for Manifold learning:\nThe term manifold learning was proposed by Roweis & Saul (2000) who proposed the Locally Linear Embedding (LLE) algorithm and Tenenbaum et al. (2000), who introduced the Isomap algorithm. There are three basic approaches to manifold learning: Locally linear approximations, Principal Curves and Surfaces, and Embeddings.\n\n\nLocal Linear Approximations\n\nBased on classical PCA\nPCA performed on a weighted covariance matrix, with weights decaying with distance from any reference point\nApproximates data locally on a curved manifold around a point\nReduces dimension locally but provides no global representation\n\n\n\nPrincipal Curves and Surfaces\n\nData assumed to be of the form \\(x_i = x^*_i + \\epsilon\\)\nThe Subspace Constrained Mean Shift (SCMS) algorithm of Ozertem & Erdogmus (2011) iteratively maps each \\(x_i\\) to \\(y_i \\in R^D\\) lying on the principal curve\nMethod can be extended to principal surfaces\n\n\n\n\n\n\nEmbeddings\nMeilă and Zhang propose a provisional taxonomy of embedding algorithms, which they concede is superficial but which adequately characterizes the state of the art. All approaches begin with information about the data summarized in a weighted neighborhood graph. An embedding algorithm then produces a smooth mapping that is designed to distort the neighborhood information as little as possible. The algorithms differ in their choice of information they preserve and in the constraints on smoothness. The fundamental categories of embedding algorithms are:\n\n“One-shot” algorithms that derive embedding coordinates from principal eigenvectors of a matrix associated with the neighborhood graph of a data set or by solving an optimization problem.\nAttraction-repulsion algorithms that proceed from an initial embedding, often produced by iterative improvements of a one-shot algorithm.\n\n\nOne-Shot Embedding Algorithms\nOne-Shot algorithms include:\n\nDiffusion Maps (DM): Coifman & Lafon (2006) which uses the eigenvectors of the Laplacian matrix to embed the data.\nISOMAP - Tenenbaum et al. (2000)\n\nPreserves shortest graph paths\n\nLaplacian Eigenmaps - Belkin & Niyogi (2003)\nLocal Transient Space Alignment (LTSA) - Zhang & Zha (2004)\n\n\n\nAttraction-Replusion Embedding Algorithms\nAttraction-Repulsion Algorithms include:\n\nLow Distortion Local Embeddings (LDLE) Lohli, Cloninger, and Mishne (2021)\nMaximum Variance Unfolding (MVU) Weinberger and Saul (2006)\nStochastic Neighbor Embedding (SNE) Hinton and Roweis (2002)\nt-SNE Van der Maaten and Hinton (2008)\nUniform Manifold Approximation and Projection (UMAP) MCinnes et al (2018)\n\n\n\n\n\n\nSNE and t-SNE\nThis section briefly describes the SNE algorithm and its improved variation t-SNE, which were designed to visualize high dimensional data to a two or three-dimensional space.\n\nSNE\nThe intuition behind Stochastic Neighbor Embedding (SNE), described in the paper by Hinton & Roweis (2002), is to emphasize local distances and employ a cost function that enforces both keeping the images of nearby objects nearby and keeping the images of widely separated objects relatively far apart.\nMost embedding methods require each high-dimensional data point to be associated with only a single location in the low-dimensional space, making it difficult to unfold “many-to-one” mappings in which a single ambiguous object really belongs in several disparate locations in the low-dimensional space. SNE tries to place high-dimensional data points in a low-dimensional space so as to optimally preserve neighborhood identity and allow multiple different low-dimensional images. For example, because of its probabilistic formulation, SNE has the ability to be extended to mixtures in which ambiguous high-dimensional objects such as the word “bank” can be associated with several widely-separated images (e.g., both “river” and “finance”) in the low-dimensional space.\nThe basic idea underlying SNE is to construct a Gaussian probability distribution \\(P_i\\) over each point, \\(x_i\\), in the high dimensional space that describes the conditional probability \\(p_{j|i}\\) that i would pick j as its neighbor. \\[p_{j|i} =  exp(-\\| x_i - x_j\\|^2/2\\sigma_i^2) \\sum_{k \\neq i} exp(-\\| x_i - x_k\\|^2 / 2\\sigma_i^2)\\]\nThen, find a similar distribution, \\(Q_i\\), over the points in the points \\(y_i\\) in the low dimensional space to which the \\(x_i\\) are mapped. If, for all i, \\(p_{i|j}=q_{i|j}\\), then the similarities will be preserved. The \\({y_i}\\) points are found by using gradient descent to minimize the sum of all the Kullback-Liebler divergences using the cost function: \\[C = \\sum_i KL(P_i \\| Q_i) = \\sum_i \\sum_j p_{j|i}log(p_{j|i}/q_{j|i)}\\]\nIn the high dimensional space, the \\(\\sigma_i\\) values are selected by performing a binary search for the \\(\\sigma_i\\) that produces a \\(P_i\\) with a fixed perplexity specified by the user. \\(Perp(P_i) = 2^{H(P_i)}\\) where \\({H(P_i)} = -\\sum_j p_{j|i}log_2 p_{j|i}\\) is the Shannon entropy measured in bits. The \\(\\sigma_i\\) for the low dimensional space are set to \\(1/\\sqrt2\\).\n\n\nt-SNE\nThe t-SNE algorithm, described in van der Maaten and Hinton (2008), is an improvement on SNE that overcomes several technical difficulties. The main differences from SNE are that (1) t-SNE uses a symmetric version of the cost function, which has a simpler gradient, and (2) t-SNE uses a t-distribution with one degree of freedom for the points in the low dimensional space. These help overcome optimization problems and mitigate the effect of the Crowding Problem in which the area available in the low dimensional map to accommodate moderately distant data points will not be sufficient.\nVan der Maaten and Hinton point out that in both SNE and t-SNE,\n\n“…the gradient may be interpreted as the resultant force created by a set of springs between the map points \\(y_i\\) and \\(y_j\\) . . . The spring between \\(y_i\\) and \\(y_j\\) repels or attracts the map points depending on whether distance between the two in the map is too small or too large to represent the similarities between the two high dimensional points.”\n\nThe final result, t-SNE, is an algorithm that preserves local similarities between points while preserving enough of the global structure to recognize clusters. The art of both these algorithms comprises not only marshaling appropriate mathematics to realize intuitive geometric ideas about data relationships, but also in working through the many technical difficulties and optimization problems to provide reasonable performance."
  },
  {
    "objectID": "posts/manifold-learning/index.html#r-examples",
    "href": "posts/manifold-learning/index.html#r-examples",
    "title": "Manifold Learning",
    "section": "R Examples",
    "text": "R Examples\nThis section provides examples of using the tsne package to compute two and three-dimensional embeddings on two different data sets, the palmerpenguins and the AMES Housing Data. Note because it takes several minutes to fit the models on my underpowered MacBook Air, the code below shows how to run the tsne() command, but actually reads in the model fit from an RDS file.\n\n\n\nExample 1: The Penguins\nFor our first example, let’s look at the penguins data set from the palmerpenguins package. It has six variables we can use to feed the t-SNE algorithm, and we know that we would like any clusters identified to correspond with the three species of penguins.\n\n\nShow the code\ndf_p &lt;- palmerpenguins::penguins\n# glimpse(df_p)\n\n\nPrepare data frames for fitting the model and then for subsequent plotting.\n\n\nShow the code\ndf_p_fit &lt;- df_p |&gt; \n  mutate(island = as.integer(island), sex = as.integer(sex)) |&gt;\n  select(c(-year, -species)) |&gt; \n  na.omit()\n\ndf_p_plot &lt;- df_p |&gt; select(-year) |&gt; na.omit() \n\n\nFit the t-sne model.\n\n\nShow the code\n# fit_pen &lt;- tsne(df_p_fit, epoch_callback = NULL, perplexity=50)\nfit_pen &lt;- readRDS(\"fit_pen_2D\")\n\n\nNext, we add the coordinates fit by the model to our plotting data frame and plot. As we would expect, the clusters identified by t-SNE line up very nicely with the penguin species, and island. All of the Gentoo are on Biscoe Island.\n\n\nShow the code\ndf_p_plot &lt;- df_p_plot |&gt; mutate(x = fit_pen[, 1], y = fit_pen[, 2])\n\ndf_p_plot |&gt; ggplot(aes(x, y, colour = species, shape = island)) +\n  geom_point() +\n  scale_color_manual(values = c(\"blue\", \"red\", \"green\")) +\n  ggtitle(\"2D Embedding of Penguins Data\")\n\n\n\n\n\n\n\n\n\nHere is a projection onto a three-dimensional space.\n\n\nShow the code\n# fit_pen_3D = tsne(df_p_fit, epoch_callback = NULL, perplexity=50, k=3)\nfit_pen_3D &lt;- readRDS(\"fit_pen_3D\")\n\n\nThe threejs visualization emphasizes the single Chinstrap observation floating in space near the Adelle clusters and the two Gentoos reaching the edge of Chinstrap Island.\n\n\nShow the code\nx &lt;- fit_pen_3D[,1] \ny &lt;- fit_pen_3D[,2]\nz &lt;- fit_pen_3D[,3]\n\n\ndf_p_plot &lt;- df_p_plot |&gt; mutate(\n  color = if_else(species == \"Adelie\", \"blue\", \n                  if_else( species == \"Gentoo\",\"green\", \"red\")))\n\nscatterplot3js(x,y,z, color=df_p_plot$color, cex.symbols = .3, \n               labels = df_p_plot$species)\n\n\n\n\n\n\n\n\n\n\n\nExamble 2: The Ames Housing Dataset\nWith 74 variables, the AMES Housing Data provides a convincing display of of the usefulness of the t-sne algorithm.\n\n\nShow the code\ndata(ames, package = \"modeldata\")\n#glimpse(ames)\n\n\nPrepare ames for processing with tsne by changing all factors to numeric data and fit the model.\n\n\nShow the code\ndf_ames &lt;- ames %&gt;% mutate_if(is.factor, as.numeric)\n#fit_ames &lt;- tsne(df_ames, epoch_callback = NULL, perplexity=50)\nfit_ames &lt;- readRDS(\"tsne_fit_ames\")\n#head(fit_ames)\n\ndf_ames_plot &lt;- ames |&gt; mutate(x = fit_ames[,1], y = fit_ames[,2],\n                               MS_Zoning = as.character(MS_Zoning),\n                               MS_Zoning = replace(MS_Zoning, MS_Zoning == \"C_all\", \"C or I\"),\n                               MS_Zoning = replace(MS_Zoning, MS_Zoning == \"I_all\" , \"C or I\")\n                               )\n                              \n                               \n             \n            \ndf_ames_plot |&gt; ggplot(aes(x,y, shape = MS_Zoning, color = Neighborhood)) + \n                geom_point() +\n                guides(color = FALSE, size = \"none\") +\n                ggtitle(\"2D Embedding of Ames Data Colored by Neighborhood\")\n\n\n\n\n\n\n\n\n\nHere is the projection onto a three-dimensional space that is also colored by Neighborhood. It shows the separation among the clusters which appear to reside in a three-dimensional ellipsoid. As was the case with the two-dimensional plot, neighborhoods appear to be fairly well mixed among the clusters.\n\n\nShow the code\nset.seed(1234)\n\n#fit_ames_3D &lt;- tsne(ames, epoch_callback = NULL, perplexity=50,k=3)\nfit_ames_3D &lt;- readRDS(\"tsne_fit_ames_3D\")\n#head(fit_ames_3D)\n\ndf_plot_ames_3D &lt;- df_ames |&gt; mutate(\n  x = fit_ames_3D[, 1],\n  y = fit_ames_3D[, 2],\n  z = fit_ames_3D[, 3],\n  neighborhood = ames$Neighborhood,\n  zone = ames$MS_Zoning\n)\n\nx &lt;- fit_ames_3D[, 1]\ny &lt;- fit_ames_3D[, 2]\nz &lt;- fit_ames_3D[, 3]\n\nscatterplot3js(x, y, z, cex.symbols = .1, col = rainbow(length(df_plot_ames_3D$Neighborhood)))\n\n\n\n\n\n\n\n\n\n\nR Packages\nThe following is a short list of R packages that may be helpful for Manifold Learning.\ncml vo.2.2: Finds a low-dimensional embedding of high-dimensional data, conditioning on available manifold information. The current version supports conditional MDS (based on either conditional SMACOF in Bui (2021) or closed-form solution in Bui (2022) and conditional ISOMAP in Bui (2021).\ndyndimred v1.0.4: Provides a common interface for applying dimensionality reduction methods, such as Principal Component Analysis (‘PCA’), Independent Component Analysis (‘ICA’), diffusion maps, Locally-Linear Embedding (‘LLE’), t-distributed Stochastic Neighbor Embedding (‘t-SNE’), and Uniform Manifold Approximation and Projection (‘UMAP’). Has built-in support for sparse matrices.\nhydra v0.1.0: Calculate an optimal embedding of a set of data points into low-dimensional hyperbolic space. This uses the strain-minimizing hyperbolic embedding of Keller-Ressel and Nargang (2019).\nmatrixLaplacian v1.0: Constructs the normalized Laplacian matrix of a square matrix, returns the eigenvectors (singular vectors) and visualization of normalized Laplacian map.\nphateR v1.0.7: Implements a novel conceptual framework for learning and visualizing the manifold inherent to biological systems in which smooth transitions mark the progressions of cells from one state to another.\nRiemann v0.1.4: provides a variety of algorithms for manifold-valued data, including Fréchet summaries, hypothesis testing, clustering, visualization, and other learning tasks. See Bhattacharya and Bhattacharya (2012) for general exposition of statistics on manifolds. See the vignette.\nRtsne v:0.17: Provides a R wrapper around the fast T-distributed Stochastic Neighbor Embedding implementation by Van der Maaten.\nspectralGraphTopology v0.2.3: Learning Graphs from Data via Spectral Constraints, It provides implementations of state-of-the-art algorithms such as Combinatorial Graph Laplacian Learning (CGL), Spectral Graph Learning (SGL), Graph Estimation based on Majorization-Minimization (GLE-MM), and Graph Estimation based on Alternating Direction Method of Multipliers (GLE-ADMM). See the vignette.\ntsne v0.1-3.1: A “pure R” implementation of the t-SNE algorithm.\numap v0.2.10.0: Implements the Uniform manifold approximation and projection algorithm for dimension reduction, which was described by McInnes and Healy (2018). See the vignette.\nuwot v0.2.2: An implementation of the Uniform Manifold Approximation and Projection dimensionality reduction by McInnes et al. (2018). It also provides means to transform new data and to carry out supervised dimensionality reduction. An implementation of the related LargeVis method of Tang et al. (2016) is also provided. See the uwot website(https://github.com/jlmelville/uwot&gt;) for more documentation and examples."
  },
  {
    "objectID": "posts/august-2024-top-40-new-cran-packages/index.html",
    "href": "posts/august-2024-top-40-new-cran-packages/index.html",
    "title": "August 2024: Top 40 New CRAN Packages",
    "section": "",
    "text": "“Top 40” is back, broadcasting on the new R Works blog. I hope to continue the monthly evaluation of R packages that ran for several years on RStudio’s R Views Blog. The following is an idiosyncratic selection of the forty best new R packages submitted to CRAN in August 2024 organized into fourteen categories: Artificial Intelligence, Computational Methods, Data, Ecology, Environment, Genomics, Machine Learning, Medicine, Pharma, Science, Statistics, Time Series, Utilities, and Visualization."
  },
  {
    "objectID": "posts/august-2024-top-40-new-cran-packages/index.html#comments",
    "href": "posts/august-2024-top-40-new-cran-packages/index.html#comments",
    "title": "August 2024: Top 40 New CRAN Packages",
    "section": "Comments",
    "text": "Comments"
  },
  {
    "objectID": "posts/september-2024-top-40-new-cran-packages/index.html",
    "href": "posts/september-2024-top-40-new-cran-packages/index.html",
    "title": "September 2024: Top 40 New CRAN Packages",
    "section": "",
    "text": "Two hundred thirty new packages made it to CRAN in September, many of them were interesting, and selecting only forty made for some difficult decisions. When there a difficult choice, I opted in favor of the sciences. Here are my picks for the Top 40 new packages in fifteen categories: AI, Archaeology, Biology, Computational Methods, Data, Genomics, Linguistics, Machine Learning, Medicine, Networks, Pharma, Physics, Statistics, Utilities, and Visualization.\n\nAI\ngroqR v0.0.1: Provides a suite of functions and RStudio Add-ins leveraging the capabilities of open-source Large Language Models (LLMs) to support R developers. Features include text rewriting, translation, and general query capabilities. Programming-focused functions provide assistance with debugging, translating, commenting, documenting, and unit testing code, as well as suggesting variable and function names. Look here for examples.\n\n\nArchaeology\neratosthenes v0.0.2: Estimates unknown historical or archaeological dates subject to relationships with other dates and absolute constraints, derived as marginal densities from the full joint conditional distribution. Includes rule-based estimation of the production dates of artifact types. See Collins-Elliott (2024) for background and the vignettes on aligning relative sequences and gibbs sampling for archaeology dates.\n\n\nBiology\npcvr v1.0.0: Provides functions to analyse common types of plant phenotyping data and a simplified interface to longitudinal growth modeling and select Bayesian statistics. See Kruschke (2018), Kruschke (2013) and Kruschke (2021) for background on the Bayesian methods. There are four vignettes including Bellwether workflow and Longitudinal Growth Modeling.\n\n\n\n\n\northGS v0.1.5: Provides tools to analyze and infer orthology and paralogy relationships between glutamine synthetase proteins in seed plants. See the vignettes Searching for Orthologous and Unraveling the Hiden Paralogous.\n\n\n\n\n\n\n\nChemistry\nMethodOpt v1.0.0: Implements a GUI to apply an advanced method optimization algorithm to various sampling and analysis instruments. Functions include generating experimental designs, uploading and viewing data, and performing various analyses to determine the optimal method. See Granger & Mannion (2024) for details and the vignette for examples.\nSCFMonitor v0.3.5: Self-Consistent Field(SCF) calculation method is one of the most important steps in the calculation methods of quantum chemistry. See Ehrenreich & Cohen (1959) . This package enables Gaussian quantum chemistry calculation software users to easily read the Gaussian .log files and monitor the SCF convergence and geometry optimization process. Look here for examples.\n\n\n\n\n\n\n\nComputational Methods\nXDNUTS v1.2: Implements Hamiltonian Monte Carlo for both continuous and discontinuous posterior distributions with customisable trajectory length termination criterion. See Nishimura et al. (2020) for the original Discontinuous Hamiltonian Monte Carlo; and Hoffman et al. (2014) and Betancourt (2016) possible Hamiltonian Monte Carlo termination criteria. The vignette offers examples.\n\n\n\n\n\n\n\nData\nclintrialx v0.1.0: Provides functions to fetch clinical trial data from sources like [ClinicalTrials.gov](https://clinicaltrials.gov/} and the Clinical Trials Access to Aggregate Content database that supports pagination and bulk downloads. See the vignette.\n\n\n\n\n\nColOpenData v0.3.0: Provides tools to download and wrangle Colombian socioeconomic, geospatial,population and climate data from DANE at the National Administrative Department of Statistics and IDEAM at the Institute of Hydrology, Meteorology and Environmental Studies. It solves the problem of Colombian data being issued in different web pages and sources by using functions that allow the user to select the desired database and download it without having to do the exhausting acquisition process. There are six vignettes including How to download climate data and Population Projections.\n\n\n\n\n\nmodgo v1.0.1: Provides functions to generate synthetic data from a real dataset using the combination of rank normal inverse transformation with the calculation of correlation matrix and completely artificial data may be generated through the use of Generalized Lambda Distribution and Generalized Poisson Distribution. See the vignette.\n\n\n\n\n\ndtmapi v0.0.2: Provides functions to allow humanitarian community, academia, media, government, and non-governmental organizations to utilize the data collected by the Displacement Tracking Matrix, a unit in the International Organization for Migration. See the vignette to get started.\n\n\nEcology\ndouconca v1.2.1: Implements the two step double constrained correspondence analysis (dc-CA) for analyzing multi-trait multi-environment ecological data described inter Braak et al. (2018). This algorithm combines and extends community or sample and species-level analyses.\n\n\n\n\n\nGeoThinneR v1.1.0: Provides efficient geospatial thinning algorithms to reduce the density of coordinate data while maintaining spatial relationships. Implements K-D Tree and brute-force distance-based thinning, as well as grid-based and precision-based thinning methods. See Elseberg et al. (2012) for background and the vignette for examples.\n\n\n\n\n\n\n\nGenomics\neasybio v1.1.0: Provides a toolkit for single-cell annotation with the CellMarker2.0 database and streamlines biological label assignment in single-cell RNA-seq data and facilitates transcriptomic analysis, including preparation of TCGA and GEO datasets, differential expression analysis and visualization of enrichment analysis results. See Wei Cui (2024) for details and the two vignettes bulk RNAsewuence workflow and Single Cell Annotation for examples.\n\n\n\n\n\nGenoPop v0.9.3: Implements tools for efficient processing of large, whole genome genotype data sets in variant call format including several functions to calculate commonly used population genomic metrics and a method for reference panel free genotype imputation. See Gurke & Mayer (2024) for background and the vignette to get started. \nSuperCell v1.0: Provides tools to aggregate large single-cell data into metacell dataset by merging together gene expression of very similar cells See the vignettes Example of SuperCell pipeline and SuperCell runs for different samples.\n\n\n\n\n\n\n\nLingusitics\nmaxent.ot v1.0.0: Provides tools to fit Maximum Entropy models to phonology data. See Mayer, Tan & Zuraw and the vignette for an overview.\n\n\nMachine Learning\nconversim v0.1.0: Provides tools to analyze and compare conversations using various similarity measures including topic, lexical, semantic, structural, stylistic, sentiment, participant, and timing similarities. Methods are based on established research: For example see Landauer et al. (1998) Jaccard (1912) and Salton & Buckley (1988). Thee are four vignettes including analyzing similarities between two long speaches and analyzing similarities in conversational sequence in one Dyad and across multiple Dyads.\ndsld v0.2.2: Provides statistical and graphical tools for detecting and measuring discrimination and bias, be it racial, gender, age or other. Detection and remediation of bias in machine learning algorithms. See the Quick Start Guide.\n\n\nMedicine\nSurvMA v1.6.8: Implements a model averaging approach to predict personalized survival probabilities by using a weighted average of multiple candidate models to approximate the conditional survival function.Two scenarios of candidate models are allowed: the partial linear Cox model and the time-varying coefficient Cox model. See Wang (2023) for details and look here for an example.\n\n\n\n\n\nwintime v0.2.0: Provides methods to perform an analysis of time-to-event clinical trial data using various methods that calculate and compare treatment effects on ordered composite endpoints. See Troendle et al. (2024) for the details of the methods and the vignette for examples.\n\n\nNetworks\narlclustering v1.0.5: Implements an innovative approach to community detection in social networks using Association Rules Learning providing tools for processing graph and rules objects, generating association rules, and detecting communities based on node interactions. See El-Moussaoui et al. (2021) for details. There are eight vignettes including General Introduction and Testing WordAdjacency dataset.\nggtangle v0.0.2: Extends the ggplot2 plotting system to support network visualization for network associated data. See the vignette.\n\n\n\n\n\n\n\nPharma\nsdtm.oak v0.1.0: Provides a framework to develop CDISC, SDTM datasets in R and potentially automate the process. There are six vignettes including on on Algorithms.\n\n\n\n\n\n\n\nPhysics\nrice v0.3.0: Provides functions to calibrate radiocarbon dates, different radiocarbon realms (C14 age, F14C, pMC, D14C) and to estimate the effects of contamination or local reservoir offsets. See Reimer and Reimer 2001 and Stuiver and Polach (1977) for background and the vignette for examples.\n\n\n\n\n\nSTICr v1.0: Comprises a collection of functions for processing raw data from Stream Temperature, Intermittency, and Conductivity (STIC) loggers. ‘STICr’ (pronounced “sticker”) that includes functions for tidying, calibrating, classifying, and doing quality checks on data from STIC sensors. See Wheeler/Zipper et al. (2023) for background and the vignette for an Introduction.\n\n\n\n\n\n\n\nStatistics\ndpasurv v0.1.0: Provides functions to implement dynamic path analysis for survival data via Aalen’s additive hazards model. See Fosen et al., (2006) for details. There is an oveview and a vignette on plotting with ggplot2.\n\n\n\n\n\nLearnVizLLM v1.0.0: Implements tools to summarize the characteristics of linear fixed models without data or a fitted model by converting code code for fitting nlme::lme() and lme4::lmer() models into tables, equations, and visuals. See the vignette for details.\n\n\n\n\n\nlnmixsurv v3.1.6: Combines the mixture distributions of Fruhwirth-Schnatter(2006) and the data augmentation techniques of Tanner and Wong (1987) to implement Bayesian Survival models that accommodate different behavior over time and consider higher censored survival times. There are five vignettes including a [Get started guide}\nPath.Analysis v0.1: Provides functions for conducting sequential path coefficient analysis and testing direct effects and functions for estimating correlation, drawing correlograms, heatmaps, and path diagrams. See Arminian et al. (2008) for background and the vignette for examples.\n\n\n\n\n\n\n\nUtilities\ncharcuterie v0.0.4: Creates a new chars class which looks like a string but is actually a vector of individual characters, making strings iterable and enabling vector operations on ‘strings’ such as reverse, sort, head, and set operations. See the vignettes Example Usage and Use Cases.\n\n\n\n\n\ndtreg v1.0.0: Provides tools to interact with data type registries and create machine-readable data. See the vinette.\nfctutils v0.0.7: Provides a collection of utility functions for manipulating and analyzing factor vectors in R. It offers tools for filtering, splitting, combining, and reordering factor levels based on various criteria. See the vignette.\ninterface v0.1.2: Provides a run time type system, allowing users to define and implement interfaces, enums, typed data.frame/data.table, as well as typed functions. This package enables stricter type checking and validation, improving code structure, robustness and reliability. There is a vignette and a way to support the author.\npikchrV0.97 : Provides an interface to pikchr q markup language for creating diagrams within technical documentation. See the vignette for examples.\n\n\n\n\n\nrnix v0.12.4: Provides tools to run the nix package manager. There are fifteen vignettes including a Getting Started Gude.\nqs2 v0.1.1: Provides tools to efficiently serialize R objects using one of two compression formats: the qs2 format, which uses R serialization while optimizing compression and disk I/O, and the qdata format which uses custom serialization to achieve slightly faster performance and better compression. qs2 format can be directly converted to the standard RDS. See the vignette\n\n\nVisualization\nggalign v0.0.4: Implements an extension to ggplot2 that offers various tools for organizing and arranging plots including the ability to consistently align a specific axis across multiple ggplot objects. There are seven vignettes including Examples and Heatmap Layout.\n\n\n\n\n\nsfcurv v1.0: Implements all possible forms of 2x2 and 3x3 space-filling curves, i.e., the generalized forms of the Hilbert curve, the Peano curve and the Peano curve in the meander type. Look here for examples.\n\n\n\n\n\nsurreal v0.0.1: Implements the Residual (Sur)Realism algorithm described by Stefanski (2007) to generate datasets that reveal hidden images or messages in their residual plots. See README for examples.\n\n\n\n\n\nsurvSAKK v1.3.1: Provides functions to incorporate various statistics and layout customization options to enhance the efficiency and adaptability of the Kaplan-Meier plots. See the vignette."
  },
  {
    "objectID": "posts/four-weights/index.html",
    "href": "posts/four-weights/index.html",
    "title": "Bachet’s Four Weights Problem",
    "section": "",
    "text": "Here’s another puzzle from Henry Dudeney’s article “The World’s Best Puzzles,” The Strand Magazine, December 1908. According to Dudeney, this puzzle is originally from Problèmes plaisans et délectables qui se font par les nombres (Pleasant and delectable number problems), by French mathematician Claude Gaspar Bachet de Méziriac (1851-1636).1\nThis seems like a variation on the Frobenius coin problem, which, in general, is NP-hard. Fortunately, this specific instance is not.\nAs before, here’s Chirico’s The Mathematicians for you to look at while you try to solve this. Solution below."
  },
  {
    "objectID": "posts/four-weights/index.html#the-solution",
    "href": "posts/four-weights/index.html#the-solution",
    "title": "Bachet’s Four Weights Problem",
    "section": "The Solution",
    "text": "The Solution\nThe four weights are (1, 3, 9, 27). Before we confirm this computationally, let’s compute the solution using induction.\nLet’s rephrase the problem as\n\nYou have \\(n\\) weights… such that you can weigh any number of pounds in the range 1:m….”\n\nwhere \\(m\\) depends on how many weights you have. We can also observe that the sum of all the weights must equal the weight of the heaviest object you can measure, and that object must weigh \\(m\\) pounds.\nThen, for the case \\(n = 1\\), you have a single weight \\(w_1 = 1\\), and you can weigh any object that weighs one pound (the interval 1:1). Now let’s look at the case \\(n=2\\).\n\n1. The weights \\((w_1 = 1, w_2 = 3)\\) can weigh any object from 1 to 4 pounds.\nI’ll use \\(x\\) to denote the object to be weighed, and the notation \\([\\{set_1\\} | \\{set_2\\}]\\) to denote what’s on the left and the right side of the scale. I’ll always put \\(x\\) in the right-hand pan.\n\\(x = 1\\) is weighed as \\([\\{1\\} | \\{x\\}]\\). This can be written as \\(1 = x\\).\n\\(x = 2\\) is weighed as \\([\\{3\\} | \\{x, 1\\}]\\). This can be written as \\(3 = x + 1\\), or \\(3 - 1 = x\\).\n\\(x = 3\\) is weighed as \\([\\{3\\} | \\{x\\}]\\). This can be written as \\(3 = x\\).\n\\(x = 4\\) is weighed as \\([\\{1, 3\\} | \\{x\\}]\\). This can be written as \\(1 + 3 = x\\).\nYou can see from the above that the general form of the solution is\n\\[\ns_1 w_1 + s_2 w_2 = x\n\\]\nwhere \\((w_1, w_2) = (1, 3)\\) and \\(s_1 \\in \\{-1, 0, 1\\}\\). A positive coefficient means the weight is in the left pan, a negative one means it’s in the right pan, and zero means the weight isn’t used. This is essentially a “signed trinary” representation of \\(x\\). For two digits, signed trinary can represent \\(3^2 = 9\\) values.\nLet’s just see what that looks like in R:\n\nlibrary(poorman)\n\nsigns = c(-1, 0, 1)\n\nS = expand.grid (s1 = signs, s2 = signs)\n\nknitr::kable(S)\n\n\n\n\ns1\ns2\n\n\n\n\n-1\n-1\n\n\n0\n-1\n\n\n1\n-1\n\n\n-1\n0\n\n\n0\n0\n\n\n1\n0\n\n\n-1\n1\n\n\n0\n1\n\n\n1\n1\n\n\n\n\n\nLet’s take the linear combination of s1 and s2, using the weights (1, 3).\n\nS = S |&gt;\n  mutate(x = 1 * s1 + 3 * s2)\n\nknitr::kable(S)\n\n\n\n\ns1\ns2\nx\n\n\n\n\n-1\n-1\n-4\n\n\n0\n-1\n-3\n\n\n1\n-1\n-2\n\n\n-1\n0\n-1\n\n\n0\n0\n0\n\n\n1\n0\n1\n\n\n-1\n1\n2\n\n\n0\n1\n3\n\n\n1\n1\n4\n\n\n\n\n\nYou can confirm for yourself that using another pair of weights won’t necessarily give you 9 unique values.\nWe are only interested in the positive values for our problem. We can calculate that the number of positive values is\n\\[\n(3^2 - 1)/2 = (9 - 1)/2 = 4.\n\\]\nIn other words, the weights (1, 3) can weigh the values 1:4.\n\n\n2. The case of three weights\nHow many values can we weigh with three weights, and what are they? It’s clear that \\(w_1\\) and \\(w_2\\) must be the same as above, since any value in the range 1:4 can be represented as \\(s_1 * 1 + s_2 * 3 + 0 * w_3\\). Now to find \\(w_3\\).\nWe know that \\(3^3 = 27\\), which means that we can represent \\((27 - 1)/2 = 13\\) positive values, with 13 being the maximum. So the three weights must sum to 13, which gives us \\(w_3 = 13 - (1 + 3) = 9\\). The set of weights is (1, 3, 9).\n\n\n3. Finally, the case of four weights\nWe can calculate that \\(3^4 = 81\\), which corresponds to \\((81 - 1)/2 = 40\\) positive values (surprise!). Therefore, we have that \\(w_4 = 40 - (1 + 3 + 9) = 27\\). And so the set of weights we want is (1, 3, 9, 27), as stated above.\nNow, let’s confirm it.\n\nS = expand.grid (s1 = signs,\n                 s2 = signs,\n                 s3 = signs,\n                 s4 = signs) |&gt;\n  mutate(x = 1 * s1 + 3 * s2 + 9 * s3 + 27 * s4) |&gt;\n  filter(x &gt; 0)\n\n# confirm we have the values 1:40\nstopifnot(S$x == 1:40)\n\nknitr::kable(S)\n\n\n\n\n\ns1\ns2\ns3\ns4\nx\n\n\n\n\n42\n1\n0\n0\n0\n1\n\n\n43\n-1\n1\n0\n0\n2\n\n\n44\n0\n1\n0\n0\n3\n\n\n45\n1\n1\n0\n0\n4\n\n\n46\n-1\n-1\n1\n0\n5\n\n\n47\n0\n-1\n1\n0\n6\n\n\n48\n1\n-1\n1\n0\n7\n\n\n49\n-1\n0\n1\n0\n8\n\n\n50\n0\n0\n1\n0\n9\n\n\n51\n1\n0\n1\n0\n10\n\n\n52\n-1\n1\n1\n0\n11\n\n\n53\n0\n1\n1\n0\n12\n\n\n54\n1\n1\n1\n0\n13\n\n\n55\n-1\n-1\n-1\n1\n14\n\n\n56\n0\n-1\n-1\n1\n15\n\n\n57\n1\n-1\n-1\n1\n16\n\n\n58\n-1\n0\n-1\n1\n17\n\n\n59\n0\n0\n-1\n1\n18\n\n\n60\n1\n0\n-1\n1\n19\n\n\n61\n-1\n1\n-1\n1\n20\n\n\n62\n0\n1\n-1\n1\n21\n\n\n63\n1\n1\n-1\n1\n22\n\n\n64\n-1\n-1\n0\n1\n23\n\n\n65\n0\n-1\n0\n1\n24\n\n\n66\n1\n-1\n0\n1\n25\n\n\n67\n-1\n0\n0\n1\n26\n\n\n68\n0\n0\n0\n1\n27\n\n\n69\n1\n0\n0\n1\n28\n\n\n70\n-1\n1\n0\n1\n29\n\n\n71\n0\n1\n0\n1\n30\n\n\n72\n1\n1\n0\n1\n31\n\n\n73\n-1\n-1\n1\n1\n32\n\n\n74\n0\n-1\n1\n1\n33\n\n\n75\n1\n-1\n1\n1\n34\n\n\n76\n-1\n0\n1\n1\n35\n\n\n77\n0\n0\n1\n1\n36\n\n\n78\n1\n0\n1\n1\n37\n\n\n79\n-1\n1\n1\n1\n38\n\n\n80\n0\n1\n1\n1\n39\n\n\n81\n1\n1\n1\n1\n40\n\n\n\n\n\nAnd we are done. At this point, I’ll note that we’ve just re-invented a signed base-3 number system: digit \\(i\\) (counting from zero) repr"
  },
  {
    "objectID": "posts/four-weights/index.html#comments",
    "href": "posts/four-weights/index.html#comments",
    "title": "Bachet’s Four Weights Problem",
    "section": "Comments",
    "text": "Comments"
  },
  {
    "objectID": "posts/four-weights/index.html#footnotes",
    "href": "posts/four-weights/index.html#footnotes",
    "title": "Bachet’s Four Weights Problem",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAmong Bachet’s accomplishments is a method of constructing magic squares, a way of solving indeterminate equations with continued fractions, the proof of Bézout’s Identity, and a Latin translation of Arithmetica by Diophantus (he of the Diophantine equations). Famously, Fermat’s last theorem was a scribbled margin note on his copy of this very translation.↩︎"
  },
  {
    "objectID": "posts/welcome-to-rworks/index.html",
    "href": "posts/welcome-to-rworks/index.html",
    "title": "Welcome to R Works!",
    "section": "",
    "text": "Artwork by @allison_horst\n\n\nThis is a heartfelt welcome to all members of the R community, from long-time readers of R Views to those just joining us. For years, R Views served as a source of resources, insights, and inspiration for R users. We are delighted to bring those ambitions to a new, Quarto-powered technical blog. Our goal is that R Works becomes a hub for news, opinions, and stories as the landscape of R continues to grow and evolve.\nR is more than code, it’s also the people behind it. We hope this blog reflects diverse perspectives and experiences from across the community. To that end, we invite your voices to help shape this space together. If you have unique perspectives, updates, commentary, or examples of using R that you’d love to share, we want to hear from you. Email us at rworks.editors@gmail.com.\nHappy Reading!"
  },
  {
    "objectID": "posts/100bushels/index.html",
    "href": "posts/100bushels/index.html",
    "title": "100 Bushels of Corn",
    "section": "",
    "text": "About the author\n\n\n\nNina Zumel is a data scientist based in San Francisco, with 20+ years of experience in machine learning, statistics, and analytics. She is the co-founder of the data science consulting firm Win-Vector LLC, and (with John Mount) the co-author of Practical Data Science with R, now in its second edition.\nI was browsing the December, 1908 issue of The Strand Magazine (it’s related to a hobby of mine), when I came across an article called “The World’s Best Puzzles”, by Henry Dudeney, who seems to have been the Martin Gardner of his day. Here’s a cool puzzle from that article, which according to Dudeney was first recorded by Alcuin, Abbot of Canterbury (735-804). I assume it’s from his manuscript Propositiones ad Acutendos Juvenes (Problems to Sharpen Youths)."
  },
  {
    "objectID": "posts/100bushels/index.html#the-puzzle",
    "href": "posts/100bushels/index.html#the-puzzle",
    "title": "100 Bushels of Corn",
    "section": "The Puzzle",
    "text": "The Puzzle\n\n100 bushes of corn are distributed to 100 people such that every man receives 3 bushels, every woman 2 bushels, and every child 1/2 a bushel. How many men, women, and children are there?\n\nThere are seven solutions; Dudeney gives one: 20 men, 0 women, and 80 children. Can you find the other six?\nLet’s put the puzzle into algebra, so it’s easier to discuss.\n\\[\n\\begin{aligned}\nm + w + c &= 100 \\\\\n3m + 2w + 0.5c &= 100 \\\\\n\\end{aligned}\n\\]\nSolve for \\(m\\), \\(w\\), and \\(c\\).\nThis problem (or one very close to it), is known as a system of Diophantine equations.\nHere’s a picture to look at while you try to solve it. The answer is below. Don’t peek!\n\n\n\nThe Mathematicians, Chirico (1917)"
  },
  {
    "objectID": "posts/100bushels/index.html#the-solution",
    "href": "posts/100bushels/index.html#the-solution",
    "title": "100 Bushels of Corn",
    "section": "The Solution",
    "text": "The Solution\nHere’s my solution. I’ll break it into steps. From the problem statement, we know \\(m\\), \\(w\\), and \\(c\\) are all nonnegative integers.\n1. \\(c\\) is even.\nThat there is an even number of children is obvious from the fact that the total number of bushels is integral, and that the number of men, women, and children all have to be integral.\n2. \\(w\\) is a multiple of 5.\nTo prove this, we take the two original equations and eliminate \\(m\\), by multiplying the first equation by \\(-3\\) and adding the two together.\n\\[\n\\begin{aligned}\n-3m &- 3w &- 3c &= -300 \\\\\n3m &+ 2w &+ 0.5c &= 100\n\\end{aligned}\n\\]\nthis gives us:\n\\[\n\\begin{aligned}\n-w & -2.5c &= -200 \\\\\nw &+ 2.5c &= 200\n\\end{aligned}\n\\]\nAnother way to write the last equation is\n\\[\nw + (5/2) c = 200\n\\]\nSince \\(c\\) is even, \\((5/2) c\\) is divisible by 5, and 200 is divisible by 5; therefore, \\(w\\) is divisible by 5. QED\n3. \\(w \\leq 30\\)\nTo prove this, let’s eliminate \\(c\\).\n\\[\n\\begin{aligned}\n-0.5m &- 0.5w &-0.5c &= -50\\\\\n   3m &+ 2w   &+ 0.5c &= 100\n\\end{aligned}   \n\\]\nthis results in:\n\\[\n\\begin{aligned}\n2.5m &+ 1.5w &+ 0   &= 50 \\\\\n   5m &+   3w &+ 0   &= 100\n\\end{aligned}\n\\]\nwhich gives us\n\\[\nm = 20 - (3/5) w\n\\]\nNow we apply the fact that \\(m \\geq 0\\):\n\\[\n\\begin{aligned}\n20 - (3/5) w &\\geq 0 \\\\\n(3/5) w &\\leq20 \\\\\n3 w  &\\leq100 \\\\\nw &\\leq 100/3 = 33.333...\n\\end{aligned}\n\\]\nAnd since we know that \\(w\\) must be a multiple of 5, this gives us \\(w \\leq 30\\). QED\nWhat are the multiples of 5 that are less than or equal to 30?\n\n\nShow the code\nw = seq(from=0, to=30, by=5)\nw\n\n\n[1]  0  5 10 15 20 25 30\n\n\nThat’s 7 values—exactly what we’re looking for! So we’re basically done, but we can fill in all the counts just to polish it off. I’ll do that in R, but you can do it in any language, of course.\n\n\nShow the code\n# from Step 3\nm = 20 - (3 / 5) * w\n\n# from the fact that there are 100 people total\nc = 100 - (m + w)\n\npframe = data.frame(\n  men = m,\n  women = w,\n  children = c,\n  total_pop = m + w + c,\n  bushels = 3 * m + 2 * w + 0.5 * c\n)\n\nknitr::kable(pframe, caption = \"Allocations of men, women, and children\")\n\n\n\nAllocations of men, women, and children\n\n\nmen\nwomen\nchildren\ntotal_pop\nbushels\n\n\n\n\n20\n0\n80\n100\n100\n\n\n17\n5\n78\n100\n100\n\n\n14\n10\n76\n100\n100\n\n\n11\n15\n74\n100\n100\n\n\n8\n20\n72\n100\n100\n\n\n5\n25\n70\n100\n100\n\n\n2\n30\n68\n100\n100\n\n\n\n\n\nAnd there you have it: the seven solutions to the “100 bushels of corn” problem."
  },
  {
    "objectID": "posts/r-work-opportunities/index.html",
    "href": "posts/r-work-opportunities/index.html",
    "title": "R Work Opportunities",
    "section": "",
    "text": "I’ve been spotting new opportunities in data science, analytics, and research that highlight R programming skills and wanted to share a few interesting positions. Here are jobs open as of publishing (please check the links for the latest status). Locations are included when specified.\nRemote Roles\nHybrid Roles\nOnsite\nThanks to the community meetups and forums where these roles were highlighted:\nI’ll continue sharing open roles with R requirements periodically, so check back for new leads!"
  },
  {
    "objectID": "posts/r-work-opportunities/index.html#comments",
    "href": "posts/r-work-opportunities/index.html#comments",
    "title": "R Work Opportunities",
    "section": "Comments",
    "text": "Comments"
  },
  {
    "objectID": "posts/meta-analysis/index.html",
    "href": "posts/meta-analysis/index.html",
    "title": "Examining Meta-Analysis",
    "section": "",
    "text": "In this post, we would like to review the idea of meta-analysis and compare a traditional, frequentist style, random effects meta-analysis to Bayesian methods. We will do this using the meta R package and a Bayesian analysis conducted with R but actually carried out by the Stan programming language on the back end. We will use a small but interesting textbook data set of summary data from eight randomized controlled trials. These studies examined the effectiveness of the calcium channel blocker Amlodipine compared to a placebo in improving work capacity in patients with angina. The data set and the analysis with the meta package come directly from the textbook by Chen and Peace (2013). Although there are several R packages that are capable of doing the Bayesian meta-analysis, we chose to work in rstan to demonstrate its flexibility and hint how one might go about doing a large complex study that doesn’t quite fit with pre-programmed paradigms.\nTo follow our examples, you really don’t need to know much more about meta-analysis than the definition offered by Wikipedia contributors (2024): “meta-analysis is a method of synthesis of quantitative data from multiple independent studies addressing a common research question”. If you want to dig deeper, a good overview of the goals and terminology can be found in Israel and Richter (2011). The online textbook Doing Meta-Analysis in R: A Hands-on Guide by Mathias Harrier and Ebert (2021) will take you a long way in performing your own work."
  },
  {
    "objectID": "posts/meta-analysis/index.html#example",
    "href": "posts/meta-analysis/index.html#example",
    "title": "Examining Meta-Analysis",
    "section": "Example",
    "text": "Example\nLet’s begin: load the required packages and read in the data.\n\n\nShow the code\nlibrary(meta)\n\nangina &lt;- read.csv(file = \"AmlodipineData.csv\",\n                   strip.white = TRUE,\n                   stringsAsFactors = FALSE)\n\nangina |&gt;\n  knitr::kable()\n\n\n\n\n\nProtocol\nnE\nmeanE\nvarE\nnC\nmeanC\nvarC\n\n\n\n\n154\n46\n0.2316\n0.2254\n48\n-0.0027\n0.0007\n\n\n156\n30\n0.2811\n0.1441\n26\n0.0270\n0.1139\n\n\n157\n75\n0.1894\n0.1981\n72\n0.0443\n0.4972\n\n\n162\n12\n0.0930\n0.1389\n12\n0.2277\n0.0488\n\n\n163\n32\n0.1622\n0.0961\n34\n0.0056\n0.0955\n\n\n166\n31\n0.1837\n0.1246\n31\n0.0943\n0.1734\n\n\n303\n27\n0.6612\n0.7060\n27\n-0.0057\n0.9891\n\n\n306\n46\n0.1366\n0.1211\n47\n-0.0057\n0.1291\n\n\n\n\n\nThe data set contains eight rows, each representing the measured effects of treatment and control on different groups. The column definitions are:\n\nProtocol id number of the study the row is summarizing.\nnE number of patients in the treatment group.\nmeanE mean treatment effect observed.\nvarE variance of treatment effect observed.\nnC number of patients in the control group.\nmeanC mean control effect observed.\nvarC variance of control effect observed.\n\n\nNaive Pooling\nA statistically inefficient, naive technique to combine the studies would be to pool all of the data. This is usually not even possible, as most studies don’t share data — and at best share summaries. We might hope this is enough to get a good estimate of the expected difference between treatment and non-treatment. However this ignores variance and treats low-quality results on the same footing as high-quality results, yielding unreliable results. Naive pooling also lacks standard diagnostic procedures and indications.\nThat being said, let’s form the naive pooled estimate.\n\n\nShow the code\nsum(angina$nE * angina$meanE) / sum(angina$nE) - sum(angina$nC * angina$meanC) / sum(angina$nC)\n\n\n[1] 0.2012727"
  },
  {
    "objectID": "posts/meta-analysis/index.html#a-fixed-effects-model",
    "href": "posts/meta-analysis/index.html#a-fixed-effects-model",
    "title": "Examining Meta-Analysis",
    "section": "A Fixed Effects Model",
    "text": "A Fixed Effects Model\nPerhaps the simplest reliable analysis is the fixed effects model. The underlying assumption for the fixed-effects model is that the true underlying effect or difference between treatment and control, \\(\\delta\\), is the same for all studies in the meta-analysis and that all possible risk factors are the same for all studies. Each study has its own observed effect size \\(\\hat{\\delta_i}\\). However, each is assumed to be a noisy estimate of \\(\\delta\\).\nSuch a study is summarized as follows.\n\n\nShow the code\nfixed.angina &lt;- metacont(\n  nE, meanE, sqrt(varE),\n  nC, meanC, sqrt(varC),\n  data = angina,\n  study.lab = \"Protocol\",\n  random = FALSE)\n\nsummary(fixed.angina)\n\n\n       MD            95%-CI %W(common)\n1  0.2343 [ 0.0969; 0.3717]       21.2\n2  0.2541 [ 0.0663; 0.4419]       11.4\n3  0.1451 [-0.0464; 0.3366]       10.9\n4 -0.1347 [-0.3798; 0.1104]        6.7\n5  0.1566 [ 0.0072; 0.3060]       17.9\n6  0.0894 [-0.1028; 0.2816]       10.8\n7  0.6669 [ 0.1758; 1.1580]        1.7\n8  0.1423 [-0.0015; 0.2861]       19.4\n\nNumber of studies: k = 8\nNumber of observations: o = 596 (o.e = 299, o.c = 297)\n\n                        MD           95%-CI    z  p-value\nCommon effect model 0.1619 [0.0986; 0.2252] 5.01 &lt; 0.0001\n\nQuantifying heterogeneity (with 95%-CIs):\n tau^2 = 0.0001 [0.0000; 0.1667]; tau = 0.0116 [0.0000; 0.4082]\n I^2 = 43.2% [0.0%; 74.9%]; H = 1.33 [1.00; 2.00]\n\nTest of heterogeneity:\n     Q d.f. p-value\n 12.33    7  0.0902\n\nDetails of meta-analysis methods:\n- Inverse variance method\n- Restricted maximum-likelihood estimator for tau^2\n- Q-Profile method for confidence interval of tau^2 and tau\n- Calculation of I^2 based on Q\n\n\nThe primary result of the summary is: the overall estimate for \\(\\delta\\) is 0.1619, much smaller than the naive pooling estimate of 0.2013. Also, a number of diagnostics seem “okay.” From the forest plot, it is easy to see that the Amlodipine treatment is not statistically significant for four of the protocols, but the overall effect of the difference between the treatment and the control is statistically significant. The plot also shows the test for heterogeneity, which indicates that there is no evidence against homogeneity.\n\n\nShow the code\nmeta::forest(fixed.angina, layout = \"RevMan5\")\n\n\n\n\n\n\n\n\n\nNotice the fixed effects model is not the earlier naive mean; it is also not theoretically the same estimate as the random effects model, which we now discuss."
  },
  {
    "objectID": "posts/meta-analysis/index.html#a-random-effects-model",
    "href": "posts/meta-analysis/index.html#a-random-effects-model",
    "title": "Examining Meta-Analysis",
    "section": "A Random Effects Model",
    "text": "A Random Effects Model\nThe next level of modeling is a random effects model. In this style model, we admit that the studies may be, in fact, studying different populations and different effects. A perhaps cleaner way to think about this is not as a type of effect (fixed or random), but as a model structured as a hierarchy (Gelman and Hill (2006)). The idea is that the analyst claims the populations are related, and the different effects are drawn from a common distribution, and observations are then drawn from these unobserved per-population effects. Inference is possible as the observed outcomes distributionally constrain the unobserved parameters.\nThe underlying assumption for the random-effects model is that each study has its own true underlying treatment effect, \\(\\delta_i\\), with variance \\({\\sigma_i}^2\\) that is estimated by \\(\\hat{\\delta_i}\\) Furthermore, all of the \\(\\delta_i\\) follow a \\(N(\\delta,\\tau^2)\\) distribution. Hence,\n\\[\n\\begin{align}\n\\hat{\\delta_i} &\\sim N(\\delta,\\sigma_i^2) \\\\\n\\delta_i &\\sim N(\\delta,\\tau^2)\n\\end{align}\n\\]\n\nFit the Random-Effects Model\n\n\nShow the code\nrandom.angina &lt;- metacont(\n  nE, meanE, sqrt(varE),\n  nC, meanC, sqrt(varC),\n  data = angina,\n  study.lab = \"Protocol\",\n  random = TRUE)\nsummary(random.angina)\n\n\n       MD            95%-CI %W(common) %W(random)\n1  0.2343 [ 0.0969; 0.3717]       21.2       21.1\n2  0.2541 [ 0.0663; 0.4419]       11.4       11.4\n3  0.1451 [-0.0464; 0.3366]       10.9       11.0\n4 -0.1347 [-0.3798; 0.1104]        6.7        6.7\n5  0.1566 [ 0.0072; 0.3060]       17.9       17.9\n6  0.0894 [-0.1028; 0.2816]       10.8       10.9\n7  0.6669 [ 0.1758; 1.1580]        1.7        1.7\n8  0.1423 [-0.0015; 0.2861]       19.4       19.3\n\nNumber of studies: k = 8\nNumber of observations: o = 596 (o.e = 299, o.c = 297)\n\n                         MD           95%-CI    z  p-value\nCommon effect model  0.1619 [0.0986; 0.2252] 5.01 &lt; 0.0001\nRandom effects model 0.1617 [0.0978; 0.2257] 4.96 &lt; 0.0001\n\nQuantifying heterogeneity (with 95%-CIs):\n tau^2 = 0.0001 [0.0000; 0.1667]; tau = 0.0116 [0.0000; 0.4082]\n I^2 = 43.2% [0.0%; 74.9%]; H = 1.33 [1.00; 2.00]\n\nTest of heterogeneity:\n     Q d.f. p-value\n 12.33    7  0.0902\n\nDetails of meta-analysis methods:\n- Inverse variance method\n- Restricted maximum-likelihood estimator for tau^2\n- Q-Profile method for confidence interval of tau^2 and tau\n- Calculation of I^2 based on Q\n\n\n\n\nShow the code\nmeta::forest(random.angina, layout = \"RevMan5\")\n\n\n\n\n\n\n\n\n\nThe random effects result is a \\(\\delta\\) estimate of about 0.1617. In this case, not much different than the fixed effects estimate."
  },
  {
    "objectID": "posts/meta-analysis/index.html#issues",
    "href": "posts/meta-analysis/index.html#issues",
    "title": "Examining Meta-Analysis",
    "section": "Issues",
    "text": "Issues\nSome issues associated with the above analyses include:\n\nLimited control of the distributional assumptions. What if we wanted to assume specific distributions on the un-observed individual outcomes? What if we want to change assumptions for a sensitivity analysis?\nThe results are limited to point estimates of the distribution parameters.\nThe results may depend on the underlying assumption of Normal distributions of published summaries.\nThe reported significance is not necessarily the probability of positive effect for a new subject.\n\n\nBayesian analysis\nTo try and get direct control of the model, graphs, and the model explanation, we will perform a direct hierarchical analysis using Stan through the rstan package.\nWe want to estimate the unobserved true value of treatment effect by a meta-analysis of different studies. The challenges of meta-analysis include:\n\nThe studies may be of somewhat different populations, implying different mean and variance of treatment and control response.\nWe are working from summary data from the studies and not data on individual patients.\n\nWe are going to make things simple for the analyst and ask only for approximate distributional assumptions on the individual respondents in terms of unknown, to-be-inferred parameters. Then we will use Stan to sample individual patient outcomes (and unobserved parameters) that are biased to be consistent with the known summaries. This saves having to know or assume distributions of the summary statistics. And, in fact, we could try individual distributions different from the “Normal” we plug in here.\nThe advantage of working this way is that the modeling assumptions are explicitly visible and controllable. The matching downside is that we have to own our modeling assumptions; we lose the anonymity of the implicit justification of invoking tradition and recognized authorities.\nTo get this to work, we use two Stan tricks or patterns:\n\nInstantiate many intermediate variables (such as individual effects).\nSimulate equality enforcement by saying a specified difference tends to be small.\n\nFirst, load the required packages.\n\n\nShow the code\n# attach packages\nlibrary(ggplot2)\nlibrary(rstan)\nlibrary(digest)\nsource(\"define_Stan_model.R\")\n\n\nAnd then prepare the study names for display.\n\n\nShow the code\nn_studies = nrow(angina)\n# make strings for later use\ndescriptions = vapply(\n  seq(n_studies),\n  function(i) { paste0(\n    'Protocol ', angina[i, 'Protocol'], ' (',\n    'nE=', angina[i, 'nE'], ', meanE=', angina[i, 'meanE'],\n    ', nC=', angina[i, 'nC'], ', meanC=', angina[i, 'meanC'],\n    ')') },\n  character(1))\n\n\nThe modeling principles are as follows:\n\nThere are unobserved true treatment and control effects we want to estimate. Call these \\(\\mu^{treatment}\\) and \\(\\mu^{control}\\). We assume a shared standard deviation \\(\\sigma\\). We can easily model without a shared standard deviation by introducing more parameters to the model specification.\nFor each protocol or study, we have ideal unobserved mean treatment effects and mean control effects. Call these \\(\\mu_{i}^{treatment}\\) and \\(\\mu_{i}^{control}\\) for \\(i = 1 \\cdots 8\\).\n\nThe equations bringing each study \\(i\\) into our Bayesian model are as follows.\n\\[\\begin{align}\n\\mu^{treatment}_i &\\sim N(\\mu^{treatment}, \\sigma^2) &\\# \\; \\mu^{treatment} \\; \\text{is what we are trying to infer} \\\\\n\\mu^{control}_i &\\sim N(\\mu^{control}, \\sigma^2) &\\# \\; \\mu^{control} \\;\\text{is what we are trying to infer} \\\\\n\nsubject^{treatment}_{i,j} &\\sim N(\\mu^{treatment}_i, \\sigma_i^2) &\\# \\; \\text{unobserved} \\\\\nsubject^{control}_{i,j} &\\sim N(\\mu^{control}_i, \\sigma_i^2) &\\# \\; \\text{unobserved} \\\\\n\nmean_j(subject^{treatment}_{i,j}) - observed\\_mean^{treatment}_{i} &\\sim N(0, 0.01) &\\# \\; \\text{force inferred to be near observed} \\\\\nmean_j(subject^{control}_{i,j}) - observed\\_mean^{control}_{i} &\\sim N(0, 0.01) &\\# \\; \\text{force inferred near to be observed} \\\\\nvar_j(subject^{treatment}_{i,j}) - observed\\_var^{treatment}_{i} &\\sim N(0, 0.01) &\\# \\; \\text{force inferred to be near observed} \\\\\nvar_j(subject^{control}_{i,j}) - observed\\_var^{control}_{i} &\\sim N(0, 0.01) &\\# \\; \\text{force inferred near to be observed}\n\\end{align}\\]\nThe above equations are designed to be argued over. They need to be believed to relate the unobserved true treatment and control effects to the recorded study summaries. If they are not believed, is there a correction that would fix that? If the equations are good enough, then we can sample the implied posterior distribution of the unknown true treatment and control effects, which would finish the meta-analysis. The driving idea is that it may be easier to discuss how unobserved individual measurements may relate to observed and unobserved summaries and parameters than to work out how to relate statistical summaries to statistical summaries.\nIt is mechanical to translate the above relations into a Stan source model to make the desired inferences. The lines marked “force inferred to be near observed” are not distributional beliefs- but just a tool for enforcing that the inferred individual data should match the claimed summary statistics.\n\n\nShow the code\n# the Stan data\nstan_data = list(\n  n_studies = n_studies,\n  nE = array(angina$nE, dim = n_studies),  # deal with length 1 arrays confused with scalars in JSON path\n  meanE = array(angina$meanE, dim = n_studies),\n  varE = array(angina$varE, dim = n_studies), \n  nC = array(angina$nC, dim = n_studies), \n  meanC = array(angina$meanC, dim = n_studies), \n  varC = array(angina$varC, dim = n_studies))\n\n\nHere we define the Stan Model.\n\n\nShow the code\nanalysis_src_joint &lt;- define_Stan_model(n_studies = n_studies, c_replacement = '')\n\n\nThis code runs the procedure. The function run_cache() runs the standard stan() function, saving the result to a file cache for quick and deterministic re-re-renderings of the notebook. The caching is not required, but the runs took about five minutes on an old-Intel based Mac.\n\n\nShow the code\n# run the sampling procedure\nfit_joint &lt;- run_cached(\n  stan,\n  list(\n  model_code = analysis_src_joint,  # Stan program\n  data = stan_data,                 # named list of data\n  chains = 4,                       # number of Markov chains\n  warmup = 2000,                    # number of warm up iterations per chain\n  iter = 4000,                      # total number of iterations per chain\n  cores = 4,                        # number of cores (could use one per chain)\n  refresh = 0,                      # no progress shown\n  pars = c(\"lp__\",                  # parameters to bring back\n           \"inferred_grand_treatment_mean\", \n           \"inferred_grand_control_mean\", \n           \"inferred_between_group_stddev\",\n           \"inferred_group_treatment_mean\", \n           \"inferred_group_control_mean\",\n           \"inferred_in_group_stddev\", \n           \"sampled_meanE\", \n           \"sampled_varE\",\n           \"sampled_meanC\", \n           \"sampled_varC\")\n  ),\n  prefix = \"Amlodipine_joint\"\n)\n\n\nAnd then, we extract the results.\n\n\nShow the code\n# show primary inference\ninference &lt;- fit_joint |&gt;\n  as.data.frame() |&gt;\n  (`[`)(c(\"inferred_grand_treatment_mean\", \"inferred_grand_control_mean\", \"inferred_between_group_stddev\")) |&gt;\n  colMeans() |&gt;\n  as.list() |&gt;\n  data.frame() \ninference['delta'] &lt;- inference['inferred_grand_treatment_mean'] - inference['inferred_grand_control_mean']\n\ninference |&gt;\n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\ninferred_grand_treatment_mean\ninferred_grand_control_mean\ninferred_between_group_stddev\ndelta\n\n\n\n\n0.2002917\n0.0391202\n0.0650105\n0.1611715\n\n\n\n\n\nAnd our new estimate is: 0.1611715 which is very similar to the previous results. We can graph the inferred posterior distribution of effect size as follows.\nFirst, we plot the estimated posterior distribution of both the treatment and control effects. This is the final result of the analysis, using all of the data, simulated from the eight trials. In this result, high-variance studies have a diminished influence on the overall estimated effect sizes. Running all of the simulated data together without maintaining the trial structure would produce an inflated 0.2 again.\n\n\nShow the code\n# plot the grand group inferences \ndual_density_plot(\n  fit_joint, \n  c1 = 'inferred_grand_treatment_mean', \n  c2 = 'inferred_grand_control_mean',\n  title = 'grand estimate')\n\n\n\n\n\n\n\n\n\n\n\nWatching the Pooling\nWe can examine how the hierarchical model changes the estimates as follows. In each case we are plotting the posterior distribution of the unknown treatment and control effect estimates, this time per original study, with and without pooling analysis.\nTo do this, we define an additional “the studies are unrelated model”.\n\n\nShow the code\nanalysis_src_independent &lt;- define_Stan_model(n_studies = n_studies, c_replacement = '//')\n\n\nWe then fit the model or use previously cached results.\n\n\nShow the code\n# run the sampling procedure\nfit_independent &lt;- run_cached(\n  stan,\n  list(\n  model_code = analysis_src_independent,  # Stan program\n  data = stan_data,                       # named list of data\n  chains = 4,                             # number of Markov chains\n  warmup = 2000,                          # number of warm up iterations per chain\n  iter = 4000,                            # total number of iterations per chain\n  cores = 4,                              # number of cores (could use one per chain)\n  refresh = 0,                            # no progress shown\n  pars = c(\"lp__\",                        # parameters to bring back\n           \"inferred_group_treatment_mean\", \n           \"inferred_group_control_mean\",\n           \"inferred_in_group_stddev\", \n           \"sampled_meanE\", \n           \"sampled_varE\",\n           \"sampled_meanC\",\n           \"sampled_varC\")\n  ),\n  prefix = \"Amlodipine_independent\"\n)\n\n\nThen, we can plot the compared inferences of the hierarchical and independent models.\nThe top plot shows the Bayesian inference that would result using only data from a single study. The bottom plot shows the estimate made for the given study, given the data from the other studies. Notice that in this first pair of plots, the bottom plot sharpens the distributions and tends to pull the means together. The first two plots have the means pulled in, the third pushed out, and behavior varies by group from then on. Notice in Protocol 162, the inferred means are reversed. These graphs are the Bayesian analogs of the forest plots above.\n\n\nShow the code\n# plot comparison inferences\nfor(i in seq(n_studies)) {\n  print(double_dual_density_plot(\n    fitA = fit_independent,\n    fitB = fit_joint,\n    c1 = gsub('{i}', as.character(i), 'inferred_group_treatment_mean[{i}]', fixed = TRUE), \n    c2 = gsub('{i}', as.character(i), 'inferred_group_control_mean[{i}]', fixed = TRUE), \n    title = gsub('{p}', descriptions[[i]], '{p}\\nestimates', fixed = TRUE),\n    vlines=c(angina[i, 'meanE'], angina[i, 'meanC'])))\n}"
  },
  {
    "objectID": "posts/meta-analysis/index.html#conclusion",
    "href": "posts/meta-analysis/index.html#conclusion",
    "title": "Examining Meta-Analysis",
    "section": "Conclusion",
    "text": "Conclusion\nIt is a challenge to communicate exactly what meta-analysis a given standard package actually implements. It is our assumption that many standard meta-analyses reflect the tools available to the researchers and do not necessarily provide the ability to implement distributions that reflect the modeling assumptions they would prefer. Or, to be blunt, you don’t really know what the packaged models are doing until you can match them to a known calculation.\nPerhaps the greatest difference between the “standard” and Bayesian approaches is that Bayesian modeling requires the analyst to really own the modeling assumptions. This is why we wrote out so many of the modeling distributional assumptions as formulas instead of as named methodologies in our method descriptions.\nWe feel that there is a gap in the available practical literature. There is room for more teaching materials making meta-analysis more approachable by relating them to explicit model structures."
  },
  {
    "objectID": "posts/meta-analysis/index.html#references",
    "href": "posts/meta-analysis/index.html#references",
    "title": "Examining Meta-Analysis",
    "section": "References",
    "text": "References\n\n\nChen, Ding-Geng, and Karl E. Peace. 2013. Applied Meta-Analysis with R. CRC Press.\n\n\nGelman, Andrew, and Jennifer Hill. 2006. Data Analysis Using Regression and Multilevel/Hierarchical Models. Analytical Methods for Social Research. Cambridge University Press.\n\n\nIsrael, Heidi, and Randy R. Richter. 2011. “A Guide to Understanding Meta-Analysis.” Journal of Orthopaedic & Sports Physical Therapy 41 (7): 496–504. https://doi.org/10.2519/jospt.2011.3333.\n\n\nMathias Harrier, Toshi A. Furukawa, Pim Cuijpers, and David D. Ebert. 2021. Doing Meta-Analysis with R: A Hands-on Guide. CRC Press.\n\n\nWikipedia contributors. 2024. “Meta-Analysis — Wikipedia, the Free Encyclopedia.” https://en.wikipedia.org/w/index.php?title=Meta-analysis&oldid=1251112044."
  },
  {
    "objectID": "posts/meta-analysis/index.html#appendix-reproducing-the-result",
    "href": "posts/meta-analysis/index.html#appendix-reproducing-the-result",
    "title": "Examining Meta-Analysis",
    "section": "Appendix: Reproducing the Result",
    "text": "Appendix: Reproducing the Result\nThe files required to reproduce the result are:\n\nExaminingMetaAnalysis.qmd: this notebook.\nAmlodipineData.csv: the original data.\ndefine_Stan_model.R: helper functions.\n(optional) cache_Amlodipine_independent_b818d8db720377f42b728cbc88a94e58.RDS: the cached result.\n(optional) cache_Amlodipine_joint_f0ae8de9e99d9396a5f2a21f5638030f.RDS: the cached result."
  },
  {
    "objectID": "posts/meta-analysis/index.html#comments",
    "href": "posts/meta-analysis/index.html#comments",
    "title": "Examining Meta-Analysis",
    "section": "Comments",
    "text": "Comments"
  },
  {
    "objectID": "posts/october-2024-top-40-new-cran-packages/index.html",
    "href": "posts/october-2024-top-40-new-cran-packages/index.html",
    "title": "October 2024: Top 40 New CRAN Packages",
    "section": "",
    "text": "One hundred eighty-one new packages made CRAN’s final cut in October. Here are my Top 40 picks in thirteen categories: AI, Climate Analysis, Computational Methods, Data, Epidemiology, Genomics, Machine Learning, Medicine, Quality Management, Statistics, Time Series, Utilities, and Visualization."
  },
  {
    "objectID": "posts/october-2024-top-40-new-cran-packages/index.html#comments",
    "href": "posts/october-2024-top-40-new-cran-packages/index.html#comments",
    "title": "October 2024: Top 40 New CRAN Packages",
    "section": "Comments",
    "text": "Comments"
  },
  {
    "objectID": "posts/100Bushels-Revisited/index.html",
    "href": "posts/100Bushels-Revisited/index.html",
    "title": "100 Bushels of Corn, Revisited",
    "section": "",
    "text": "About the authors\n\n\n\nJohn Mount is a data scientist based in San Francisco, with 20+ years of experience in machine learning, statistics, and analytics. He is the co-founder of the data science consulting firm Win-Vector LLC, and (with Nina Zumel) the co-author of Practical Data Science with R, now in its second edition.\nNina Zumel is a data scientist based in San Francisco, with 20+ years of experience in machine learning, statistics, and analytics. She is the co-founder of the data science consulting firm Win-Vector LLC, and (with John Mount) the co-author of Practical Data Science with R, now in its second edition."
  },
  {
    "objectID": "posts/100Bushels-Revisited/index.html#introduction",
    "href": "posts/100Bushels-Revisited/index.html#introduction",
    "title": "100 Bushels of Corn, Revisited",
    "section": "Introduction",
    "text": "Introduction\nNina Zumel presented the “100 Bushels of Corn” puzzle here as a fun example of using R as a calculator. What if we want R to solve the puzzle for us, instead of merely being a calculator?\nLet’s give that a go."
  },
  {
    "objectID": "posts/100Bushels-Revisited/index.html#setting-up-the-problem",
    "href": "posts/100Bushels-Revisited/index.html#setting-up-the-problem",
    "title": "100 Bushels of Corn, Revisited",
    "section": "Setting Up the Problem",
    "text": "Setting Up the Problem\n\n100 bushes of corn are distributed to 100 people such that every man receives 3 bushels, every woman 2 bushels, and every child 1/2 a bushel. How many men, women, and children are there?\n\nWe can write the 100 Bushels of Corn problem as finding integer vectors x that satisfy a %*% x = b for the following a, b. The first row specifies the constraint on the total number of men, women and children; the second row specifies the constraint on how many bushels each person gets. Notice that we doubled the values of the second equation to keep everything integral.\n\na &lt;- matrix(c(1, 1, 1, 6, 4, 1),\n            nrow = 2,\n            ncol = 3,\n            byrow = TRUE)\n\na\n\n     [,1] [,2] [,3]\n[1,]    1    1    1\n[2,]    6    4    1\n\n\n\nb &lt;- as.matrix(c(100, 200))\n\nb\n\n     [,1]\n[1,]  100\n[2,]  200\n\n\nThere are at least two main ways to solve this:\n\nBrute force. This is kind of the point of computers and programming languages.\nLinear algebra, in particular linear algebra over the ring of integers. This approach shows some of the richness of the R package environment curated at CRAN.\n\nLet’s take a quick look at these two solution styles."
  },
  {
    "objectID": "posts/100Bushels-Revisited/index.html#brute-force-solution",
    "href": "posts/100Bushels-Revisited/index.html#brute-force-solution",
    "title": "100 Bushels of Corn, Revisited",
    "section": "Brute Force Solution",
    "text": "Brute Force Solution\nA brute force solution is as follows. First, we get a simple upper bound on each variable. We can do this by checking one row of a %*% x = b.\n\nupper_bounds &lt;- floor(b[2] / a[2, ])\nupper_bounds\n\n[1]  33  50 200\n\n\nNow we try all plausible solutions.\n\nfor (x1 in 0:upper_bounds[[1]]) {\n  for (x2 in 0:upper_bounds[[2]]) {\n    # use a row of a to solve for x3\n    x3 &lt;- (b[1] - (a[1, 1] * x1 + a[1, 2] * x2)) / a[1, 3]\n    # check constraints\n    if ((x3 &gt;= 0) && (abs(x3 %% 1) &lt; 1e-8)) {\n      x = as.matrix(c(x1, x2, x3))\n      if (all(a %*% x == b)) {\n        print(c(x1, x2, x3))\n      }\n    }\n  }\n}\n\n[1]  2 30 68\n[1]  5 25 70\n[1]  8 20 72\n[1] 11 15 74\n[1] 14 10 76\n[1] 17  5 78\n[1] 20  0 80\n\n\nAnd this gives us exactly the 7 solutions Nina found. This is some of the magic of having a programmable computer: one can cheaply try a lot of potential solutions without needing a lot of theory."
  },
  {
    "objectID": "posts/100Bushels-Revisited/index.html#ring-theory-solution",
    "href": "posts/100Bushels-Revisited/index.html#ring-theory-solution",
    "title": "100 Bushels of Corn, Revisited",
    "section": "Ring Theory Solution",
    "text": "Ring Theory Solution\nIt turns out there is a systematic way to find all of the integral solutions to a linear system quickly, at least for low dimensional solution spaces. To do this we will use a ring theory or linear algebra matrix factorization called the Hermite normal form. Fortunately, R has a package for this, called numbers. We attach this package as follows.\n\nlibrary(numbers)\n\nThis package will find for us a lower diagonal integer matrix h and a square unimodular matrix u such that h = a %*% u. Unimodular matrices map the space of integer vectors Zn to the same space of integer vectors in a 1 to 1, onto, and invertible manner. This means finding integer solution vectors to a %*% x = b is equivalent to the problem of finding integer solutions to h %*% y = b, where x = u %*% y. This second problem is easier, as h is lower diagonal- so solving this system is just a matter of “back filling”.\nLet’s first find h, u.\n\nhnf &lt;- hermiteNF(a)\nh &lt;- hnf$H\nu &lt;- hnf$U\nstopifnot(all(h == a %*% u))\n\n\n# lower triangular transform of a\nh\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n\n\n\n# unimodular transform\nu\n\n     [,1] [,2] [,3]\n[1,]    7   -1   -3\n[2,]  -12    2    5\n[3,]    6   -1   -2\n\n\nWe now have a %*% u = h. a %*% x = b implies h %*% y = b where x = u %*% y. Let’s solve for a specific solution xs by back substitution.\n\n# back substitute to solve h %*% y = b\n# this uses the fact that h is lower-triangular\nh_rank &lt;- sum(diag(h) != 0)\nstopifnot(h_rank &gt; 1)\ny &lt;- numeric(ncol(a))\ny[1] &lt;- b[1] / h[1, 1]\nfor (i in 2:h_rank) {\n  y[i] &lt;- (b[i] - sum(h[i, 1:(i - 1)] * y[1:(i - 1)])) / h[i, i]\n}\nstopifnot(all(b == h %*% y))\nxs &lt;- u %*% y\nstopifnot(all(a %*% xs == b))\nxs\n\n     [,1]\n[1,]  500\n[2,] -800\n[3,]  400\n\n\nIt is a standard result of linear algebra that all solutions of a %*% x = b are of the form x = xs + z where a %*% z = 0 (that is, z is in the null space of h and a). In our case, the null space is spanned by the last column of u.\nLet’s show this column.\n\nnull_basis &lt;- u[, (h_rank + 1):ncol(u), drop = FALSE]\n\nnull_basis\n\n     [,1]\n[1,]   -3\n[2,]    5\n[3,]   -2\n\n\nSo in our case: all integer solutions of a %*% x = b are of the form [500, -800, 400] + k * [-3, 5, -2] for integer k.\nNow we just need to pick k to make everything non-negative (an implicit puzzle condition!). The sign changes of entries of [500, -800, 400] + k * [-3, 5, -2] happen for k where one of the coordinates is equal to zero. These are:\n\n500 - 3 * k == 0\n-800 + 5 * k == 0, and\n400 - 2 * k == 0.\n\nSo the k of interest are in the range ceiling(max(0, 800/5)) &lt;= k &lt;= floor(min(500/3, 400/2)), or 160 &lt;= k &lt;= 166. This gives us:\n\nfor (k in 160:166) {\n  soln &lt;- xs + k * null_basis\n  print(c(soln[1, 1], soln[2, 1], soln[3, 1]))\n}\n\n[1] 20  0 80\n[1] 17  5 78\n[1] 14 10 76\n[1] 11 15 74\n[1]  8 20 72\n[1]  5 25 70\n[1]  2 30 68\n\n\nAnd these are again exactly the solutions Nina Zumel gave in the first 100 Bushels post."
  },
  {
    "objectID": "posts/100Bushels-Revisited/index.html#conclusion",
    "href": "posts/100Bushels-Revisited/index.html#conclusion",
    "title": "100 Bushels of Corn, Revisited",
    "section": "Conclusion",
    "text": "Conclusion\nR gives the ability to exploit any combination of innate ability and knowledge, borrowed ability, or brute force in solving problems."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to R Works.\nR Works is the blog devoted to the R community and the R language. We imagine it as a place to read considered opinions about topics of interest to the R community, learn what is happening at conferences and user group meetings around the world, discover new R packages, and explore applications of R in various areas of statistics or data science.\nR Works is powered by Quarto. We moved to this platform to provide an enhanced experience for our readers. With Quarto’s advanced features, we can showcase R-related topics and deliver high-quality content.\nThe blog is edited by Joseph Rickert and Isabella Velásquez, who curate the content and ensure its quality. We hope that R Works will continue to attract other voices from around the R community. If you have something to say on an R-related topic, news, commentary, or an example of using R that you would like to share with the R community, please review the How to page.\nSyndicated on R-Bloggers and R Weekly."
  }
]