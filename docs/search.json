[
  {
    "objectID": "how-to-contribute.html",
    "href": "how-to-contribute.html",
    "title": "How to Contribute",
    "section": "",
    "text": "We hope R Works will attract other voices from around the R Community, including yourself! We’re always looking for high-quality, original work to demonstrate the power of R. If you would like to contribute R-related topics, news, commentary, or examples, please follow this process to submit a post for consideration."
  },
  {
    "objectID": "how-to-contribute.html#faqs",
    "href": "how-to-contribute.html#faqs",
    "title": "How to Contribute",
    "section": "FAQs",
    "text": "FAQs\n\nCan I cross-post my blog post?\n\nYes, you can cross-post your blog post as long as it is published at the same time as the one on R Works.\n\nDo you edit posts from the past?\n\nOnce published, we only edit posts from the past for typos or broken links."
  },
  {
    "objectID": "how-to-contribute.html#code-of-conduct",
    "href": "how-to-contribute.html#code-of-conduct",
    "title": "How to Contribute",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nR Works is released with a Contributor Code of Conduct. By contributing to this project, you agree to abide by its terms."
  },
  {
    "objectID": "how-to-contribute.html#license",
    "href": "how-to-contribute.html#license",
    "title": "How to Contribute",
    "section": "License",
    "text": "License\nUnless otherwise noted, content on R Works is licensed under the CC-BY license."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R Works",
    "section": "",
    "text": "Examining Meta-Analysis\n\n\n\n\n\nIn this post we would like to review the idea of meta-analysis and compare a traditional, frequentist style, random effects meta-analysis to Bayesian methods.\n\n\n\n\n\n2024-11-26\n\n\nJohn Mount, Joseph Rickert\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 2024: Top 40 New CRAN Packages\n\n\n\n\n\n\nTop 40\n\n\n\nOne hundred eighty-one new packages made CRAN’s final cut in October.\n\n\n\n\n\n2024-11-25\n\n\nJoseph Rickert\n\n\n\n\n\n\n\n\n\n\n\n\n100 Bushels of Corn, Revisited\n\n\n\n\n\n\nPuzzle Corner\n\n\n\nWe find more solutions to the 100 Bushels of Corn puzzle using the numbers R package.\n\n\n\n\n\n2024-11-22\n\n\nJohn Mount, Nina Zumel\n\n\n\n\n\n\n\n\n\n\n\n\n100 Bushels of Corn\n\n\n\n\n\n\nPuzzle Corner\n\n\n\n100 bushes of corn are distributed to 100 people such that every man receives 3 bushels, every woman 2 bushels, and every child 1/2 a bushel. How many men, women, and children are there? (Solved with R).\n\n\n\n\n\n2024-11-15\n\n\nNina Zumel\n\n\n\n\n\n\n\n\n\n\n\n\nManifold Learning\n\n\n\n\n\nManifold Learning reduces data dimensions to discover patterns for analysis and visualization. This post provides an overview of Manifold Learning and its algorithms, the tsne package, and other R tools and resources.\n\n\n\n\n\n2024-11-11\n\n\nJoseph Rickert\n\n\n\n\n\n\n\n\n\n\n\n\nR Work Opportunities\n\n\n\n\n\n\nR Community\n\n\n\nExplore new job opportunities that highlight R skills.\n\n\n\n\n\n2024-11-10\n\n\nIsabella Velásquez\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 2024: Top 40 New CRAN Packages\n\n\n\n\n\n\nTop 40\n\n\n\nTwo hundred thirty new packages made it to CRAN in September. Here are my “Top 40” selections in 17 categories.\n\n\n\n\n\n2024-10-31\n\n\nJoseph Rickert\n\n\n\n\n\n\n\n\n\n\n\n\nAugust 2024: Top 40 New CRAN Packages\n\n\n\n\n\n\nTop 40\n\n\n\n“Top 40” is back, broadcasting on the new R Works blog.\n\n\n\n\n\n2024-10-30\n\n\nJoseph Rickert\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to R Works!\n\n\n\n\n\n\nR Community\n\n\n\nWe hope that the R Works blog informs and inspires R users everywhere.\n\n\n\n\n\n2024-10-30\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/manifold-learning/index.html",
    "href": "posts/manifold-learning/index.html",
    "title": "Manifold Learning",
    "section": "",
    "text": "Many algorithms supporting AI and machine learning depend on the notion of embeddings. Data sets are mapped to or embedded in high-dimensional Euclidean vector spaces. Then, various mathematical strategies are employed to reduce data size by mapping these high dimensional points to structures in lower dimensional spaces in ways that preserve some important structural properties of the high dimensional data. Classic examples are the Word2Vec algorithm which maps similar words to nearby points in vector spaces, and Principal Components Analysis which maps multidimensional vector data to lower dimensional spaces while preserving the variance of the data points. The mathematical term for the structures sought to contain the data in the lower dimensional space is manifold. Manifolds are basically vector spaces with additional structure that enable notions such as connectedness and smoothness to make sense. Think of a sphere as a two-dimensional manifold in a three-dimensional space, or lines and circles as one-dimensional structures in a two-dimensional space.\nOver the past fifteen years or so, these kinds of geometric ideas about working with data have coalesced into the very mathematical field of Manifold Learning. In this post, I hope to provide a way for those of us who are not mathematicians, but willing to do some work to explore this incredibly interesting field. I’ll do this by pointing to some of the accessible literature, providing a couple of simple examples, and listing some R resources for exploration."
  },
  {
    "objectID": "posts/manifold-learning/index.html#an-overview-of-manifold-learning",
    "href": "posts/manifold-learning/index.html#an-overview-of-manifold-learning",
    "title": "Manifold Learning",
    "section": "An overview of Manifold Learning",
    "text": "An overview of Manifold Learning\nThis section comprises some notes on the marvelous review paper Manifold Learning: What, How, and Why by Marina Meilă and Hanyu Zhang. The paper is a comprehensive, clearly written, historical approach at a level suitable for beginners. It is an expert guide to the vast literature on the subject. The Annual Reviews version of the paper at the link above is a pleasure to work with because almost all of the essential papers are hyperlinked.\n\n\n\nThe Basic Problem\nThe basic problem motivating manifold learning is data reduction. Given a data set with D features or explanatory variables, how can we transform it into a smaller data set with fewer features in a way that retains all of the essential information and provides some insight about the structure of the data? The idea is similar to PCA. Here, we assume the data exist in a D-dimensional vector space but mostly lie in or near a k-dimensional subspace. PCA provides a linear mapping from \\(R^D\\) to \\(R^k\\).\n\n\nThe Manifold Assumption\nThe data are a sample from a probability distribution with support on, or near, a D-dimensional manifold embedded in \\(R^D\\).\n\n\nThree Paradigms for Manifold learning:\nThe term manifold learning was proposed by Roweis & Saul (2000) who proposed the Locally Linear Embedding (LLE) algorithm and Tenenbaum et al. (2000), who introduced the Isomap algorithm. There are three basic approaches to manifold learning: Locally linear approximations, Principal Curves and Surfaces, and Embeddings.\n\n\nLocal Linear Approximations\n\nBased on classical PCA\nPCA performed on a weighted covariance matrix, with weights decaying with distance from any reference point\nApproximates data locally on a curved manifold around a point\nReduces dimension locally but provides no global representation\n\n\n\nPrincipal Curves and Surfaces\n\nData assumed to be of the form \\(x_i = x^*_i + \\epsilon\\)\nThe Subspace Constrained Mean Shift (SCMS) algorithm of Ozertem & Erdogmus (2011) iteratively maps each \\(x_i\\) to \\(y_i \\in R^D\\) lying on the principal curve\nMethod can be extended to principal surfaces\n\n\n\n\n\n\nEmbeddings\nMeilă and Zhang propose a provisional taxonomy of embedding algorithms, which they concede is superficial but which adequately characterizes the state of the art. All approaches begin with information about the data summarized in a weighted neighborhood graph. An embedding algorithm then produces a smooth mapping that is designed to distort the neighborhood information as little as possible. The algorithms differ in their choice of information they preserve and in the constraints on smoothness. The fundamental categories of embedding algorithms are:\n\n“One-shot” algorithms that derive embedding coordinates from principal eigenvectors of a matrix associated with the neighborhood graph of a data set or by solving an optimization problem.\nAttraction-repulsion algorithms that proceed from an initial embedding, often produced by iterative improvements of a one-shot algorithm.\n\n\nOne-Shot Embedding Algorithms\nOne-Shot algorithms include:\n\nDiffusion Maps (DM): Coifman & Lafon (2006) which uses the eigenvectors of the Laplacian matrix to embed the data.\nISOMAP - Tenenbaum et al. (2000)\n\nPreserves shortest graph paths\n\nLaplacian Eigenmaps - Belkin & Niyogi (2003)\nLocal Transient Space Alignment (LTSA) - Zhang & Zha (2004)\n\n\n\nAttraction-Replusion Embedding Algorithms\nAttraction-Repulsion Algorithms include:\n\nLow Distortion Local Embeddings (LDLE) Lohli, Cloninger, and Mishne (2021)\nMaximum Variance Unfolding (MVU) Weinberger and Saul (2006)\nStochastic Neighbor Embedding (SNE) Hinton and Roweis (2002)\nt-SNE Van der Maaten and Hinton (2008)\nUniform Manifold Approximation and Projection (UMAP) MCinnes et al (2018)\n\n\n\n\n\n\nSNE and t-SNE\nThis section briefly describes the SNE algorithm and its improved variation t-SNE, which were designed to visualize high dimensional data to a two or three-dimensional space.\n\nSNE\nThe intuition behind Stochastic Neighbor Embedding (SNE), described in the paper by Hinton & Roweis (2002), is to emphasize local distances and employ a cost function that enforces both keeping the images of nearby objects nearby and keeping the images of widely separated objects relatively far apart.\nMost embedding methods require each high-dimensional data point to be associated with only a single location in the low-dimensional space, making it difficult to unfold “many-to-one” mappings in which a single ambiguous object really belongs in several disparate locations in the low-dimensional space. SNE tries to place high-dimensional data points in a low-dimensional space so as to optimally preserve neighborhood identity and allow multiple different low-dimensional images. For example, because of its probabilistic formulation, SNE has the ability to be extended to mixtures in which ambiguous high-dimensional objects such as the word “bank” can be associated with several widely-separated images (e.g., both “river” and “finance”) in the low-dimensional space.\nThe basic idea underlying SNE is to construct a Gaussian probability distribution \\(P_i\\) over each point, \\(x_i\\), in the high dimensional space that describes the conditional probability \\(p_{j|i}\\) that i would pick j as its neighbor. \\[p_{j|i} =  exp(-\\| x_i - x_j\\|^2/2\\sigma_i^2) \\sum_{k \\neq i} exp(-\\| x_i - x_k\\|^2 / 2\\sigma_i^2)\\]\nThen, find a similar distribution, \\(Q_i\\), over the points in the points \\(y_i\\) in the low dimensional space to which the \\(x_i\\) are mapped. If, for all i, \\(p_{i|j}=q_{i|j}\\), then the similarities will be preserved. The \\({y_i}\\) points are found by using gradient descent to minimize the sum of all the Kullback-Liebler divergences using the cost function: \\[C = \\sum_i KL(P_i \\| Q_i) = \\sum_i \\sum_j p_{j|i}log(p_{j|i}/q_{j|i)}\\]\nIn the high dimensional space, the \\(\\sigma_i\\) values are selected by performing a binary search for the \\(\\sigma_i\\) that produces a \\(P_i\\) with a fixed perplexity specified by the user. \\(Perp(P_i) = 2^{H(P_i)}\\) where \\({H(P_i)} = -\\sum_j p_{j|i}log_2 p_{j|i}\\) is the Shannon entropy measured in bits. The \\(\\sigma_i\\) for the low dimensional space are set to \\(1/\\sqrt2\\).\n\n\nt-SNE\nThe t-SNE algorithm, described in van der Maaten and Hinton (2008), is an improvement on SNE that overcomes several technical difficulties. The main differences from SNE are that (1) t-SNE uses a symmetric version of the cost function, which has a simpler gradient, and (2) t-SNE uses a t-distribution with one degree of freedom for the points in the low dimensional space. These help overcome optimization problems and mitigate the effect of the Crowding Problem in which the area available in the low dimensional map to accommodate moderately distant data points will not be sufficient.\nVan der Maaten and Hinton point out that in both SNE and t-SNE,\n\n“…the gradient may be interpreted as the resultant force created by a set of springs between the map points \\(y_i\\) and \\(y_j\\) . . . The spring between \\(y_i\\) and \\(y_j\\) repels or attracts the map points depending on whether distance between the two in the map is too small or too large to represent the similarities between the two high dimensional points.”\n\nThe final result, t-SNE, is an algorithm that preserves local similarities between points while preserving enough of the global structure to recognize clusters. The art of both these algorithms comprises not only marshaling appropriate mathematics to realize intuitive geometric ideas about data relationships, but also in working through the many technical difficulties and optimization problems to provide reasonable performance."
  },
  {
    "objectID": "posts/manifold-learning/index.html#r-examples",
    "href": "posts/manifold-learning/index.html#r-examples",
    "title": "Manifold Learning",
    "section": "R Examples",
    "text": "R Examples\nThis section provides examples of using the tsne package to compute two and three-dimensional embeddings on two different data sets, the palmerpenguins and the AMES Housing Data. Note because it takes several minutes to fit the models on my underpowered MacBook Air, the code below shows how to run the tsne() command, but actually reads in the model fit from an RDS file.\n\n\n\nExample 1: The Penguins\nFor our first example, let’s look at the penguins data set from the palmerpenguins package. It has six variables we can use to feed the t-SNE algorithm, and we know that we would like any clusters identified to correspond with the three species of penguins.\n\n\nShow the code\ndf_p &lt;- palmerpenguins::penguins\n# glimpse(df_p)\n\n\nPrepare data frames for fitting the model and then for subsequent plotting.\n\n\nShow the code\ndf_p_fit &lt;- df_p |&gt; \n  mutate(island = as.integer(island), sex = as.integer(sex)) |&gt;\n  select(c(-year, -species)) |&gt; \n  na.omit()\n\ndf_p_plot &lt;- df_p |&gt; select(-year) |&gt; na.omit() \n\n\nFit the t-sne model.\n\n\nShow the code\n# fit_pen &lt;- tsne(df_p_fit, epoch_callback = NULL, perplexity=50)\nfit_pen &lt;- readRDS(\"fit_pen_2D\")\n\n\nNext, we add the coordinates fit by the model to our plotting data frame and plot. As we would expect, the clusters identified by t-SNE line up very nicely with the penguin species, and island. All of the Gentoo are on Biscoe Island.\n\n\nShow the code\ndf_p_plot &lt;- df_p_plot |&gt; mutate(x = fit_pen[, 1], y = fit_pen[, 2])\n\ndf_p_plot |&gt; ggplot(aes(x, y, colour = species, shape = island)) +\n  geom_point() +\n  scale_color_manual(values = c(\"blue\", \"red\", \"green\")) +\n  ggtitle(\"2D Embedding of Penguins Data\")\n\n\n\n\n\n\n\n\n\nHere is a projection onto a three-dimensional space.\n\n\nShow the code\n# fit_pen_3D = tsne(df_p_fit, epoch_callback = NULL, perplexity=50, k=3)\nfit_pen_3D &lt;- readRDS(\"fit_pen_3D\")\n\n\nThe threejs visualization emphasizes the single Chinstrap observation floating in space near the Adelle clusters and the two Gentoos reaching the edge of Chinstrap Island.\n\n\nShow the code\nx &lt;- fit_pen_3D[,1] \ny &lt;- fit_pen_3D[,2]\nz &lt;- fit_pen_3D[,3]\n\n\ndf_p_plot &lt;- df_p_plot |&gt; mutate(\n  color = if_else(species == \"Adelie\", \"blue\", \n                  if_else( species == \"Gentoo\",\"green\", \"red\")))\n\nscatterplot3js(x,y,z, color=df_p_plot$color, cex.symbols = .3, \n               labels = df_p_plot$species)\n\n\n\n\n\n\n\n\n\n\n\nExamble 2: The Ames Housing Dataset\nWith 74 variables, the AMES Housing Data provides a convincing display of of the usefulness of the t-sne algorithm.\n\n\nShow the code\ndata(ames, package = \"modeldata\")\n#glimpse(ames)\n\n\nPrepare ames for processing with tsne by changing all factors to numeric data and fit the model.\n\n\nShow the code\ndf_ames &lt;- ames %&gt;% mutate_if(is.factor, as.numeric)\n#fit_ames &lt;- tsne(df_ames, epoch_callback = NULL, perplexity=50)\nfit_ames &lt;- readRDS(\"tsne_fit_ames\")\n#head(fit_ames)\n\ndf_ames_plot &lt;- ames |&gt; mutate(x = fit_ames[,1], y = fit_ames[,2],\n                               MS_Zoning = as.character(MS_Zoning),\n                               MS_Zoning = replace(MS_Zoning, MS_Zoning == \"C_all\", \"C or I\"),\n                               MS_Zoning = replace(MS_Zoning, MS_Zoning == \"I_all\" , \"C or I\")\n                               )\n                              \n                               \n             \n            \ndf_ames_plot |&gt; ggplot(aes(x,y, shape = MS_Zoning, color = Neighborhood)) + \n                geom_point() +\n                guides(color = FALSE, size = \"none\") +\n                ggtitle(\"2D Embedding of Ames Data Colored by Neighborhood\")\n\n\n\n\n\n\n\n\n\nHere is the projection onto a three-dimensional space that is also colored by Neighborhood. It shows the separation among the clusters which appear to reside in a three-dimensional ellipsoid. As was the case with the two-dimensional plot, neighborhoods appear to be fairly well mixed among the clusters.\n\n\nShow the code\nset.seed(1234)\n\n#fit_ames_3D &lt;- tsne(ames, epoch_callback = NULL, perplexity=50,k=3)\nfit_ames_3D &lt;- readRDS(\"tsne_fit_ames_3D\")\n#head(fit_ames_3D)\n\ndf_plot_ames_3D &lt;- df_ames |&gt; mutate(\n  x = fit_ames_3D[, 1],\n  y = fit_ames_3D[, 2],\n  z = fit_ames_3D[, 3],\n  neighborhood = ames$Neighborhood,\n  zone = ames$MS_Zoning\n)\n\nx &lt;- fit_ames_3D[, 1]\ny &lt;- fit_ames_3D[, 2]\nz &lt;- fit_ames_3D[, 3]\n\nscatterplot3js(x, y, z, cex.symbols = .1, col = rainbow(length(df_plot_ames_3D$Neighborhood)))\n\n\n\n\n\n\n\n\n\n\nR Packages\nThe following is a short list of R packages that may be helpful for Manifold Learning.\ncml vo.2.2: Finds a low-dimensional embedding of high-dimensional data, conditioning on available manifold information. The current version supports conditional MDS (based on either conditional SMACOF in Bui (2021) or closed-form solution in Bui (2022) and conditional ISOMAP in Bui (2021).\ndyndimred v1.0.4: Provides a common interface for applying dimensionality reduction methods, such as Principal Component Analysis (‘PCA’), Independent Component Analysis (‘ICA’), diffusion maps, Locally-Linear Embedding (‘LLE’), t-distributed Stochastic Neighbor Embedding (‘t-SNE’), and Uniform Manifold Approximation and Projection (‘UMAP’). Has built-in support for sparse matrices.\nhydra v0.1.0: Calculate an optimal embedding of a set of data points into low-dimensional hyperbolic space. This uses the strain-minimizing hyperbolic embedding of Keller-Ressel and Nargang (2019).\nmatrixLaplacian v1.0: Constructs the normalized Laplacian matrix of a square matrix, returns the eigenvectors (singular vectors) and visualization of normalized Laplacian map.\nphateR v1.0.7: Implements a novel conceptual framework for learning and visualizing the manifold inherent to biological systems in which smooth transitions mark the progressions of cells from one state to another.\nRiemann v0.1.4: provides a variety of algorithms for manifold-valued data, including Fréchet summaries, hypothesis testing, clustering, visualization, and other learning tasks. See Bhattacharya and Bhattacharya (2012) for general exposition of statistics on manifolds. See the vignette.\nRtsne v:0.17: Provides a R wrapper around the fast T-distributed Stochastic Neighbor Embedding implementation by Van der Maaten.\nspectralGraphTopology v0.2.3: Learning Graphs from Data via Spectral Constraints, It provides implementations of state-of-the-art algorithms such as Combinatorial Graph Laplacian Learning (CGL), Spectral Graph Learning (SGL), Graph Estimation based on Majorization-Minimization (GLE-MM), and Graph Estimation based on Alternating Direction Method of Multipliers (GLE-ADMM). See the vignette.\ntsne v0.1-3.1: A “pure R” implementation of the t-SNE algorithm.\numap v0.2.10.0: Implements the Uniform manifold approximation and projection algorithm for dimension reduction, which was described by McInnes and Healy (2018). See the vignette.\nuwot v0.2.2: An implementation of the Uniform Manifold Approximation and Projection dimensionality reduction by McInnes et al. (2018). It also provides means to transform new data and to carry out supervised dimensionality reduction. An implementation of the related LargeVis method of Tang et al. (2016) is also provided. See the uwot website(https://github.com/jlmelville/uwot&gt;) for more documentation and examples."
  },
  {
    "objectID": "posts/august-2024-top-40-new-cran-packages/index.html",
    "href": "posts/august-2024-top-40-new-cran-packages/index.html",
    "title": "August 2024: Top 40 New CRAN Packages",
    "section": "",
    "text": "“Top 40” is back, broadcasting on the new R Works blog. I hope to continue the monthly evaluation of R packages that ran for several years on RStudio’s R Views Blog. The following is an idiosyncratic selection of the forty best new R packages submitted to CRAN in August 2024 organized into fourteen categories: Artificial Intelligence, Computational Methods, Data, Ecology, Environment, Genomics, Machine Learning, Medicine, Pharma, Science, Statistics, Time Series, Utilities, and Visualization.\n\nArtificial Intelligence\ngemini.R v0.5.2: Provides an R interface to Google Gemini API for advanced language processing, text generation, and other AI-driven capabilities within the R environment. See README to get started.\npromptr v1.0.0: Provides functions to form and submit prompts to OpenAI’s Large Language Models. Designed to be particularly useful for text classification problems in the social sciences. See Ornstein, Blasingame, & Truscott (2024) for details and README for an example.\n\n\n\n\n\n\n\nComputational Methods\nqvirus v 0.0.2: Provides code and resources to explore the intersection of quantum computing and artificial intelligence (AI) in the context of analyzing Cluster of Differentiation 4 (CD4) lymphocytes and optimizing antiretroviral therapy (ART) for human immunodeficiency virus (HIV). See the vignettes Introduction, Applications, and Entanglement.\nRcppBessel v1.0.0: Exports an Rcpp interface for the Bessel functions in the ‘Bessel’ package, which can then be called from the C++ code of other packages. For the original ‘Fortran’ implementation of these functions, see Amos (1995). There is a vignette.\n\n\nData\naebdata v0.1.0: Facilitates access to the data from the Atlas do Estado Brasileiro maintained by the Instituto de Pesquisa Econômica Aplicada (Ipea). It allows users to search for specific series, list series, or themes, and download data when available. See the vignette.\ncapesData v0.0.1: Provides information on activities to promote scholarships in Brazil and abroad for international mobility programs recorded in the CAPES database from 2010 to 2019. See README to get started.\n\n\nEcology\npriorCON v0.1.1: Provides a tool set that incorporates graph community detection methods into systematic conservation planning. It is designed to enhance spatial prioritization by focusing on the protection of areas with high ecological connectivity and on clusters of features that exhibit strong ecological linkages. See the Introduction.\n\n\n\n\n\n\n\nEnvironment\nprior3D v0.1.0: Offers a comprehensive toolset for 3D systematic conservation planning, conducting nested prioritization analyses across multiple depth levels and ensuring efficient resource allocation throughout the water column. See Doxa et al. (2022) for background and the vignette for an example.\n\n\n\n\n\nraem v0.1.0: Implements a model of single-layer groundwater flow in steady-state under the Dupuit-Forchheimer assumption can be created by placing elements such as wells, area-sinks, and line-sinks at arbitrary locations in the flow field. See Haitjema (1995) for the underlying theory and the vignettes Overview and Exporting spatial data.\n\n\n\n\n\n\n\nGenomics\nkmeRtone v 1.0: Provides functions for multi-purpose k-meric enrichment analysis, which measures the enrichment of k-mers by comparing the population of k-mers in the case loci with an internal negative control group consisting of k-mers from regions close to, yet sufficiently distant from, the case loci. This method captures both the local sequencing variations and broader sequence influences while also correcting for potential biases. See the GitHub repo for an overview.\nrYWAASB v0.1: Provides a new ranking algorithm to distinguish the top-ranked genotypes. “WAASB” refers to the “Weighted Average of Absolute Scores” provided by Olivoto et al. (2019), which quantifies the stability of genotypes across different environments using linear mixed-effect models. See the vignette for an example.\n\n\n\n\n\n\n\nMachine Learning\ncvLM v1.0.4: Provides efficient implementations of cross-validation techniques for linear and ridge regression models, leveraging C++ code with Rcpp that supports leave-one-out, generalized, and K-fold cross-validation methods. See README for an example.\ngeodl v0.2.0: Provides tools for semantic segmentation of geospatial data using convolutional neural network-based deep learning, including utility functions for manipulating data, model checks, functions to implement a UNet architecture with four blocks in the encoder, assessment metrics, and more. The package relies on torch but does not require installing a Python environment. Models can be trained using a Compute Unified Device Architecture (CUDA)-enabled graphics processing unit (GPU). There are ten vignettes, including spatialPredictionDemo and topoDLDemo.\n\n\n\n\n\nidiolect v1.0.1: Provides functions for the comparative authorship analysis of disputed and undisputed texts within the Likelihood Ratio Framework for expressing evidence in forensic science and implements well-known algorithms, including Smith and Aldridge’s (2011) Cosine Delta and Koppel and Winter’s (2014) Impostors Method. See the vignette.\n\n\n\n\n\nkdml v1.0.0: Implements distance metrics for mixed-type data consisting of continuous, nominal, and ordinal variables, which can be used in any distance-based algorithm, such as distance-based clustering. See Ghashti and Thompson (2024) for dkps() methodology, Ghashti (2024) for dkss() methodology, and the vignette.\nkerntools v1.0.2: Provides kernel functions for diverse types of data including, but not restricted to: non-negative and real vectors, real matrices, categorical and ordinal variables, sets, strings, plus other utilities like kernel similarity, kernel Principal Components Analysis (PCA) and features’ importance for Support Vector Machines (SVMs). See the vignette.\n\n\n\nScatter plot for Drac kernal PCA\n\n\nMorphoRegions v0.1.0: Provides functions to computationally identify regions in serially homologous structures such as, but not limited to, the vertebrate backbone. Regions are modeled as segmented linear regressions, with each segment corresponding to a region and region boundaries (or breakpoints) corresponding to changes along the serially homologous structure.\n\n\n\n\n\n\n\nMedicine\nneuroUP v0.3.1: Provides functions to calculate the precision in mean differences (raw or Cohen’s D) and correlation coefficients for different sample sizes using permutations of the collected functional magnetic resonance imaging (fMRI) data. See Klapwijk et al. (2024) for background and the vignette for an introduction.\n\n\n\n\n\nsmiles v0.1-0: Provides tools aimed at making data synthesis and evidence evaluation easier for both experienced practitioners and newcomers. See the Cochrane Handbook for Systematic Reviews of Interventions and the vignette for examples.\n\n\n\n\n\ntsgc v0.0: Provides tools to analyze and forecast epidemic trajectories based on a dynamic Gompertz model and a state space approach that uses the Kalman filter for robust estimation of the non-linear growth pattern commonly observed in epidemic data. See Harvey and Kattuman (2020), Harvey and Kattuman (2021), and Ashby et al. (2024) for background and the vignette for details.\n\n\n\n\n\n\n\nPharma\nadmiralpeds v0.1.0: Provides a toolbox for programming Clinical Data Standards Interchange Consortium (CDISC) compliant Analysis Data Model (ADaM) data sets in R. See the vignette for an example.\nMALDIcellassay v0.4.47: Implements tools to conduct automated cell-based assays using Matrix-Assisted Laser Desorption/Ionization (MALDI) methods for high-throughput screening of signals responsive to treatments. The methodologies were introduced by Weigt et al. (2018) and Unger et al. (2021). See the vignette for an example.\n\n\n\n\n\n\n\nScience\nbarrks v1.0.0: Implements models to calculate the bark beetle phenology and their submodels, onset of infestation, beetle development, diapause initiation, and mortality, which can be customized and combined. Models include PHENIPS-Clim, PHENIPS, RITY, CHAPY, and BSO. There are five vignettes, including The BSO model and Example: Model Comparison.\n\n\n\n\n\nfluxible v0.0.1: Provides functions to process the raw data from closed loop flux chamber (or tent) setups into ecosystem gas fluxes usable for analysis. Implemented models include exponential Zhao et al. (2018) and quadratic and linear models to estimate the fluxes from the raw data. See the vignette for an example.\n\n\nStatistics\nbage v0.7.4: Provides functions for Bayesian estimation and forecasting of age-specific rates, probabilities, and means based on the Template Model Builder. There are six vignettes, including the Mathematical Details and an Example.\n\n\n\n\n\nclustMC v0.1.1: Implements cluster-based multiple comparisons tests and also provides a visual representation in the form of a dendrogram. See Rienzo, Guzmán & Casanoves (2002) and Bautista, Smith & Steiner (1997), and the vignette for examples.\n\n\n\n\n\nsvycdiff v0.1.1: Provides three methods for estimating the population average controlled difference for a given outcome between levels of a binary treatment, exposure, or other group membership variables of interest for clustered, stratified survey samples where sample selection depends on the comparison group. See Salerno et al. (2024) for background and the vignette for an example.\nwishmom v1.1.0: Provides functions for computing moments and coefficients related to the Beta-Wishart and Inverse Beta-Wishart distributions, including functions for calculating the expectation of matrix-valued functions of the Beta-Wishart distribution, coefficient matrices, and expectation of matrix-valued functions of the inverse Beta-Wishart distribution. See the vignette for details.\n\n\nTime Series\ntican v1.0.1: Provides functions to analyze and plot time-intensity curves such as those that arise from contrast-enhanced ultrasound images. See the vignette.\ntidychangepoint v0.0.1: Provides a tidy, unified interface for several different changepoint detection algorithms, along with a consistent numerical and graphical reporting leveraging the broom and ggplot2 packages. See the vignette.\n\n\n\n\n\n\n\nUtilities\nfio v0.1.2: Provides tools to simplify the process of importing and managing input-output matrices from Microsoft Excel into R. It leverages the R6 class for memory-efficient object-oriented programming implements all linear algebra computations in Rust. See the vignette.\nlitedown v0.2: Implements a lightweight version of R Markdown, which enables rendering R Markdown to Markdown without using knitr, and Markdown to lightweight HTML/LaTeX documents using the commonmark package instead of Pandoc. This package can be viewed as a trimmed-down version of R Markdown and knitr, which does not aim at rich Markdown features or a large variety of output formats. There are vignettes on Markdown Examples, HTML Output Examples, and Making HTMLSlides.\nmaestro v0.2.0: Implements a framework for creating and orchestrating data pipelines allowing users to organize, orchestrate, and monitor multiple pipelines in a single project. There are four vignettes, including a Quick Start Guide and Use Cases.\nosum v0.1.0: Inspired by S-PLUS function objects.summary(), provides a function that returns data class, storage mode, mode, type, dimension, and size information for R objects in the specified environment. Various filtering and sorting options are also proposed. See the vignette.\novertureR v0.2.3: Implements n integrated R interface to the Overture’ API which allows R users to return Overture data as dbplyr data frames or materialized sf spatial data frames. See README for examples.\n\n\n\n\n\nRcppMagicEnum v0.0.1: Provides Rcpp bindings to header-only modern C++ template library Magic Enum. See README to get started.\n\n\n\n\n\ntidymodelr v1.0.0: Provides a function to transform long data into a matrix form to allow for ease of input into modeling packages for regression, principal components, imputation, or machine learning along with level analysis wrapper functions for correlation and principal components analysis. See README for examples.\n\n\nVisualization\nbullseye v0.1.0: Provides a tidy data structure and visualizations for multiple or grouped variable correlations, general association measures, diagnostics, and other pairwise scores suitable for numerical, ordinal, and nominal variables. Supported measures include distance correlation, maximal information, ace correlation, Kendall’s tau, and polychoric correlation. There are three vignettes including Calculating Pairwise Scores and Visualizing Pairwise Scores.\n\n\n\n\n\nflowmapper v0.1.2: Adds flow maps to ggplot2 plots. These are layers that visualize the nodes as circles and the bilateral flows between the nodes as bidirectional half-arrows. Look here for details and examples.\n\n\n\n\n\nggreveal v0.1.3: Provides functions that make it easy to reveal ggplot2 graphs incrementally. The functions take a plot produced with ggplot2 and return a list of plots showing data incrementally by panels, layers, groups, the values in an axis, or any arbitrary aesthetic. See the GitHub repo for examples."
  },
  {
    "objectID": "posts/september-2024-top-40-new-cran-packages/index.html",
    "href": "posts/september-2024-top-40-new-cran-packages/index.html",
    "title": "September 2024: Top 40 New CRAN Packages",
    "section": "",
    "text": "Two hundred thirty new packages made it to CRAN in September, many of them were interesting, and selecting only forty made for some difficult decisions. When there a difficult choice, I opted in favor of the sciences. Here are my picks for the Top 40 new packages in fifteen categories: AI, Archaeology, Biology, Computational Methods, Data, Genomics, Linguistics, Machine Learning, Medicine, Networks, Pharma, Physics, Statistics, Utilities, and Visualization.\n\nAI\ngroqR v0.0.1: Provides a suite of functions and RStudio Add-ins leveraging the capabilities of open-source Large Language Models (LLMs) to support R developers. Features include text rewriting, translation, and general query capabilities. Programming-focused functions provide assistance with debugging, translating, commenting, documenting, and unit testing code, as well as suggesting variable and function names. Look here for examples.\n\n\nArchaeology\neratosthenes v0.0.2: Estimates unknown historical or archaeological dates subject to relationships with other dates and absolute constraints, derived as marginal densities from the full joint conditional distribution. Includes rule-based estimation of the production dates of artifact types. See Collins-Elliott (2024) for background and the vignettes on aligning relative sequences and gibbs sampling for archaeology dates.\n\n\nBiology\npcvr v1.0.0: Provides functions to analyse common types of plant phenotyping data and a simplified interface to longitudinal growth modeling and select Bayesian statistics. See Kruschke (2018), Kruschke (2013) and Kruschke (2021) for background on the Bayesian methods. There are four vignettes including Bellwether workflow and Longitudinal Growth Modeling.\n\n\n\n\n\northGS v0.1.5: Provides tools to analyze and infer orthology and paralogy relationships between glutamine synthetase proteins in seed plants. See the vignettes Searching for Orthologous and Unraveling the Hiden Paralogous.\n\n\n\n\n\n\n\nChemistry\nMethodOpt v1.0.0: Implements a GUI to apply an advanced method optimization algorithm to various sampling and analysis instruments. Functions include generating experimental designs, uploading and viewing data, and performing various analyses to determine the optimal method. See Granger & Mannion (2024) for details and the vignette for examples.\nSCFMonitor v0.3.5: Self-Consistent Field(SCF) calculation method is one of the most important steps in the calculation methods of quantum chemistry. See Ehrenreich & Cohen (1959) . This package enables Gaussian quantum chemistry calculation software users to easily read the Gaussian .log files and monitor the SCF convergence and geometry optimization process. Look here for examples.\n\n\n\n\n\n\n\nComputational Methods\nXDNUTS v1.2: Implements Hamiltonian Monte Carlo for both continuous and discontinuous posterior distributions with customisable trajectory length termination criterion. See Nishimura et al. (2020) for the original Discontinuous Hamiltonian Monte Carlo; and Hoffman et al. (2014) and Betancourt (2016) possible Hamiltonian Monte Carlo termination criteria. The vignette offers examples.\n\n\n\n\n\n\n\nData\nclintrialx v0.1.0: Provides functions to fetch clinical trial data from sources like [ClinicalTrials.gov](https://clinicaltrials.gov/} and the Clinical Trials Access to Aggregate Content database that supports pagination and bulk downloads. See the vignette.\n\n\n\n\n\nColOpenData v0.3.0: Provides tools to download and wrangle Colombian socioeconomic, geospatial,population and climate data from DANE at the National Administrative Department of Statistics and IDEAM at the Institute of Hydrology, Meteorology and Environmental Studies. It solves the problem of Colombian data being issued in different web pages and sources by using functions that allow the user to select the desired database and download it without having to do the exhausting acquisition process. There are six vignettes including How to download climate data and Population Projections.\n\n\n\n\n\nmodgo v1.0.1: Provides functions to generate synthetic data from a real dataset using the combination of rank normal inverse transformation with the calculation of correlation matrix and completely artificial data may be generated through the use of Generalized Lambda Distribution and Generalized Poisson Distribution. See the vignette.\n\n\n\n\n\ndtmapi v0.0.2: Provides functions to allow humanitarian community, academia, media, government, and non-governmental organizations to utilize the data collected by the Displacement Tracking Matrix, a unit in the International Organization for Migration. See the vignette to get started.\n\n\nEcology\ndouconca v1.2.1: Implements the two step double constrained correspondence analysis (dc-CA) for analyzing multi-trait multi-environment ecological data described inter Braak et al. (2018). This algorithm combines and extends community or sample and species-level analyses.\n\n\n\n\n\nGeoThinneR v1.1.0: Provides efficient geospatial thinning algorithms to reduce the density of coordinate data while maintaining spatial relationships. Implements K-D Tree and brute-force distance-based thinning, as well as grid-based and precision-based thinning methods. See Elseberg et al. (2012) for background and the vignette for examples.\n\n\n\n\n\n\n\nGenomics\neasybio v1.1.0: Provides a toolkit for single-cell annotation with the CellMarker2.0 database and streamlines biological label assignment in single-cell RNA-seq data and facilitates transcriptomic analysis, including preparation of TCGA and GEO datasets, differential expression analysis and visualization of enrichment analysis results. See Wei Cui (2024) for details and the two vignettes bulk RNAsewuence workflow and Single Cell Annotation for examples.\n\n\n\n\n\nGenoPop v0.9.3: Implements tools for efficient processing of large, whole genome genotype data sets in variant call format including several functions to calculate commonly used population genomic metrics and a method for reference panel free genotype imputation. See Gurke & Mayer (2024) for background and the vignette to get started. \nSuperCell v1.0: Provides tools to aggregate large single-cell data into metacell dataset by merging together gene expression of very similar cells See the vignettes Example of SuperCell pipeline and SuperCell runs for different samples.\n\n\n\n\n\n\n\nLingusitics\nmaxent.ot v1.0.0: Provides tools to fit Maximum Entropy models to phonology data. See Mayer, Tan & Zuraw and the vignette for an overview.\n\n\nMachine Learning\nconversim v0.1.0: Provides tools to analyze and compare conversations using various similarity measures including topic, lexical, semantic, structural, stylistic, sentiment, participant, and timing similarities. Methods are based on established research: For example see Landauer et al. (1998) Jaccard (1912) and Salton & Buckley (1988). Thee are four vignettes including analyzing similarities between two long speaches and analyzing similarities in conversational sequence in one Dyad and across multiple Dyads.\ndsld v0.2.2: Provides statistical and graphical tools for detecting and measuring discrimination and bias, be it racial, gender, age or other. Detection and remediation of bias in machine learning algorithms. See the Quick Start Guide.\n\n\nMedicine\nSurvMA v1.6.8: Implements a model averaging approach to predict personalized survival probabilities by using a weighted average of multiple candidate models to approximate the conditional survival function.Two scenarios of candidate models are allowed: the partial linear Cox model and the time-varying coefficient Cox model. See Wang (2023) for details and look here for an example.\n\n\n\n\n\nwintime v0.2.0: Provides methods to perform an analysis of time-to-event clinical trial data using various methods that calculate and compare treatment effects on ordered composite endpoints. See Troendle et al. (2024) for the details of the methods and the vignette for examples.\n\n\nNetworks\narlclustering v1.0.5: Implements an innovative approach to community detection in social networks using Association Rules Learning providing tools for processing graph and rules objects, generating association rules, and detecting communities based on node interactions. See El-Moussaoui et al. (2021) for details. There are eight vignettes including General Introduction and Testing WordAdjacency dataset.\nggtangle v0.0.2: Extends the ggplot2 plotting system to support network visualization for network associated data. See the vignette.\n\n\n\n\n\n\n\nPharma\nsdtm.oak v0.1.0: Provides a framework to develop CDISC, SDTM datasets in R and potentially automate the process. There are six vignettes including on on Algorithms.\n\n\n\n\n\n\n\nPhysics\nrice v0.3.0: Provides functions to calibrate radiocarbon dates, different radiocarbon realms (C14 age, F14C, pMC, D14C) and to estimate the effects of contamination or local reservoir offsets. See Reimer and Reimer 2001 and Stuiver and Polach (1977) for background and the vignette for examples.\n\n\n\n\n\nSTICr v1.0: Comprises a collection of functions for processing raw data from Stream Temperature, Intermittency, and Conductivity (STIC) loggers. ‘STICr’ (pronounced “sticker”) that includes functions for tidying, calibrating, classifying, and doing quality checks on data from STIC sensors. See Wheeler/Zipper et al. (2023) for background and the vignette for an Introduction.\n\n\n\n\n\n\n\nStatistics\ndpasurv v0.1.0: Provides functions to implement dynamic path analysis for survival data via Aalen’s additive hazards model. See Fosen et al., (2006) for details. There is an oveview and a vignette on plotting with ggplot2.\n\n\n\n\n\nLearnVizLLM v1.0.0: Implements tools to summarize the characteristics of linear fixed models without data or a fitted model by converting code code for fitting nlme::lme() and lme4::lmer() models into tables, equations, and visuals. See the vignette for details.\n\n\n\n\n\nlnmixsurv v3.1.6: Combines the mixture distributions of Fruhwirth-Schnatter(2006) and the data augmentation techniques of Tanner and Wong (1987) to implement Bayesian Survival models that accommodate different behavior over time and consider higher censored survival times. There are five vignettes including a [Get started guide}\nPath.Analysis v0.1: Provides functions for conducting sequential path coefficient analysis and testing direct effects and functions for estimating correlation, drawing correlograms, heatmaps, and path diagrams. See Arminian et al. (2008) for background and the vignette for examples.\n\n\n\n\n\n\n\nUtilities\ncharcuterie v0.0.4: Creates a new chars class which looks like a string but is actually a vector of individual characters, making strings iterable and enabling vector operations on ‘strings’ such as reverse, sort, head, and set operations. See the vignettes Example Usage and Use Cases.\n\n\n\n\n\ndtreg v1.0.0: Provides tools to interact with data type registries and create machine-readable data. See the vinette.\nfctutils v0.0.7: Provides a collection of utility functions for manipulating and analyzing factor vectors in R. It offers tools for filtering, splitting, combining, and reordering factor levels based on various criteria. See the vignette.\ninterface v0.1.2: Provides a run time type system, allowing users to define and implement interfaces, enums, typed data.frame/data.table, as well as typed functions. This package enables stricter type checking and validation, improving code structure, robustness and reliability. There is a vignette and a way to support the author.\npikchrV0.97 : Provides an interface to pikchr q markup language for creating diagrams within technical documentation. See the vignette for examples.\n\n\n\n\n\nrnix v0.12.4: Provides tools to run the nix package manager. There are fifteen vignettes including a Getting Started Gude.\nqs2 v0.1.1: Provides tools to efficiently serialize R objects using one of two compression formats: the qs2 format, which uses R serialization while optimizing compression and disk I/O, and the qdata format which uses custom serialization to achieve slightly faster performance and better compression. qs2 format can be directly converted to the standard RDS. See the vignette\n\n\nVisualization\nggalign v0.0.4: Implements an extension to ggplot2 that offers various tools for organizing and arranging plots including the ability to consistently align a specific axis across multiple ggplot objects. There are seven vignettes including Examples and Heatmap Layout.\n\n\n\n\n\nsfcurv v1.0: Implements all possible forms of 2x2 and 3x3 space-filling curves, i.e., the generalized forms of the Hilbert curve, the Peano curve and the Peano curve in the meander type. Look here for examples.\n\n\n\n\n\nsurreal v0.0.1: Implements the Residual (Sur)Realism algorithm described by Stefanski (2007) to generate datasets that reveal hidden images or messages in their residual plots. See README for examples.\n\n\n\n\n\nsurvSAKK v1.3.1: Provides functions to incorporate various statistics and layout customization options to enhance the efficiency and adaptability of the Kaplan-Meier plots. See the vignette."
  },
  {
    "objectID": "posts/100bushels/index.html",
    "href": "posts/100bushels/index.html",
    "title": "100 Bushels of Corn",
    "section": "",
    "text": "About the author\n\n\n\nNina Zumel is a data scientist based in San Francisco, with 20+ years of experience in machine learning, statistics, and analytics. She is the co-founder of the data science consulting firm Win-Vector LLC, and (with John Mount) the co-author of Practical Data Science with R, now in its second edition.\nI was browsing the December, 1908 issue of The Strand Magazine (it’s related to a hobby of mine), when I came across an article called “The World’s Best Puzzles”, by Henry Dudeney, who seems to have been the Martin Gardner of his day. Here’s a cool puzzle from that article, which according to Dudeney was first recorded by Alcuin, Abbot of Canterbury (735-804). I assume it’s from his manuscript Propositiones ad Acutendos Juvenes (Problems to Sharpen Youths)."
  },
  {
    "objectID": "posts/100bushels/index.html#the-puzzle",
    "href": "posts/100bushels/index.html#the-puzzle",
    "title": "100 Bushels of Corn",
    "section": "The Puzzle",
    "text": "The Puzzle\n\n100 bushes of corn are distributed to 100 people such that every man receives 3 bushels, every woman 2 bushels, and every child 1/2 a bushel. How many men, women, and children are there?\n\nThere are seven solutions; Dudeney gives one: 20 men, 0 women, and 80 children. Can you find the other six?\nLet’s put the puzzle into algebra, so it’s easier to discuss.\n\\[\n\\begin{aligned}\nm + w + c &= 100 \\\\\n3m + 2w + 0.5c &= 100 \\\\\n\\end{aligned}\n\\]\nSolve for \\(m\\), \\(w\\), and \\(c\\).\nThis problem (or one very close to it), is known as a system of Diophantine equations.\nHere’s a picture to look at while you try to solve it. The answer is below. Don’t peek!\n\n\n\nThe Mathematicians, Chirico (1917)"
  },
  {
    "objectID": "posts/100bushels/index.html#the-solution",
    "href": "posts/100bushels/index.html#the-solution",
    "title": "100 Bushels of Corn",
    "section": "The Solution",
    "text": "The Solution\nHere’s my solution. I’ll break it into steps. From the problem statement, we know \\(m\\), \\(w\\), and \\(c\\) are all nonnegative integers.\n1. \\(c\\) is even.\nThat there is an even number of children is obvious from the fact that the total number of bushels is integral, and that the number of men, women, and children all have to be integral.\n2. \\(w\\) is a multiple of 5.\nTo prove this, we take the two original equations and eliminate \\(m\\), by multiplying the first equation by \\(-3\\) and adding the two together.\n\\[\n\\begin{aligned}\n-3m &- 3w &- 3c &= -300 \\\\\n3m &+ 2w &+ 0.5c &= 100\n\\end{aligned}\n\\]\nthis gives us:\n\\[\n\\begin{aligned}\n-w & -2.5c &= -200 \\\\\nw &+ 2.5c &= 200\n\\end{aligned}\n\\]\nAnother way to write the last equation is\n\\[\nw + (5/2) c = 200\n\\]\nSince \\(c\\) is even, \\((5/2) c\\) is divisible by 5, and 200 is divisible by 5; therefore, \\(w\\) is divisible by 5. QED\n3. \\(w \\leq 30\\)\nTo prove this, let’s eliminate \\(c\\).\n\\[\n\\begin{aligned}\n-0.5m &- 0.5w &-0.5c &= -50\\\\\n   3m &+ 2w   &+ 0.5c &= 100\n\\end{aligned}   \n\\]\nthis results in:\n\\[\n\\begin{aligned}\n2.5m &+ 1.5w &+ 0   &= 50 \\\\\n   5m &+   3w &+ 0   &= 100\n\\end{aligned}\n\\]\nwhich gives us\n\\[\nm = 20 - (3/5) w\n\\]\nNow we apply the fact that \\(m \\geq 0\\):\n\\[\n\\begin{aligned}\n20 - (3/5) w &\\geq 0 \\\\\n(3/5) w &\\leq20 \\\\\n3 w  &\\leq100 \\\\\nw &\\leq 100/3 = 33.333...\n\\end{aligned}\n\\]\nAnd since we know that \\(w\\) must be a multiple of 5, this gives us \\(w \\leq 30\\). QED\nWhat are the multiples of 5 that are less than or equal to 30?\n\n\nShow the code\nw = seq(from=0, to=30, by=5)\nw\n\n\n[1]  0  5 10 15 20 25 30\n\n\nThat’s 7 values—exactly what we’re looking for! So we’re basically done, but we can fill in all the counts just to polish it off. I’ll do that in R, but you can do it in any language, of course.\n\n\nShow the code\n# from Step 3\nm = 20 - (3 / 5) * w\n\n# from the fact that there are 100 people total\nc = 100 - (m + w)\n\npframe = data.frame(\n  men = m,\n  women = w,\n  children = c,\n  total_pop = m + w + c,\n  bushels = 3 * m + 2 * w + 0.5 * c\n)\n\nknitr::kable(pframe, caption = \"Allocations of men, women, and children\")\n\n\n\nAllocations of men, women, and children\n\n\nmen\nwomen\nchildren\ntotal_pop\nbushels\n\n\n\n\n20\n0\n80\n100\n100\n\n\n17\n5\n78\n100\n100\n\n\n14\n10\n76\n100\n100\n\n\n11\n15\n74\n100\n100\n\n\n8\n20\n72\n100\n100\n\n\n5\n25\n70\n100\n100\n\n\n2\n30\n68\n100\n100\n\n\n\n\n\nAnd there you have it: the seven solutions to the “100 bushels of corn” problem."
  },
  {
    "objectID": "posts/welcome-to-rworks/index.html",
    "href": "posts/welcome-to-rworks/index.html",
    "title": "Welcome to R Works!",
    "section": "",
    "text": "Artwork by @allison_horst\n\n\nThis is a heartfelt welcome to all members of the R community, from long-time readers of R Views to those just joining us. For years, R Views served as a source of resources, insights, and inspiration for R users. We are delighted to bring those ambitions to a new, Quarto-powered technical blog. Our goal is that R Works becomes a hub for news, opinions, and stories as the landscape of R continues to grow and evolve.\nR is more than code, it’s also the people behind it. We hope this blog reflects diverse perspectives and experiences from across the community. To that end, we invite your voices to help shape this space together. If you have unique perspectives, updates, commentary, or examples of using R that you’d love to share, we want to hear from you. Email us at rworks.editors@gmail.com.\nHappy Reading!"
  },
  {
    "objectID": "posts/r-work-opportunities/index.html",
    "href": "posts/r-work-opportunities/index.html",
    "title": "R Work Opportunities",
    "section": "",
    "text": "I’ve been spotting new opportunities in data science, analytics, and research that highlight R programming skills and wanted to share a few interesting positions. Here are jobs open as of publishing (please check the links for the latest status). Locations are included when specified.\nRemote Roles\n\nEarly Career Statistician, RTI\nData Scientist, Fors Marsh\nData Engineer, Zelus Analytics\nSenior Researcher, The Ohio State University\nData Analyst, Providence St. Joseph Health\nSenior Research Scientist, Harmony Labs\n\nHybrid Roles\n\nAssociate Data Scientist, Seyfarth - Chicago\nStaff Data Scientist, Growth, Hinge Health - San Francisco\nResearch Associate – Data Governance and Privacy, Urban Institute - Washington, D.C.\nStaff Data Scientist, Causal Inference, Intuit - San Diego/Mountain View/Los Angeles\nData Analyst, GTM Insights and Operations, Pluralsight - India\nIPUMS US Microdata Data Analyst 1, University of Minnesota - Minneapolis\nSenior Healthcare Data Scientist, Vertex Pharmaceuticals - Boston\n\nOnsite\n\nAnalyst, City of Boston - Boston\nSenior Data Journalist, Wall Street Journal - New York, New York\nHead of Design and Integration, Blue Ventures - Various Locations\nData Visualizations Development - Manager, Bristol Myers Squibb - Hyderabad\nData Scientist - Research and Development, Pittsburgh Pirates - Pittsburgh\nData Scientist, ACES - Various Locations\nSenior Research Scientist, Stanford\n\nThanks to the community meetups and forums where these roles were highlighted:\n\nData Science Hangout\nData Science Learning Community Slack\nR-Ladies Slack\n#rstats on Bluesky\n\nI’ll continue sharing open roles with R requirements periodically, so check back for new leads!"
  },
  {
    "objectID": "posts/meta-analysis/index.html",
    "href": "posts/meta-analysis/index.html",
    "title": "Examining Meta-Analysis",
    "section": "",
    "text": "In this post, we would like to review the idea of meta-analysis and compare a traditional, frequentist style, random effects meta-analysis to Bayesian methods. We will do this using the meta R package and a Bayesian analysis conducted with R but actually carried out by the Stan programming language on the back end. We will use a small but interesting textbook data set of summary data from eight randomized controlled trials. These studies examined the effectiveness of the calcium channel blocker Amlodipine compared to a placebo in improving work capacity in patients with angina. The data set and the analysis with the meta package come directly from the textbook by Chen and Peace (2013). Although there are several R packages that are capable of doing the Bayesian meta-analysis, we chose to work in rstan to demonstrate its flexibility and hint how one might go about doing a large complex study that doesn’t quite fit with pre-programmed paradigms.\nTo follow our examples, you really don’t need to know much more about meta-analysis than the definition offered by Wikipedia contributors (2024): “meta-analysis is a method of synthesis of quantitative data from multiple independent studies addressing a common research question”. If you want to dig deeper, a good overview of the goals and terminology can be found in Israel and Richter (2011). The online textbook Doing Meta-Analysis in R: A Hands-on Guide by Mathias Harrier and Ebert (2021) will take you a long way in performing your own work."
  },
  {
    "objectID": "posts/meta-analysis/index.html#example",
    "href": "posts/meta-analysis/index.html#example",
    "title": "Examining Meta-Analysis",
    "section": "Example",
    "text": "Example\nLet’s begin: load the required packages and read in the data.\n\n\nShow the code\nlibrary(meta)\n\nangina &lt;- read.csv(file = \"AmlodipineData.csv\",\n                   strip.white = TRUE,\n                   stringsAsFactors = FALSE)\n\nangina |&gt;\n  knitr::kable()\n\n\n\n\n\nProtocol\nnE\nmeanE\nvarE\nnC\nmeanC\nvarC\n\n\n\n\n154\n46\n0.2316\n0.2254\n48\n-0.0027\n0.0007\n\n\n156\n30\n0.2811\n0.1441\n26\n0.0270\n0.1139\n\n\n157\n75\n0.1894\n0.1981\n72\n0.0443\n0.4972\n\n\n162\n12\n0.0930\n0.1389\n12\n0.2277\n0.0488\n\n\n163\n32\n0.1622\n0.0961\n34\n0.0056\n0.0955\n\n\n166\n31\n0.1837\n0.1246\n31\n0.0943\n0.1734\n\n\n303\n27\n0.6612\n0.7060\n27\n-0.0057\n0.9891\n\n\n306\n46\n0.1366\n0.1211\n47\n-0.0057\n0.1291\n\n\n\n\n\nThe data set contains eight rows, each representing the measured effects of treatment and control on different groups. The column definitions are:\n\nProtocol id number of the study the row is summarizing.\nnE number of patients in the treatment group.\nmeanE mean treatment effect observed.\nvarE variance of treatment effect observed.\nnC number of patients in the control group.\nmeanC mean control effect observed.\nvarC variance of control effect observed.\n\n\nNaive Pooling\nA statistically inefficient, naive technique to combine the studies would be to pool all of the data. This is usually not even possible, as most studies don’t share data — and at best share summaries. We might hope this is enough to get a good estimate of the expected difference between treatment and non-treatment. However this ignores variance and treats low-quality results on the same footing as high-quality results, yielding unreliable results. Naive pooling also lacks standard diagnostic procedures and indications.\nThat being said, let’s form the naive pooled estimate.\n\n\nShow the code\nsum(angina$nE * angina$meanE) / sum(angina$nE) - sum(angina$nC * angina$meanC) / sum(angina$nC)\n\n\n[1] 0.2012727"
  },
  {
    "objectID": "posts/meta-analysis/index.html#a-fixed-effects-model",
    "href": "posts/meta-analysis/index.html#a-fixed-effects-model",
    "title": "Examining Meta-Analysis",
    "section": "A Fixed Effects Model",
    "text": "A Fixed Effects Model\nPerhaps the simplest reliable analysis is the fixed effects model. The underlying assumption for the fixed-effects model is that the true underlying effect or difference between treatment and control, \\(\\delta\\), is the same for all studies in the meta-analysis and that all possible risk factors are the same for all studies. Each study has its own observed effect size \\(\\hat{\\delta_i}\\). However, each is assumed to be a noisy estimate of \\(\\delta\\).\nSuch a study is summarized as follows.\n\n\nShow the code\nfixed.angina &lt;- metacont(\n  nE, meanE, sqrt(varE),\n  nC, meanC, sqrt(varC),\n  data = angina,\n  study.lab = \"Protocol\",\n  random = FALSE)\n\nsummary(fixed.angina)\n\n\n       MD            95%-CI %W(common)\n1  0.2343 [ 0.0969; 0.3717]       21.2\n2  0.2541 [ 0.0663; 0.4419]       11.4\n3  0.1451 [-0.0464; 0.3366]       10.9\n4 -0.1347 [-0.3798; 0.1104]        6.7\n5  0.1566 [ 0.0072; 0.3060]       17.9\n6  0.0894 [-0.1028; 0.2816]       10.8\n7  0.6669 [ 0.1758; 1.1580]        1.7\n8  0.1423 [-0.0015; 0.2861]       19.4\n\nNumber of studies: k = 8\nNumber of observations: o = 596 (o.e = 299, o.c = 297)\n\n                        MD           95%-CI    z  p-value\nCommon effect model 0.1619 [0.0986; 0.2252] 5.01 &lt; 0.0001\n\nQuantifying heterogeneity (with 95%-CIs):\n tau^2 = 0.0001 [0.0000; 0.1667]; tau = 0.0116 [0.0000; 0.4082]\n I^2 = 43.2% [0.0%; 74.9%]; H = 1.33 [1.00; 2.00]\n\nTest of heterogeneity:\n     Q d.f. p-value\n 12.33    7  0.0902\n\nDetails of meta-analysis methods:\n- Inverse variance method\n- Restricted maximum-likelihood estimator for tau^2\n- Q-Profile method for confidence interval of tau^2 and tau\n- Calculation of I^2 based on Q\n\n\nThe primary result of the summary is: the overall estimate for \\(\\delta\\) is 0.1619, much smaller than the naive pooling estimate of 0.2013. Also, a number of diagnostics seem “okay.” From the forest plot, it is easy to see that the Amlodipine treatment is not statistically significant for four of the protocols, but the overall effect of the difference between the treatment and the control is statistically significant. The plot also shows the test for heterogeneity, which indicates that there is no evidence against homogeneity.\n\n\nShow the code\nmeta::forest(fixed.angina, layout = \"RevMan5\")\n\n\n\n\n\n\n\n\n\nNotice the fixed effects model is not the earlier naive mean; it is also not theoretically the same estimate as the random effects model, which we now discuss."
  },
  {
    "objectID": "posts/meta-analysis/index.html#a-random-effects-model",
    "href": "posts/meta-analysis/index.html#a-random-effects-model",
    "title": "Examining Meta-Analysis",
    "section": "A Random Effects Model",
    "text": "A Random Effects Model\nThe next level of modeling is a random effects model. In this style model, we admit that the studies may be, in fact, studying different populations and different effects. A perhaps cleaner way to think about this is not as a type of effect (fixed or random), but as a model structured as a hierarchy (Gelman and Hill (2006)). The idea is that the analyst claims the populations are related, and the different effects are drawn from a common distribution, and observations are then drawn from these unobserved per-population effects. Inference is possible as the observed outcomes distributionally constrain the unobserved parameters.\nThe underlying assumption for the random-effects model is that each study has its own true underlying treatment effect, \\(\\delta_i\\), with variance \\({\\sigma_i}^2\\) that is estimated by \\(\\hat{\\delta_i}\\) Furthermore, all of the \\(\\delta_i\\) follow a \\(N(\\delta,\\tau^2)\\) distribution. Hence,\n\\[\n\\begin{align}\n\\hat{\\delta_i} &\\sim N(\\delta,\\sigma_i^2) \\\\\n\\delta_i &\\sim N(\\delta,\\tau^2)\n\\end{align}\n\\]\n\nFit the Random-Effects Model\n\n\nShow the code\nrandom.angina &lt;- metacont(\n  nE, meanE, sqrt(varE),\n  nC, meanC, sqrt(varC),\n  data = angina,\n  study.lab = \"Protocol\",\n  random = TRUE)\nsummary(random.angina)\n\n\n       MD            95%-CI %W(common) %W(random)\n1  0.2343 [ 0.0969; 0.3717]       21.2       21.1\n2  0.2541 [ 0.0663; 0.4419]       11.4       11.4\n3  0.1451 [-0.0464; 0.3366]       10.9       11.0\n4 -0.1347 [-0.3798; 0.1104]        6.7        6.7\n5  0.1566 [ 0.0072; 0.3060]       17.9       17.9\n6  0.0894 [-0.1028; 0.2816]       10.8       10.9\n7  0.6669 [ 0.1758; 1.1580]        1.7        1.7\n8  0.1423 [-0.0015; 0.2861]       19.4       19.3\n\nNumber of studies: k = 8\nNumber of observations: o = 596 (o.e = 299, o.c = 297)\n\n                         MD           95%-CI    z  p-value\nCommon effect model  0.1619 [0.0986; 0.2252] 5.01 &lt; 0.0001\nRandom effects model 0.1617 [0.0978; 0.2257] 4.96 &lt; 0.0001\n\nQuantifying heterogeneity (with 95%-CIs):\n tau^2 = 0.0001 [0.0000; 0.1667]; tau = 0.0116 [0.0000; 0.4082]\n I^2 = 43.2% [0.0%; 74.9%]; H = 1.33 [1.00; 2.00]\n\nTest of heterogeneity:\n     Q d.f. p-value\n 12.33    7  0.0902\n\nDetails of meta-analysis methods:\n- Inverse variance method\n- Restricted maximum-likelihood estimator for tau^2\n- Q-Profile method for confidence interval of tau^2 and tau\n- Calculation of I^2 based on Q\n\n\n\n\nShow the code\nmeta::forest(random.angina, layout = \"RevMan5\")\n\n\n\n\n\n\n\n\n\nThe random effects result is a \\(\\delta\\) estimate of about 0.1617. In this case, not much different than the fixed effects estimate."
  },
  {
    "objectID": "posts/meta-analysis/index.html#issues",
    "href": "posts/meta-analysis/index.html#issues",
    "title": "Examining Meta-Analysis",
    "section": "Issues",
    "text": "Issues\nSome issues associated with the above analyses include:\n\nLimited control of the distributional assumptions. What if we wanted to assume specific distributions on the un-observed individual outcomes? What if we want to change assumptions for a sensitivity analysis?\nThe results are limited to point estimates of the distribution parameters.\nThe results may depend on the underlying assumption of Normal distributions of published summaries.\nThe reported significance is not necessarily the probability of positive effect for a new subject.\n\n\nBayesian analysis\nTo try and get direct control of the model, graphs, and the model explanation, we will perform a direct hierarchical analysis using Stan through the rstan package.\nWe want to estimate the unobserved true value of treatment effect by a meta-analysis of different studies. The challenges of meta-analysis include:\n\nThe studies may be of somewhat different populations, implying different mean and variance of treatment and control response.\nWe are working from summary data from the studies and not data on individual patients.\n\nWe are going to make things simple for the analyst and ask only for approximate distributional assumptions on the individual respondents in terms of unknown, to-be-inferred parameters. Then we will use Stan to sample individual patient outcomes (and unobserved parameters) that are biased to be consistent with the known summaries. This saves having to know or assume distributions of the summary statistics. And, in fact, we could try individual distributions different from the “Normal” we plug in here.\nThe advantage of working this way is that the modeling assumptions are explicitly visible and controllable. The matching downside is that we have to own our modeling assumptions; we lose the anonymity of the implicit justification of invoking tradition and recognized authorities.\nTo get this to work, we use two Stan tricks or patterns:\n\nInstantiate many intermediate variables (such as individual effects).\nSimulate equality enforcement by saying a specified difference tends to be small.\n\nFirst, load the required packages.\n\n\nShow the code\n# attach packages\nlibrary(ggplot2)\nlibrary(rstan)\nlibrary(digest)\nsource(\"define_Stan_model.R\")\n\n\nAnd then prepare the study names for display.\n\n\nShow the code\nn_studies = nrow(angina)\n# make strings for later use\ndescriptions = vapply(\n  seq(n_studies),\n  function(i) { paste0(\n    'Protocol ', angina[i, 'Protocol'], ' (',\n    'nE=', angina[i, 'nE'], ', meanE=', angina[i, 'meanE'],\n    ', nC=', angina[i, 'nC'], ', meanC=', angina[i, 'meanC'],\n    ')') },\n  character(1))\n\n\nThe modeling principles are as follows:\n\nThere are unobserved true treatment and control effects we want to estimate. Call these \\(\\mu^{treatment}\\) and \\(\\mu^{control}\\). We assume a shared standard deviation \\(\\sigma\\). We can easily model without a shared standard deviation by introducing more parameters to the model specification.\nFor each protocol or study, we have ideal unobserved mean treatment effects and mean control effects. Call these \\(\\mu_{i}^{treatment}\\) and \\(\\mu_{i}^{control}\\) for \\(i = 1 \\cdots 8\\).\n\nThe equations bringing each study \\(i\\) into our Bayesian model are as follows.\n\\[\\begin{align}\n\\mu^{treatment}_i &\\sim N(\\mu^{treatment}, \\sigma^2) &\\# \\; \\mu^{treatment} \\; \\text{is what we are trying to infer} \\\\\n\\mu^{control}_i &\\sim N(\\mu^{control}, \\sigma^2) &\\# \\; \\mu^{control} \\;\\text{is what we are trying to infer} \\\\\n\nsubject^{treatment}_{i,j} &\\sim N(\\mu^{treatment}_i, \\sigma_i^2) &\\# \\; \\text{unobserved} \\\\\nsubject^{control}_{i,j} &\\sim N(\\mu^{control}_i, \\sigma_i^2) &\\# \\; \\text{unobserved} \\\\\n\nmean_j(subject^{treatment}_{i,j}) - observed\\_mean^{treatment}_{i} &\\sim N(0, 0.01) &\\# \\; \\text{force inferred to be near observed} \\\\\nmean_j(subject^{control}_{i,j}) - observed\\_mean^{control}_{i} &\\sim N(0, 0.01) &\\# \\; \\text{force inferred near to be observed} \\\\\nvar_j(subject^{treatment}_{i,j}) - observed\\_var^{treatment}_{i} &\\sim N(0, 0.01) &\\# \\; \\text{force inferred to be near observed} \\\\\nvar_j(subject^{control}_{i,j}) - observed\\_var^{control}_{i} &\\sim N(0, 0.01) &\\# \\; \\text{force inferred near to be observed}\n\\end{align}\\]\nThe above equations are designed to be argued over. They need to be believed to relate the unobserved true treatment and control effects to the recorded study summaries. If they are not believed, is there a correction that would fix that? If the equations are good enough, then we can sample the implied posterior distribution of the unknown true treatment and control effects, which would finish the meta-analysis. The driving idea is that it may be easier to discuss how unobserved individual measurements may relate to observed and unobserved summaries and parameters than to work out how to relate statistical summaries to statistical summaries.\nIt is mechanical to translate the above relations into a Stan source model to make the desired inferences. The lines marked “force inferred to be near observed” are not distributional beliefs- but just a tool for enforcing that the inferred individual data should match the claimed summary statistics.\n\n\nShow the code\n# the Stan data\nstan_data = list(\n  n_studies = n_studies,\n  nE = array(angina$nE, dim = n_studies),  # deal with length 1 arrays confused with scalars in JSON path\n  meanE = array(angina$meanE, dim = n_studies),\n  varE = array(angina$varE, dim = n_studies), \n  nC = array(angina$nC, dim = n_studies), \n  meanC = array(angina$meanC, dim = n_studies), \n  varC = array(angina$varC, dim = n_studies))\n\n\nHere we define the Stan Model.\n\n\nShow the code\nanalysis_src_joint &lt;- define_Stan_model(n_studies = n_studies, c_replacement = '')\n\n\nThis code runs the procedure. The function run_cache() runs the standard stan() function, saving the result to a file cache for quick and deterministic re-re-renderings of the notebook. The caching is not required, but the runs took about five minutes on an old-Intel based Mac.\n\n\nShow the code\n# run the sampling procedure\nfit_joint &lt;- run_cached(\n  stan,\n  list(\n  model_code = analysis_src_joint,  # Stan program\n  data = stan_data,                 # named list of data\n  chains = 4,                       # number of Markov chains\n  warmup = 2000,                    # number of warm up iterations per chain\n  iter = 4000,                      # total number of iterations per chain\n  cores = 4,                        # number of cores (could use one per chain)\n  refresh = 0,                      # no progress shown\n  pars = c(\"lp__\",                  # parameters to bring back\n           \"inferred_grand_treatment_mean\", \n           \"inferred_grand_control_mean\", \n           \"inferred_between_group_stddev\",\n           \"inferred_group_treatment_mean\", \n           \"inferred_group_control_mean\",\n           \"inferred_in_group_stddev\", \n           \"sampled_meanE\", \n           \"sampled_varE\",\n           \"sampled_meanC\", \n           \"sampled_varC\")\n  ),\n  prefix = \"Amlodipine_joint\"\n)\n\n\nAnd then, we extract the results.\n\n\nShow the code\n# show primary inference\ninference &lt;- fit_joint |&gt;\n  as.data.frame() |&gt;\n  (`[`)(c(\"inferred_grand_treatment_mean\", \"inferred_grand_control_mean\", \"inferred_between_group_stddev\")) |&gt;\n  colMeans() |&gt;\n  as.list() |&gt;\n  data.frame() \ninference['delta'] &lt;- inference['inferred_grand_treatment_mean'] - inference['inferred_grand_control_mean']\n\ninference |&gt;\n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\ninferred_grand_treatment_mean\ninferred_grand_control_mean\ninferred_between_group_stddev\ndelta\n\n\n\n\n0.2002917\n0.0391202\n0.0650105\n0.1611715\n\n\n\n\n\nAnd our new estimate is: 0.1611715 which is very similar to the previous results. We can graph the inferred posterior distribution of effect size as follows.\nFirst, we plot the estimated posterior distribution of both the treatment and control effects. This is the final result of the analysis, using all of the data, simulated from the eight trials. In this result, high-variance studies have a diminished influence on the overall estimated effect sizes. Running all of the simulated data together without maintaining the trial structure would produce an inflated 0.2 again.\n\n\nShow the code\n# plot the grand group inferences \ndual_density_plot(\n  fit_joint, \n  c1 = 'inferred_grand_treatment_mean', \n  c2 = 'inferred_grand_control_mean',\n  title = 'grand estimate')\n\n\n\n\n\n\n\n\n\n\n\nWatching the Pooling\nWe can examine how the hierarchical model changes the estimates as follows. In each case we are plotting the posterior distribution of the unknown treatment and control effect estimates, this time per original study, with and without pooling analysis.\nTo do this, we define an additional “the studies are unrelated model”.\n\n\nShow the code\nanalysis_src_independent &lt;- define_Stan_model(n_studies = n_studies, c_replacement = '//')\n\n\nWe then fit the model or use previously cached results.\n\n\nShow the code\n# run the sampling procedure\nfit_independent &lt;- run_cached(\n  stan,\n  list(\n  model_code = analysis_src_independent,  # Stan program\n  data = stan_data,                       # named list of data\n  chains = 4,                             # number of Markov chains\n  warmup = 2000,                          # number of warm up iterations per chain\n  iter = 4000,                            # total number of iterations per chain\n  cores = 4,                              # number of cores (could use one per chain)\n  refresh = 0,                            # no progress shown\n  pars = c(\"lp__\",                        # parameters to bring back\n           \"inferred_group_treatment_mean\", \n           \"inferred_group_control_mean\",\n           \"inferred_in_group_stddev\", \n           \"sampled_meanE\", \n           \"sampled_varE\",\n           \"sampled_meanC\",\n           \"sampled_varC\")\n  ),\n  prefix = \"Amlodipine_independent\"\n)\n\n\nThen, we can plot the compared inferences of the hierarchical and independent models.\nThe top plot shows the Bayesian inference that would result using only data from a single study. The bottom plot shows the estimate made for the given study, given the data from the other studies. Notice that in this first pair of plots, the bottom plot sharpens the distributions and tends to pull the means together. The first two plots have the means pulled in, the third pushed out, and behavior varies by group from then on. Notice in Protocol 162, the inferred means are reversed. These graphs are the Bayesian analogs of the forest plots above.\n\n\nShow the code\n# plot comparison inferences\nfor(i in seq(n_studies)) {\n  print(double_dual_density_plot(\n    fitA = fit_independent,\n    fitB = fit_joint,\n    c1 = gsub('{i}', as.character(i), 'inferred_group_treatment_mean[{i}]', fixed = TRUE), \n    c2 = gsub('{i}', as.character(i), 'inferred_group_control_mean[{i}]', fixed = TRUE), \n    title = gsub('{p}', descriptions[[i]], '{p}\\nestimates', fixed = TRUE),\n    vlines=c(angina[i, 'meanE'], angina[i, 'meanC'])))\n}"
  },
  {
    "objectID": "posts/meta-analysis/index.html#conclusion",
    "href": "posts/meta-analysis/index.html#conclusion",
    "title": "Examining Meta-Analysis",
    "section": "Conclusion",
    "text": "Conclusion\nIt is a challenge to communicate exactly what meta-analysis a given standard package actually implements. It is our assumption that many standard meta-analyses reflect the tools available to the researchers and do not necessarily provide the ability to implement distributions that reflect the modeling assumptions they would prefer. Or, to be blunt, you don’t really know what the packaged models are doing until you can match them to a known calculation.\nPerhaps the greatest difference between the “standard” and Bayesian approaches is that Bayesian modeling requires the analyst to really own the modeling assumptions. This is why we wrote out so many of the modeling distributional assumptions as formulas instead of as named methodologies in our method descriptions.\nWe feel that there is a gap in the available practical literature. There is room for more teaching materials making meta-analysis more approachable by relating them to explicit model structures."
  },
  {
    "objectID": "posts/meta-analysis/index.html#references",
    "href": "posts/meta-analysis/index.html#references",
    "title": "Examining Meta-Analysis",
    "section": "References",
    "text": "References\n\n\nChen, Ding-Geng, and Karl E. Peace. 2013. Applied Meta-Analysis with R. CRC Press.\n\n\nGelman, Andrew, and Jennifer Hill. 2006. Data Analysis Using Regression and Multilevel/Hierarchical Models. Analytical Methods for Social Research. Cambridge University Press.\n\n\nIsrael, Heidi, and Randy R. Richter. 2011. “A Guide to Understanding Meta-Analysis.” Journal of Orthopaedic & Sports Physical Therapy 41 (7): 496–504. https://doi.org/10.2519/jospt.2011.3333.\n\n\nMathias Harrier, Toshi A. Furukawa, Pim Cuijpers, and David D. Ebert. 2021. Doing Meta-Analysis with R: A Hands-on Guide. CRC Press.\n\n\nWikipedia contributors. 2024. “Meta-Analysis — Wikipedia, the Free Encyclopedia.” https://en.wikipedia.org/w/index.php?title=Meta-analysis&oldid=1251112044."
  },
  {
    "objectID": "posts/meta-analysis/index.html#appendix-reproducing-the-result",
    "href": "posts/meta-analysis/index.html#appendix-reproducing-the-result",
    "title": "Examining Meta-Analysis",
    "section": "Appendix: Reproducing the Result",
    "text": "Appendix: Reproducing the Result\nThe files required to reproduce the result are:\n\nExaminingMetaAnalysis.qmd: this notebook.\nAmlodipineData.csv: the original data.\ndefine_Stan_model.R: helper functions.\n(optional) cache_Amlodipine_independent_b818d8db720377f42b728cbc88a94e58.RDS: the cached result.\n(optional) cache_Amlodipine_joint_f0ae8de9e99d9396a5f2a21f5638030f.RDS: the cached result."
  },
  {
    "objectID": "posts/october-2024-top-40-new-cran-packages/index.html",
    "href": "posts/october-2024-top-40-new-cran-packages/index.html",
    "title": "October 2024: Top 40 New CRAN Packages",
    "section": "",
    "text": "One hundred eighty-one new packages made CRAN’s final cut in October. Here are my Top 40 picks in thirteen categories: AI, Climate Analysis, Computational Methods, Data, Epidemiology, Genomics, Machine Learning, Medicine, Quality Management, Statistics, Time Series, Utilities, and Visualization.\n\n\n\nAI\nhollr v1.0.0: Enables chat completion and text annotation with local and OpenAI language models and supports batch processing, multiple annotators, and consistent output formats. See README.\nspBPS v0.0-4: Provides functions for Bayesian Predictive Stacking within the Bayesian transfer learning framework for geospatial artificial systems, as described in Rewaiccw & Banerjee (2024). Core functions leverage C++, making the framework well-suited for large-scale spatial data analysis in parallel and distributed computing environments.\n\ntidyllm v0.1.0: Implements a tidy interface for integrating large language model (LLM) APIs such as Claude, ChatGPT, Groq, and local models via Ollama into R workflows and supports text and media-based interactions, interactive message history, stateful rate limit handling, and a tidy, pipeline-oriented interface for streamlined integration into data workflows. See the vignette.\n\n\nClimate Analysis\ncarbonr v0.2.1: Provides a tool for calculating carbon-equivalent emissions based on the UK Government’s Greenhouse Gas Conversion Factors report; it facilitates transparent emissions calculations for various sectors, including travel, accommodation, and clinical activities. See the vignette.\nfluxfinder v1.0.0: Parse static-chamber greenhouse gas measurement files generated by a variety of instruments; compute flux rates using multi-observation metadata; and generate diagnostic metrics and plots. Designed to be easy to integrate into reproducible scientific workflows. There is an Introduction and a vignette on integrating with the gasfluxes package.\n\n\n\nComputational Methods\ngmresls v0.2.2: Implements a method to solve a least squares system Ax~=b (dim(A)=(m,n) with m &gt;= n) with a precondition matrix B: BAx=Bb (dim(B)=(n,m)) based on the General Minimal Residual Algorithm of Saad & Schultz (1986). See README.\nmappeR v1.3.0: Implements an algorithm that generalizes the concept of a Reeb Graph as described in Mémoli and Carlsson (2007) for a Topological Data Analysis of high dimensional data. Look here for examples.\n\nTensorTools v1.0.0: Provides a set of tools for basic tensor operators based on the Discrete Fourier Transform, including the eigenvalue, QR decomposition, LU decompositions of a tensor, and the calculation of the inverse of a tensor and the transpose of a symmetric tensor. See Kernfeld et al. (2015) for the details and the vignette for examples. Note, in this context, a tensor is a multidimensional array.\n\n\nData\nopenFDA v0.1.0: Facilitates access to U.S. Food and Drug Administration’s openFDA data on drugs, devices, foodstuffs, tobacco, and more. See Kass-Hout et al. (2016) for background and the vignette to get started.\nozbabynames v0.1.0: Provides data on the most popular baby names by sex and year for each state in Australia as provided by the state and territory governments. The quality and quantity of the data varies with the state. Look here for information.\n\n\n\nEpidemiology\nepichains v0.1.1: Provides methods to simulate and analyze the size and length of branching processes with an arbitrary offspring distribution. These can be used, for example, to analyze the distribution of chain sizes or length of infectious disease outbreaks, as discussed in Farrington et al. (2003). There are six vignettes, including a Getting Started guide and Theoretical Background.\n\nepizootic v1.0.0: Extends the pattern-oriented modeling framework of the poems package to provide functions for modeling disease transmission on a population scale in a spatiotemporally explicit manner and includes seasonal time steps, dispersal functions, objects that store disease states, and a population simulator that includes disease dynamics. See the vignette for an extended example.\n\nserosv v1.0.1: Implements tools to estimate infectious diseases parameters using serological data. Implemented models include SIR models, nonparametric models, semiparametric models, and hierarchical models, which are based on the book by Hens et al. (2013). There are eight vignettes including Hierarchical Bayesian Models and Modeling directly from antibody levels.\n\n\n\nGenomics\nGSEMA v0.99.3: Provides functions to perform various steps of gene set enrichment meta-analysis, including meta-analysis of effect sizes from different pathways in different studies. See the vignette for examples.\n\nphylotypr v0.1.0: Implements Naive Bayesian Classifier from the Ribosomal Database Project that traditionally has been used to classify 16S rRNA gene sequences to bacterial taxonomic outlines but which applies to any type of gene sequence. See Wang et al. (2007) for background and the vignette for examples.\n\n\nMachine Learning\ntextpress v1.0.0: Provides a Natural Language Processing (NLP) toolkit focused on search-centric workflows with minimal dependencies. The package offers key features for web scraping, text processing, corpus search, and text embedding generation via the HuggingFace API. See README for examples.\n\n\nMedicine\neyetools v0.7.2: Provides functions to help researchers analyze eye data enabling the automation of actions across a pipeline that includes transforming binocular data, gap repair, and event-based processing such as fixations, saccades, and entry and duration in areas of interest. Functions implement the fixation and saccade detection methods proposed by Salvucci and Goldberg (2000) and visualize eye movement. See the Introduction.\n\nMedDataSets v0.1.0: Provides an extensive collection of datasets related to medicine, diseases, treatments, drugs, and public health, covering topics such as drug effectiveness, vaccine trials, survival rates, infectious disease outbreaks, and medical treatments. including AIDS, cancer, bacterial infections, and COVID-19, and information on pharmaceuticals and vaccines. These datasets are sourced from the R ecosystem and other R packages. See the vignette.\nODT v1.0.0: Implements a tree-based method specifically designed for personalized medicine applications that uses genomic and mutational data to identify optimal drug recommendations tailored to individual patient profiles. See Gimeno et al. (2023) for the details and the vignette for an example.\n\ntrtswitch v0.1.1: Implements rank-preserving structural failure time model (RPSFTM), iterative parameter estimation (IPE), inverse probability of censoring weights (IPCW), and two-stage estimation (TSE) methods for treatment switching in randomized clinical trials. See Latimer et al. (2017) for background. There are five vignettes including descriptions of RPSFTM and IPE.\nUnplanSimon v0.1.0: Implements methods to manage under- and over-enrollment in Simon’s Two-Stage Design by providing adaptive threshold adjustments and sample size recalibration, and post-inference analysis tools to support clinical trial design and evaluation. See the vignette.\n\n\nQuality Management\nr6qualitytools v1.0.1: Implements a suite of statistical tools for Quality Management, designed around the Define, Measure, Analyze, Improve, and Control (DMAIC) cycle used in Six Sigma methodology. It refactors the qualitytoolspackage incorporating ‘R6’ object-oriented programming for increased flexibility and performance and replaces traditional graphics with modern, interactive visualizations. Look here for examples.\n\n\n\nStatistics\nclusTMB: v0.1.0: Fits a spatio-temporal finite mixture model using the TMB package.. Covariate, spatial, and temporal random effects can be incorporated into the gating formula using multinomial logistic regression, the expert formula using a generalized linear mixed model framework, or both. See the vignettes Covariance Structure and Meuse Example.\n\n\n\n\n\n\nStatistics (Continued)\nextrememix v0.0.1: Fits extreme value mixture models, which are models for tails not requiring selection of a threshold, for continuous data. See Behrens et al.(2004) and Nascimento et al. (2011) for the theory and the vignette for examples.\n\nMECfda v0.1.0: Implements functions to solve scalar-on-function linear models, including generalized linear mixed effect model and quantile linear regression model, and bias correction estimation methods due to measurement error. Details about the measurement error bias correction methods. See Luan et al. (2023), Tekwe et al. (2022), Zhang et al. (2023), and Tekwe et al. (2019) for background and the vignette for an introduction.\nmultipois v0.2.0: Implements a method to analyze polytomous responses with three or more unordered categories by transforming nominal response data into counts for each categorical alternative. These counts are then analyzed using (mixed) Poisson regression as per Baker (1994).\n\nspStack v1.0.1: Fits Bayesian hierarchical spatial process models for point-referenced Gaussian, Poisson, binomial, and binary data using stacking of predictive densities by sampling from analytically available posterior distributions conditional upon some candidate values of the spatial process parameters. See Zhang et al. (2024) and Pan et al. (2024) for background and the vignette for examples.\n\nsvyROC v1.0.0: Provides functions to estimate the receiver operating characteristic (ROC) curve, area under the curve (AUC), and optimal cut-off points for individual classification, taking into account complex sampling designs when working with complex survey data. See Iparragirre et al. (2024), Iparragirre et al. (2022) and Iparragirre & Barrio (2024) for the theory and README for an example.\n\nVDPO v0.1.0: Provides a comprehensive set of tools for analyzing and manipulating functional data with non-uniform lengths and addresses two common scenarios in functional data analysis: Variable Domain Data and Partially Observed Data. See Amaro et al. (2024) for the details. There are two vignettes: Introduction and Model fitting for variable domain data.\n\n\n\nTime Series\nFTSgof v1.0.0: Offers tools for the analysis of functional time series data, focusing on white noise hypothesis testing and goodness-of-fit evaluations, alongside functions for simulating data and advanced visualization techniques. See Kokoszka et al. (2017), Yeh et al. (2023), Kim et al. (2023), and Rice et al. (2020) for the theory and the vignette for examples.\n\nutsf v1.0.0: Implements a uniform interface for univariate time series forecasting using different regression models in an autoregressive way and provides features such as preprocessing and estimation of forecast accuracy. See the vignette.\n\n\n\nUtilities\nctypesio v0.1.1: Provides functions for reading and writing binary data (with files, connections, and raw vectors) using C type descriptions that convert data between C types and R types while checking for values outside the type limits, NA values, etc. See the vignettes Parsing JPEG Markers and Handcrafting a WAV file.\nGitStats v2.1.2: Provides functions to obtain standardized data from multiple Git services, including GitHub and GitLab. There are vignettes on Getting Repositories and Setting Hosts.\nmall v0.1.0: Enables users to run multiple Large Language Model predictions against a table. The predictions run row-wise over a specified column. It works using a one-shot prompt, along with the current row’s content. Look here got examples.\nMDPIexploreR v0.2.0: Provides tools to scrape and analyze data from the MDPI journals, including functions to extract metrics such as submission-to-acceptance times, article types, and plot data, and explore patterns of self-citations. See the vignette.\n\n\n\nVisualization\naffiner v0.1.1: Provides functions to dilate, permute, project, reflect, rotate, shear, and translate 2D and 3D points. Supports parallel projections including oblique projections such as the cabinet projection as well as axonometric projections such as the isometric projection. There is an Introduction and a vignette on geometry.\n\ndoceplot v0.1.3: Provides functions to visualize data with more than two categorical variables and additional continuous variables that are particularly useful for exploring complex categorical data in the context of pathway analysis across multiple conditions. Look here for documentation.\n\nggcompare v0.0.2: Add mean comparison annotations to a ggplot to indicate if two or more groups are significantly different. For comparisons between two groups, the p-value is calculated by t-test (parametric) or Wilcoxon rank sum test (nonparametric). For comparisons among more than two groups, the p-value is calculated by One-way ANOVA (parametric) or Kruskal-Wallis test (nonparametric). Look here for examples.\n\nggpca v0.1.2: Provides tools for creating publication-ready dimensionality reduction plots, including Principal Component Analysis (PCA), t-Distributed Stochastic Neighbor Embedding (t-SNE), and Uniform Manifold Approximation and Projection (UMAP) using the ggplot2 framework. See the vignette.\n\nplotscaper v0.2.3: Implements a framework for creating interactive figures for data exploration. All plots are automatically linked and support several kinds of interactive features, including selection, zooming, panning, and parameter manipulation. There is an Introduction and four additional vignettes, including Available Interactions and Layout.\n\nvchartr v0.1.3: Provides an htmlwidgets interface to VChart, a cross-platform charting library and expressive data storyteller. See the vignette."
  },
  {
    "objectID": "posts/100Bushels-Revisited/index.html",
    "href": "posts/100Bushels-Revisited/index.html",
    "title": "100 Bushels of Corn, Revisited",
    "section": "",
    "text": "About the authors\n\n\n\nJohn Mount is a data scientist based in San Francisco, with 20+ years of experience in machine learning, statistics, and analytics. He is the co-founder of the data science consulting firm Win-Vector LLC, and (with Nina Zumel) the co-author of Practical Data Science with R, now in its second edition.\nNina Zumel is a data scientist based in San Francisco, with 20+ years of experience in machine learning, statistics, and analytics. She is the co-founder of the data science consulting firm Win-Vector LLC, and (with John Mount) the co-author of Practical Data Science with R, now in its second edition."
  },
  {
    "objectID": "posts/100Bushels-Revisited/index.html#introduction",
    "href": "posts/100Bushels-Revisited/index.html#introduction",
    "title": "100 Bushels of Corn, Revisited",
    "section": "Introduction",
    "text": "Introduction\nNina Zumel presented the “100 Bushels of Corn” puzzle here as a fun example of using R as a calculator. What if we want R to solve the puzzle for us, instead of merely being a calculator?\nLet’s give that a go."
  },
  {
    "objectID": "posts/100Bushels-Revisited/index.html#setting-up-the-problem",
    "href": "posts/100Bushels-Revisited/index.html#setting-up-the-problem",
    "title": "100 Bushels of Corn, Revisited",
    "section": "Setting Up the Problem",
    "text": "Setting Up the Problem\n\n100 bushes of corn are distributed to 100 people such that every man receives 3 bushels, every woman 2 bushels, and every child 1/2 a bushel. How many men, women, and children are there?\n\nWe can write the 100 Bushels of Corn problem as finding integer vectors x that satisfy a %*% x = b for the following a, b. The first row specifies the constraint on the total number of men, women and children; the second row specifies the constraint on how many bushels each person gets. Notice that we doubled the values of the second equation to keep everything integral.\n\na &lt;- matrix(c(1, 1, 1, 6, 4, 1),\n            nrow = 2,\n            ncol = 3,\n            byrow = TRUE)\n\na\n\n     [,1] [,2] [,3]\n[1,]    1    1    1\n[2,]    6    4    1\n\n\n\nb &lt;- as.matrix(c(100, 200))\n\nb\n\n     [,1]\n[1,]  100\n[2,]  200\n\n\nThere are at least two main ways to solve this:\n\nBrute force. This is kind of the point of computers and programming languages.\nLinear algebra, in particular linear algebra over the ring of integers. This approach shows some of the richness of the R package environment curated at CRAN.\n\nLet’s take a quick look at these two solution styles."
  },
  {
    "objectID": "posts/100Bushels-Revisited/index.html#brute-force-solution",
    "href": "posts/100Bushels-Revisited/index.html#brute-force-solution",
    "title": "100 Bushels of Corn, Revisited",
    "section": "Brute Force Solution",
    "text": "Brute Force Solution\nA brute force solution is as follows. First, we get a simple upper bound on each variable. We can do this by checking one row of a %*% x = b.\n\nupper_bounds &lt;- floor(b[2] / a[2, ])\nupper_bounds\n\n[1]  33  50 200\n\n\nNow we try all plausible solutions.\n\nfor (x1 in 0:upper_bounds[[1]]) {\n  for (x2 in 0:upper_bounds[[2]]) {\n    # use a row of a to solve for x3\n    x3 &lt;- (b[1] - (a[1, 1] * x1 + a[1, 2] * x2)) / a[1, 3]\n    # check constraints\n    if ((x3 &gt;= 0) && (abs(x3 %% 1) &lt; 1e-8)) {\n      x = as.matrix(c(x1, x2, x3))\n      if (all(a %*% x == b)) {\n        print(c(x1, x2, x3))\n      }\n    }\n  }\n}\n\n[1]  2 30 68\n[1]  5 25 70\n[1]  8 20 72\n[1] 11 15 74\n[1] 14 10 76\n[1] 17  5 78\n[1] 20  0 80\n\n\nAnd this gives us exactly the 7 solutions Nina found. This is some of the magic of having a programmable computer: one can cheaply try a lot of potential solutions without needing a lot of theory."
  },
  {
    "objectID": "posts/100Bushels-Revisited/index.html#ring-theory-solution",
    "href": "posts/100Bushels-Revisited/index.html#ring-theory-solution",
    "title": "100 Bushels of Corn, Revisited",
    "section": "Ring Theory Solution",
    "text": "Ring Theory Solution\nIt turns out there is a systematic way to find all of the integral solutions to a linear system quickly, at least for low dimensional solution spaces. To do this we will use a ring theory or linear algebra matrix factorization called the Hermite normal form. Fortunately, R has a package for this, called numbers. We attach this package as follows.\n\nlibrary(numbers)\n\nThis package will find for us a lower diagonal integer matrix h and a square unimodular matrix u such that h = a %*% u. Unimodular matrices map the space of integer vectors Zn to the same space of integer vectors in a 1 to 1, onto, and invertible manner. This means finding integer solution vectors to a %*% x = b is equivalent to the problem of finding integer solutions to h %*% y = b, where x = u %*% y. This second problem is easier, as h is lower diagonal- so solving this system is just a matter of “back filling”.\nLet’s first find h, u.\n\nhnf &lt;- hermiteNF(a)\nh &lt;- hnf$H\nu &lt;- hnf$U\nstopifnot(all(h == a %*% u))\n\n\n# lower triangular transform of a\nh\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n\n\n\n# unimodular transform\nu\n\n     [,1] [,2] [,3]\n[1,]    7   -1   -3\n[2,]  -12    2    5\n[3,]    6   -1   -2\n\n\nWe now have a %*% u = h. a %*% x = b implies h %*% y = b where x = u %*% y. Let’s solve for a specific solution xs by back substitution.\n\n# back substitute to solve h %*% y = b\n# this uses the fact that h is lower-triangular\nh_rank &lt;- sum(diag(h) != 0)\nstopifnot(h_rank &gt; 1)\ny &lt;- numeric(ncol(a))\ny[1] &lt;- b[1] / h[1, 1]\nfor (i in 2:h_rank) {\n  y[i] &lt;- (b[i] - sum(h[i, 1:(i - 1)] * y[1:(i - 1)])) / h[i, i]\n}\nstopifnot(all(b == h %*% y))\nxs &lt;- u %*% y\nstopifnot(all(a %*% xs == b))\nxs\n\n     [,1]\n[1,]  500\n[2,] -800\n[3,]  400\n\n\nIt is a standard result of linear algebra that all solutions of a %*% x = b are of the form x = xs + z where a %*% z = 0 (that is, z is in the null space of h and a). In our case, the null space is spanned by the last column of u.\nLet’s show this column.\n\nnull_basis &lt;- u[, (h_rank + 1):ncol(u), drop = FALSE]\n\nnull_basis\n\n     [,1]\n[1,]   -3\n[2,]    5\n[3,]   -2\n\n\nSo in our case: all integer solutions of a %*% x = b are of the form [500, -800, 400] + k * [-3, 5, -2] for integer k.\nNow we just need to pick k to make everything non-negative (an implicit puzzle condition!). The sign changes of entries of [500, -800, 400] + k * [-3, 5, -2] happen for k where one of the coordinates is equal to zero. These are:\n\n500 - 3 * k == 0\n-800 + 5 * k == 0, and\n400 - 2 * k == 0.\n\nSo the k of interest are in the range ceiling(max(0, 800/5)) &lt;= k &lt;= floor(min(500/3, 400/2)), or 160 &lt;= k &lt;= 166. This gives us:\n\nfor (k in 160:166) {\n  soln &lt;- xs + k * null_basis\n  print(c(soln[1, 1], soln[2, 1], soln[3, 1]))\n}\n\n[1] 20  0 80\n[1] 17  5 78\n[1] 14 10 76\n[1] 11 15 74\n[1]  8 20 72\n[1]  5 25 70\n[1]  2 30 68\n\n\nAnd these are again exactly the solutions Nina Zumel gave in the first 100 Bushels post."
  },
  {
    "objectID": "posts/100Bushels-Revisited/index.html#conclusion",
    "href": "posts/100Bushels-Revisited/index.html#conclusion",
    "title": "100 Bushels of Corn, Revisited",
    "section": "Conclusion",
    "text": "Conclusion\nR gives the ability to exploit any combination of innate ability and knowledge, borrowed ability, or brute force in solving problems."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to R Works.\nR Works is the blog devoted to the R community and the R language. We imagine it as a place to read considered opinions about topics of interest to the R community, learn what is happening at conferences and user group meetings around the world, discover new R packages, and explore applications of R in various areas of statistics or data science.\nR Works is powered by Quarto. We moved to this platform to provide an enhanced experience for our readers. With Quarto’s advanced features, we can showcase R-related topics and deliver high-quality content.\nThe blog is edited by Joseph Rickert and Isabella Velásquez, who curate the content and ensure its quality. We hope that R Works will continue to attract other voices from around the R community. If you have something to say on an R-related topic, news, commentary, or an example of using R that you would like to share with the R community, please review the How to page.\nSyndicated on R-Bloggers and R Weekly."
  }
]