{
  "hash": "da851d520e7eee0d8727bd9d96d5bbdd",
  "result": {
    "markdown": "---\ntitle: \"ARIMA NOTE\"\nauthor: \"Joseph Rickert\"\ndate: 2025-03-25\ncode-fold: true\ncode-summary: \"Show the code\"\ndescription: \".\"\ncategories: \"\"\neditor: source\n---\n\n\nIn my previous post [A First Look at TimeGPT using nixtlar](https://rworks.dev/posts/revised_TimeGPT/), I used the `auto.arima()` function from the `forecast` package to fit an ARIMA model to a time series of electricity usage data in order to compare and ARIMA forecast with the `TimeGPT` forecast. While working out the bugs in that post, I also fit an automatic ARIMA model using the newer and improved `fable` package and was very surprised by the results. In this post I will show what surprised me, work through my investigation and present some practical consequences of the problem of ARIMA identifiability.\n\nHere are the necessary libraries and the data that we will be working with.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\nlibrary(tidyverse)\nlibrary(forecast)\nlibrary(fable)\nlibrary(tsibble)\nlibrary(nixtlar) # for the electricity data\nlibrary(feasts)\nlibrary(Metrics)\n```\n:::\n\n\nAs in the `TimeGPT` post, I will use the BE electricity usage data set from the `nixtlar` package for fitting models and making forecasts. A plot of the data shows that the dime series seems to have some cyclic behavior punctuated by periods of extreme volatility. \n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\ndf <- nixtlar::electricity\n#glimpse(df)\n\ndf2 <- df |> mutate(time = as.POSIXct(ds, format = \"%Y-%m-%d %H:%M:%S\")) |> \n             filter(unique_id == \"BE\") |> select(-unique_id, -ds)\n  \n\np <- df2 |> ggplot(aes(x = time, y = y)) +\n  geom_line(color='darkblue') +\n  ggtitle(\" BE Electricity Usage Data\")\n\np\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nThis next block of code splits the data into training and test data with the last 24 observations from the BE data set being held out for forecasting.\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\nNF <- 24\n\nBE_df_wide <- df |> pivot_wider(names_from = unique_id, values_from = y) |>\n  select(ds, BE) |> drop_na()\n\nBE_train_df <- BE_df_wide %>% filter(row_number() <= n() - NF)\nBE_test_df <- tail(BE_df_wide, NF)\nBE_train_df <- BE_train_df |> rename(y = BE) |> mutate(unique_id = \"BE\")\nBE_test_df <- BE_test_df |> rename(y = BE)\n```\n:::\n\n\nNext, I format the data to make it for the `auto.arima()` function which requires that the data be expressed as a `ts()` object. \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\ntrain <- BE_train_df |> select(-unique_id) |>\nmutate(time = 1:length(ds))|> select(-ds)\nelec_ts <- ts(train$y, frequency = 24)\n```\n:::\n\n\nAnd now, the first AIRMA forecast using the `forecast` package.  Notice that the plot title reports that the `forecast::arima()` function fitted an ARIMA(2,1,1)(1,0,1)[24] model to the data. It appears to be a reasonable fit and forecast with some differencing to achieve stationarity and a seasonal component to account for the daily cycle in the data.\na\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\n#forecast_fit <- elec_ts |>\n#forecast::auto.arima() |>\n#forecast(h = NF , level = 95)\n#saveRDS(forecast_fit, \"arima_forecast.rds\")\nforecast_fit<- readRDS(\"arima_forecast.rds\")\n\nplot(forecast_fit, col=\"darkblue\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\nHere, I extract the forecast and set up a data frame to hold the comparative forecasts.\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\narima_fcst_df <- BE_test_df |> \n  mutate(time = ds,\n    BE_actual = y,\n    f_211101 = as.vector(forecast_fit$mean)) |> \n  select(-ds,-y)\nhead(arima_fcst_df,3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 3\n  time                BE_actual f_211101\n  <chr>                   <dbl>    <dbl>\n1 2016-12-30 00:00:00      44.3     46.2\n2 2016-12-30 01:00:00      44.3     45.0\n3 2016-12-30 02:00:00      41.3     44.1\n```\n:::\n:::\n\n\n\n\n### fable\n\nNow, I go through the same process but using the functions from the `fable` package which in many ways is a sophisticated upgrade to the `forecast` package with many helper functions that that encourage an efficient reproducible workflow. I learned quite a bit from the `forecast` package, but I regret that I took so long to discover `fable`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nauto_train <- BE_train_df |> select(-unique_id) |>\nmutate(time = as.POSIXct(ds, format = \"%Y-%m-%d %H:%M:%S\")) |> select(-ds)\n  \nelec_ts_2 <- auto_train |> as_tsibble(index = time) |> fill_gaps(time, .full = start())\n```\n:::\n\n\n\n\nHere is the automatic ARIMA model fit using the `fable` package and the big surprise. `fable` fits an ARIMA(0,1,4)(0,0,2)[24] to the data which looks quite different from the ARIMA(2,1,1)(1,0,1)[24] model that the `forecast` package fit.\n\n::: {.cell}\n\n```{.r .cell-code}\nfable_fit_1 <- elec_ts_2 |> model(\n    arima_fable = ARIMA(y)) |> report()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSeries: y \nModel: ARIMA(0,1,4)(0,0,2)[24] \n\nCoefficients:\n          ma1      ma2      ma3      ma4    sma1    sma2\n      -0.5062  -0.2001  -0.0645  -0.0768  0.5040  0.1312\ns.e.   0.0248   0.0270   0.0275   0.0249  0.0246  0.0227\n\nsigma^2 estimated as 718.5:  log likelihood=-7792.24\nAIC=15598.49   AICc=15598.55   BIC=15636.37\n```\n:::\n:::\n\n\n## A Digression about Notation\n\nSo, why do say that the two models look to be quite different? Well, let's work out the short hand notation for the two models and see what the math looks like.\n\n## ARIMA(2,1,1)(1,0,1)[24] from forecast package\n\nThis notation translates into: \n$$(1−\\phi_1B−\\phi_2B^2)(1−\\Phi_1B^24)(1−B)y_t=(1−\\theta_1B)(1−\\Theta_1B^24)\\varepsilon_t$$\n\nwhich fully expands to:\n\n\n$$ y_t−y_{t−1}−\\phi_1y_{t−1}+\\phi_1y_{t−2}−\\phi_2y_{t−2}+\\phi_2y_{t−3}−\\Phi_1y_{t−24}+\\Phi_1y_{t−25}+\\phi_1\\Phi_1y_{t−25}−\\phi_1\\Phi_1y_{t−26}+\\phi_2\\Phi_1y_{t−26}−\\phi_2\\Phi_1y_{t−27}=\\varepsilon_t−\\theta_1ε_{t−1}−\\Theta_1\\varepsilon_{t−24}+\\theta_1\\Theta_1\\varepsilon_{t−25}$$\n\n## ARIMA(0,1,4)(0,0,2)[24] from fable package\n\nThis notation translates into:\n\n$$(1 - B)y_t = (1 - \\theta_1B - \\theta_2 B^2 - \\theta_3 B^3 - \\theta_4 B^4)(1 - \\Theta_1 B^{24} - \\Theta_2 B^{48})\\varepsilon_t$$\nwhich expands into:\n\n$$y_t−y_{t−1}=\\varepsilon_t−\\theta_1\\varepsilon_{t−1}−\\theta_2\\varepsilon_{t−2}−\\theta_3\\varepsilon_{t−3}−\\theta_4\\varepsilon_{t−4}−\\Theta_1\\varepsilon_{t−24}+\\theta_1\\Theta_1\\varepsilon_{t−25}+\\theta_2\\Theta_1\\varepsilon_{t−26}+\\theta_3\\Theta_1\\varepsilon_{t−27}+\\theta_4\\Theta_1\\varepsilon_{t−28}−\\Theta_2\\varepsilon_{t−48}+ \\theta_1\\Theta_2\\varepsilon_{t−49} +\\theta_2\\Theta_2\\varepsilon_{t−50}+\\theta_3\\Theta_2\\varepsilon_{t−51}+\\theta_4\\Theta_2\\varepsilon_{t−52}$$\nThese difference equations don't look simikar to me, and I have no intuition why they should both be reasonable models for the data. But, let's see how the forecasts compare.\n\nPut the `fable` forecast upper case ARIMA into the data frame.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#| code-fold: true\n#| code-summary: \"Show the code\"\nfable_ARIMA_fcst_1 <- fable_fit_1 |> forecast(h = 24) \narima_fcst_df <- arima_fcst_df |> mutate(F_014002 =  as.vector(fable_ARIMA_fcst_1$.mean) )\nhead(arima_fcst_df,3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 4\n  time                BE_actual f_211101 F_014002\n  <chr>                   <dbl>    <dbl>    <dbl>\n1 2016-12-30 00:00:00      44.3     46.2     47.3\n2 2016-12-30 01:00:00      44.3     45.0     46.0\n3 2016-12-30 02:00:00      41.3     44.1     44.7\n```\n:::\n:::\n\n\nPlot and compare.\n\n\n::: {.cell .preview-image}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\ncompare_fore <- function(file){\n  arima_fcst_long_df <- file %>%\n  pivot_longer(!time, names_to = \"method\", values_to = \"mean\")\n\nq <- arima_fcst_long_df |>\n  ggplot(aes(\n    x = time,\n    y = mean,\n    group = method,\n    color = method\n  )) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +\n  geom_line() +\n  geom_point() +\n  ggtitle(\"Multiple ARIMA Forecasts\")\n\nq\n}\n\ncompare_fore(arima_fcst_df)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\n\n## An Investigation\n\nTo be on the safe side, I thought it was worthwhile checking to see that `fable` also agrees with `forecast` that a ARIMA(2,1,1)(1,0,1)[24] model is also a reasonable fit to the data. I use the `fable` package to fit the ARIMA(0,1,4)(0,0,2)[24] model discovered by the `forecast` package to the data. First, fit the ARIMA(2,1,1)(1,0,1)[24] model to the data.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\nfable_fit_2 <- elec_ts_2  %>%\nas_tsibble() %>%\nmodel(fable_ARIMA_fcst_2 = ARIMA(y ~ 0 + pdq(2, 1, 1) + PDQ(1, 0, 1))) %>%\nreport()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSeries: y \nModel: ARIMA(2,1,1)(1,0,1)[24] \n\nCoefficients:\n         ar1     ar2      ma1    sar1    sma1\n      0.4310  0.0504  -0.9373  0.3290  0.1765\ns.e.  0.0328  0.0302   0.0204  0.0529  0.0560\n\nsigma^2 estimated as 711.8:  log likelihood=-7785.05\nAIC=15582.11   AICc=15582.16   BIC=15614.58\n```\n:::\n:::\n\nAnd then, make the forecast and add it to the plotting data frame.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\nfable_ARIMA_fcst_2 <- fable_fit_2 |> forecast(h = 24)\n\narima_fcst_df <- arima_fcst_df |> mutate(F_211101 =  as.vector(fable_ARIMA_fcst_2$.mean) )\nhead(arima_fcst_df,3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 5\n  time                BE_actual f_211101 F_014002 F_211101\n  <chr>                   <dbl>    <dbl>    <dbl>    <dbl>\n1 2016-12-30 00:00:00      44.3     46.2     47.3     46.1\n2 2016-12-30 01:00:00      44.3     45.0     46.0     44.9\n3 2016-12-30 02:00:00      41.3     44.1     44.7     44.0\n```\n:::\n:::\n\n\nPlotting the forecasts shows that the forecasts from , *arima* , the original ARIMA(2,1,1)(1,0,1)[24] model from the `forecast` package, *ARIMA*, the ARIMA(0,1,4)(0,0,2)[24] model from `fable` and *ARIMA_2*, the ARIMA(2,1,1)(1,0,1)[24] model from `fable` are all more or less on top of each other.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\ncompare_fore(arima_fcst_df)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n## Checking the residuals\n\nThe following plot shows that the residuals of the two `fable` ARIMA forecasts are very highly correlated.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\nfable_fit_2_aug<- fable_fit_2 %>% augment()            # Get fitted values and residuals\nfable_fit_1_aug <- fable_fit_1 %>% augment() \nresid_fit_1 <- fable_fit_1_aug$.innov\nresid_fit_2 <- fable_fit_2_aug$.innov\nr_df <- data_frame(resid_fit_1, resid_fit_2)\nr_df |> ggplot(aes(resid_fit_1,resid_fit_2)) + geom_point(color = \"darkblue\") +\n        ylab(\"ARIMA(2,1,1)(1,0,1)[24]\") +\n        xlab(\"ARIMA(0,1,4)(0,0,2)[24]\") +\n        ggtitle(\".innov tesiduals from two `fable` ARIMA models\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\nLet's look at the residuals for the ARIMA(2,1,1)(1,0,1)[24] model. The residuals look like white noise, but the ACF plot shows a large spike at lag 23 and distribution of the residuals has too sharp of a peak to be normal.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfable_fit_2 |> gg_tsresiduals()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\nNevertheless, the Ljung-Box test for autocorrelation of the residuals also looks good. Under the hypothesis that the residuals came from a white noise process he probability of observing what we did observe would be aroud 0.3 - too high to rejuct the hypothesis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfable_fit_2_aug |> features(.innov, ljung_box, lag = 48)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  .model             lb_stat lb_pvalue\n  <chr>                <dbl>     <dbl>\n1 fable_ARIMA_fcst_2    52.4     0.307\n```\n:::\n:::\n\n\n\n## Is this Really Surprising?\n\nShould I have been surprised by seeing two different auto fit models for the same time series that produce forecasts that are really close to each other? Anyone who has every tried to find a suitable ARIMA model by following the theory in the textbooks: looking at the ACF and PACF functions etc., knows how fragile the process is. Indeed, the experts will tell you that the *identifiability* of ARIMA models is a well-known problem. Consider this note on page 305 from [Brockwell and Davis (1987)](https://link.springer.com/book/10.1007/978-1-4899-0004-3):\n\n*Of course in the modelling of real data there is rarely such a thing as the \"true order\". For the process $X_t =  \\sum_{j=0}^{\\infty} \\psi_jZ_{t-j}$  there may be many polynomials $\\theta(z)$, $\\phi(z)$ such that the coefficients of $z^j$ in $\\theta(z)/\\phi(z)$ closely approximate $\\psi_j$ for moderately small values of j. Correspondingly there may be many ARMA processes with properties similar to {X,}. This problem of identifiability becomes much more serious for multivariate processes.*\n\n\n\n## Variations on a Theme\n\nBecause the two nearly identical solutions to the problem of finding a model that adequately fits the data are essentially linear equations, I imagine that they live somewhere close to each other in a multidimensinal vector space. Are there other solutions nearby? Are there better solutions? Given that I have two solutions, it seems reasonable to assume that minor perturbations of the p,d,q,P,D,Q parameters may turn up additional models with similar AICc and RMSE profiles. Fiddling with the parameters it turned out mostly resulted in numerical errors of one sort or another or inferior solutions. But, I did find a third solution that is at least as good as the others.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\nfable_fit_3 <- elec_ts_2  |>\nas_tsibble() |>\nmodel(F_013002 = ARIMA(y ~ 0 + pdq(0, 1, 3) + PDQ(0, 0, 2))) |>\nreport()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSeries: y \nModel: ARIMA(0,1,3)(0,0,2)[24] \n\nCoefficients:\n          ma1      ma2      ma3    sma1    sma2\n      -0.5037  -0.2156  -0.0968  0.5082  0.1355\ns.e.   0.0246   0.0287   0.0252  0.0247  0.0226\n\nsigma^2 estimated as 722.2:  log likelihood=-7796.99\nAIC=15605.98   AICc=15606.03   BIC=15638.45\n```\n:::\n:::\n\n\nOnce again, we see that all of the ARIMA forecasts sit on top of each other.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\nfable_ARIMA_fcst_3 <- fable_fit_3 |> forecast(h = 24)\narima_fcst_df <- arima_fcst_df |> mutate(F_013002 =  as.vector(fable_ARIMA_fcst_3$.mean) )\ncompare_fore(arima_fcst_df)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n## Are there better solutions?\n\nFor a final try to find a better forecast I used the search feature of the fable::ARIMA function to systematically search through the model space constrained by p + q + P + Q <= 6 p + q + P + Q <= 6 & (constant + d + D <= 2)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfable_fit_4 <- elec_ts_2 |>\n  model(\n    arima_fable = ARIMA(y, stepwise=FALSE)\n  ) |> report()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSeries: y \nModel: ARIMA(2,1,1)(0,0,2)[24] \n\nCoefficients:\n         ar1     ar2      ma1    sma1    sma2\n      0.4463  0.0586  -0.9487  0.5019  0.1258\ns.e.  0.0321  0.0304   0.0197  0.0244  0.0228\n\nsigma^2 estimated as 714.6:  log likelihood=-7788.26\nAIC=15588.53   AICc=15588.58   BIC=15621\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#| code-fold: true\n#| code-summary: \"Show the code\"\nfable_fit_4_fcst <- fable_fit_4 |> forecast(h = 24)\narima_fcst_df <- arima_fcst_df |> mutate(F_211002 =  as.vector(fable_fit_4_fcst$.mean) )\nhead(arima_fcst_df,3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 7\n  time                BE_actual f_211101 F_014002 F_211101 F_013002 F_211002\n  <chr>                   <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>\n1 2016-12-30 00:00:00      44.3     46.2     47.3     46.1     47.1     47.0\n2 2016-12-30 01:00:00      44.3     45.0     46.0     44.9     45.7     45.6\n3 2016-12-30 02:00:00      41.3     44.1     44.7     44.0     44.9     44.6\n```\n:::\n:::\n\n\n\nPlot and compare.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\ncompare_fore(arima_fcst_df)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\n\nVisually, the forecasts for all of the models look very similar. Lets confirm this by checking the AICc and RMSE values one last time. The are indeed very close according to both metrics. And, as it turns out, the model discovered by the forecast package has the smallest AICc value and the smallest RMSE. \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nc_df <- data.frame(c(\"f_211101\", \"F_013002\", \"F_014002\", \"F_211002\", \"F_211101\"))\nnames(c_df) <- c(\"Model\")\n\nrmse <- c(rmse(arima_fcst_df$BE_actual,arima_fcst_df$f_211101),\n  rmse(arima_fcst_df$BE_actual,arima_fcst_df$F_013002),\n  rmse(arima_fcst_df$BE_actual,arima_fcst_df$F_014002),\n  rmse(arima_fcst_df$BE_actual,arima_fcst_df$F_211002),\n  rmse(arima_fcst_df$BE_actual,arima_fcst_df$F_211101))\n\nAICc <- c(forecast_fit$model$aicc,\n                        as.numeric(glance(fable_fit_1)[\"AICc\"]), \n                        as.numeric(glance(fable_fit_2)[\"AICc\"]), \n                        as.numeric(glance(fable_fit_3)[\"AICc\"]), \n                        as.numeric(glance(fable_fit_4)[\"AICc\"]))\nc_df <- cbind(c_df, rmse, AICc)\nc_df |> arrange(AICc)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     Model     rmse     AICc\n1 f_211101 4.966623 15581.13\n2 F_014002 5.041250 15582.16\n3 F_211101 4.954435 15588.58\n4 F_013002 4.952944 15598.55\n5 F_211002 5.188436 15606.03\n```\n:::\n:::\n\n\n\n\n\n\n\n\n\n## Summary\n\nI have three different models for the BE Electricity Usage time series. Each provides a reasonably good fit based on comparing the RMSE of its respective forecasts with the actual data of the hold-out test data. Because of they way they were constructed it seems reasonable to assume that these solutions are somehow \"close\" to each other in the vector space containing the stochastic difference equations that constitute the models. A resonable question is: are there better models that live somewhere else in the vector space? Answering this means going back to fundamentals and searching for structure that may still be in the residuals. I will leave that for another time.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}