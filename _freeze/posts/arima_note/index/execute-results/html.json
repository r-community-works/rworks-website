{
  "hash": "95338200777694edbd0ff8e3b39eb3b0",
  "result": {
    "markdown": "\n\ntitle: \"ARIMA NOTE\"\nauthor: \"Joseph Rickert\"\ndate: 2025-02-17\ncode-fold: true\ncode-summary: \"Show the code\"\ndescription: \".\"\ncategories: \"\"\neditor: source\n\n---\n\nIn my previous post [A First Look at TimeGPT using nixtlar](https://rworks.dev/posts/revised_TimeGPT/), I used the `auto.arima()` function from the `forecast` package to fit an ARIMA model to a time series of electricity usage data in order to compare and ARIMA forecast with the `TimeGPT` forecast. While working out the bugs in that post, I also fit an automatic ARIMA model using the newer and improved `fable` package and was very surprised by the results. In this post I will show what surprised me, work through my investigation and and with an interesting question.\n\nHere are the necessary libraries and the data that we will be working with.\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\nlibrary(tidyverse)\nlibrary(forecast)\nlibrary(fable)\nlibrary(tsibble)\nlibrary(nixtlar)\n```\n:::\n\nAs in the `TimeGPT` post, I will use the BE electricity usage data set from the `nixtlar` package for fitting mmodels and makig forecasts. Let's take a look at the data.\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\ndf <- nixtlar::electricity\n#glimpse(df)\n\ndf2 <- df |> mutate(time = as.POSIXct(ds, format = \"%Y-%m-%d %H:%M:%S\")) |> \n             filter(unique_id == \"BE\") |> select(-unique_id, -ds)\n  \n\np <- df2 |> ggplot(aes(x = time, y = y)) +\n  geom_line(color='darkblue') +\n  ggtitle(\" BE Electricity Usage Data\")\n\np\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\nThis next block of code splits the data into training and test data with the last 24 observations from the BE data set being held out for forecasting.\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\nNF <- 24\n\nBE_df_wide <- df |> pivot_wider(names_from = unique_id, values_from = y) |>\n  select(ds, BE) |> drop_na()\n\nBE_train_df <- BE_df_wide %>% filter(row_number() <= n() - NF)\nBE_test_df <- tail(BE_df_wide, NF)\nBE_train_df <- BE_train_df |> rename(y = BE) |> mutate(unique_id = \"BE\")\nBE_test_df <- BE_test_df |> rename(y = BE)\n```\n:::\n\nThis section of code reformats the training data into a format that is suitable for the `auto.arima()` which requires that the data be expressed as a `ts()` object. \n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\ntrain <- BE_train_df |> select(-unique_id) |>\nmutate(time = 1:length(ds))|> select(-ds)\nelec_ts <- ts(train$y, frequency = 24)\n```\n:::\n\nAnd now, the first AIRMA forecast using the `forecast` package.  Notice that the plot title reports that the `forecast::arime()` function fitted an ARIMA(2,1,1)(1,0,1)[24] model to the data.\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\n#arima <- elec_ts |>\n#  forecast::auto.arima() |>\n#  forecast(h = NF , level = 95)\n#saveRDS(arima, \"arima_forecast.rds\")\narima<- readRDS(\"arima_forecast.rds\")\n\nplot(arima, col=\"darkblue\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nHere, I extract the forecast and set up a data frame to hold the comparative forecasts.\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\narima_fcst_df <- BE_test_df |> \n  mutate(time = ds,\n    BE_actual = y,\n    arima = as.vector(arima$mean)) |> \n  select(-ds,-y)\nhead(arima_fcst_df,3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 3\n  time                BE_actual arima\n  <chr>                   <dbl> <dbl>\n1 2016-12-30 00:00:00      44.3  46.2\n2 2016-12-30 01:00:00      44.3  45.0\n3 2016-12-30 02:00:00      41.3  44.1\n```\n:::\n:::\n\n\n\n\n\n### fable\n\nNow, I go throughthe same process but using the functions from the `fable` package.\n\n::: {.cell}\n\n```{.r .cell-code}\nauto_train <- BE_train_df |> select(-unique_id) |>\nmutate(time = as.POSIXct(ds, format = \"%Y-%m-%d %H:%M:%S\")) |> select(-ds)\n  \nelec_ts_2 <- auto_train |> as_tsibble(index = time) |> fill_gaps(time, .full = start())\n```\n:::\n\n\n\nHere is the automatic ARIMA model fit using the `fable` package and the big surprise. `fable` fit an ARIMA(0,1,4)(0,0,2)[24] to the data which looks quite different from the ARIMA(2,1,1)(1,0,1)[24] model that the `forecast` package fit.\n::: {.cell}\n\n```{.r .cell-code}\nfit <- elec_ts_2 %>%\n  model(\n    arima_fable = ARIMA(y)\n  ) \nfable_ARIMA_fcst <- fit |> forecast(h = 24)\nfit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A mable: 1 x 1\n                arima_fable\n                    <model>\n1 <ARIMA(0,1,4)(0,0,2)[24]>\n```\n:::\n:::\n\n## A Digression about Notation\n\nSo, why do I assert that the two model look to be quite different? Well, let's work out the short hand notation for the two models and see what the math looks like.\n\n## ARIMA(2,1,1)(1,0,1)[24] from forecast package\n\nThis notation translates into: \n$$(1−\\phi_1B−\\phi_2B^2)(1−\\Phi_1B^24)(1−B)y_t=(1−\\theta_1B)(1−\\Theta_1B^24)\\varepsilon_t$$\n\nwhich fully expands to:\n\n\n$$ y_t−y_{t−1}−\\phi_1y_{t−1}+\\phi_1y_{t−2}−\\phi_2y_{t−2}+\\phi_2y_{t−3}−\\Phi_1y_{t−24}+\\Phi_1y_{t−25}+\\phi_1\\Phi_1y_{t−25}−\\phi_1\\Phi_1y_{t−26}+\\phi_2\\Phi_1y_{t−26}−\\phi_2\\Phi_1y_{t−27}=\\varepsilon_t−\\theta_1ε_{t−1}−\\Theta_1\\varepsilon_{t−24}+\\theta_1\\Theta_1\\varepsilon_{t−25}$$\n\n## ARIMA(0,1,4)(0,0,2)[24] from fable package\n\nThis notation translates into:\n\n$$(1 - B)y_t = (1 - \\theta_1B - \\theta_2 B^2 - \\theta_3 B^3 - \\theta_4 B^4)(1 - \\Theta_1 B^{24} - \\Theta_2 B^{48})\\varepsilon_t$$\nwhich expands into:\n\n$$y_t−y_{t−1}=\\varepsilon_t−\\theta_1\\varepsilon_{t−1}−\\theta_2\\varepsilon_{t−2}−\\theta_3\\varepsilon_{t−3}−\\theta_4\\varepsilon_{t−4}−\\Theta_1\\varepsilon_{t−24}+\\theta_1\\Theta_1\\varepsilon_{t−25}+\\theta_2\\Theta_1\\varepsilon_{t−26}+\\theta_3\\Theta_1\\varepsilon_{t−27}+\\theta_4\\Theta_1\\varepsilon_{t−28}−\\Theta_2\\varepsilon_{t−48}+ \\theta_1\\Theta_2\\varepsilon_{t−49} +\\theta_2\\Theta_2\\varepsilon_{t−50}+\\theta_3\\Theta_2\\varepsilon_{t−51}+\\theta_4\\Theta_2\\varepsilon_{t−52}$$\nThese mathematical models don't look anything alike and I have no intuition why they should both be reasonable models for the data. But, let's see how the forecasts compare\n\nPut the `fable` forecast upper case ARIMA into the data frame.\n\n::: {.cell}\n\n```{.r .cell-code}\n#| code-fold: true\n#| code-summary: \"Show the code\"\n\narima_fcst_df <- arima_fcst_df |> mutate(ARIMA =  as.vector(fable_ARIMA_fcst$.mean) )\nhead(arima_fcst_df,3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 4\n  time                BE_actual arima ARIMA\n  <chr>                   <dbl> <dbl> <dbl>\n1 2016-12-30 00:00:00      44.3  46.2  47.3\n2 2016-12-30 01:00:00      44.3  45.0  46.0\n3 2016-12-30 02:00:00      41.3  44.1  44.7\n```\n:::\n:::\n\nPlot and compare.\n\n::: {.cell .preview-image}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\ncompare_fore <- function(file){\n  arima_fcst_long_df <- file %>%\n  pivot_longer(!time, names_to = \"method\", values_to = \"mean\")\n\nq <- arima_fcst_long_df |>\n  ggplot(aes(\n    x = time,\n    y = mean,\n    group = method,\n    color = method\n  )) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +\n  geom_line() +\n  geom_point() +\n  ggtitle(\"forecast vs. fable auto ARIMA forecasts\")\n\nq\n}\n\ncompare_fore(arima_fcst_df)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\nThe two forecasts are almost on top of each other and running this code shows that the RMSE values from the actual data are also very close.\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\nRMSE <-  function(m, o){sqrt(mean((m - o)^2))}\nrms_names <- c(\"arima\", \"ARIMA\")\nrms_fcst <- array(NA_real_,\n                          dim = 2,\n                          dimnames = list(rms_names))\nrms_fcst[1] <- RMSE(arima_fcst_df$arima, arima_fcst_df$BE_actual)\nrms_fcst[2] <- RMSE(arima_fcst_df$ARIMA, arima_fcst_df$BE_actual)\n\n\nrms_fcst\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   arima    ARIMA \n4.966623 5.041250 \n```\n:::\n:::\n\n## An Investigation\n\nFirst let's check that `fable` agrees with `forecast` with respect to the ARIMA(2,1,1)(1,0,1)[24] model. I use the `fable` package to fit the ARIMA(0,1,4)(0,0,2)[24] model discovered by the `forecast` package to the data.\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_2 <- elec_ts_2  %>%\nas_tsibble() %>%\nmodel(ARIMA_2 = ARIMA(y ~ 0 + pdq(2, 1, 1) + PDQ(1, 0, 1))) %>%\nreport()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSeries: y \nModel: ARIMA(2,1,1)(1,0,1)[24] \n\nCoefficients:\n         ar1     ar2      ma1    sar1    sma1\n      0.4310  0.0504  -0.9373  0.3290  0.1765\ns.e.  0.0328  0.0302   0.0204  0.0529  0.0560\n\nsigma^2 estimated as 711.8:  log likelihood=-7785.05\nAIC=15582.11   AICc=15582.16   BIC=15614.58\n```\n:::\n:::\n \nMake the forecast and add it to the plotting data frame.\n\n::: {.cell}\n\n```{.r .cell-code}\nfable_ARIMA_fcst_2 <- fit_2 |> forecast(h = 24)\n\narima_fcst_df <- arima_fcst_df |> mutate(ARIMA_2 =  as.vector(fable_ARIMA_fcst_2$.mean) )\nhead(arima_fcst_df,3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 5\n  time                BE_actual arima ARIMA ARIMA_2\n  <chr>                   <dbl> <dbl> <dbl>   <dbl>\n1 2016-12-30 00:00:00      44.3  46.2  47.3    46.1\n2 2016-12-30 01:00:00      44.3  45.0  46.0    44.9\n3 2016-12-30 02:00:00      41.3  44.1  44.7    44.0\n```\n:::\n:::\n\nPlotting the forecasts shows that the forecasts from , *arima* , the original ARIMA(2,1,1)(1,0,1)[24] model from the `forecast` package, *ARIMA*, the ARIMA(0,1,4)(0,0,2)[24] model from `fable` and *ARIMA_2*, the ARIMA(2,1,1)(1,0,1)[24] model from `fable` are all more or less on top of each other.\n\n::: {.cell}\n\n```{.r .cell-code}\ncompare_fore(arima_fcst_df)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_2_aug<- fit_2 %>% augment()            # Get fitted values and residuals\nfit_aug <- fit %>% augment() \nARIMA <- fit_aug$.resid\nARIMA_2 <- fit_2_aug$.resid\nr_df <- data_frame(ARIMA, ARIMA_2)\nr_df |> ggplot(aes(ARIMA,ARIMA_2)) + geom_point(color = \"darkblue\") +\n        ggtitle(\"residuals ARIMA(2,1,1)(1,0,1)[24] vs ARIMA(0,1,4)(0,0,2)[24]\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\nAnd, we see that the *arima* and *ARIMA_2* RMSE errors are very close.\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nRMSE <-  function(m, o){sqrt(mean((m - o)^2))}\nrms_names <- c(\"arima\", \"ARIMA\", \"ARIMA_2\")\nrms_fcst <- array(NA_real_,\n                          dim = 3,\n                          dimnames = list(rms_names))\nrms_fcst[1] <- RMSE(arima_fcst_df$arima, arima_fcst_df$BE_actual)\nrms_fcst[2] <- RMSE(arima_fcst_df$ARIMA, arima_fcst_df$BE_actual)\nrms_fcst[3] <- RMSE(arima_fcst_df$ARIMA_2, arima_fcst_df$BE_actual)\n\nrms_fcst\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   arima    ARIMA  ARIMA_2 \n4.966623 5.041250 4.954435 \n```\n:::\n:::\n\n## Is this Really a Surprise?\n\nShould I have been surprised by seeing two different auto fit models for the same time series that produce forecasts that are really close to each other? Anyone who has every tried to find a suitable ARIMA model by followint the theory in the textbooks: looking at the ACF and PACF functions etc., knows how fragile the process is. Indeed, the experts will tell you that the *identifiability* of ARIMA models is a well-known problem. Consider this note on page 305 from [Brockwell and Davis (1987)](https://link.springer.com/book/10.1007/978-1-4899-0004-3):\n\n*Of course in the modelling of real data there is rarely such a thing as the \"true order\". For the process $X_t =  \\sum_{j=0}^{\\infty} \\psi_jZ_{t-j}$  there may be many polynomials $\\theta(z)$, $\\phi(z)$ such that the coefficients of $z^j$ in $\\theta(z)/\\phi(z)$ closely approximate $\\psi_j$ for moderately small values of j. Correspondingly there may be many ARMA processes with properties similar to {X,}. This problem of identifiability becomes much more serious for multivariate processes.*\n\nBecause the two nearly identical solutions to the problem of finding a model that adequately fits the data are essentially linear equations, I imagine that they live somewhere close to each other in a multidimensinal vector space. Are there other solutions nearby? Are there better solutions? Given that I have two solutions, it seemed reasonable that naive fiddling with the parameters might work. And although most combinations of the p,d,q,P,D,Q parameters predictably resulted in numerical errors of one sort or another, I did find did find another solution. \n\n# COMBINATION of TWO MODELS\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_aA <- elec_ts_2  %>%\nas_tsibble() %>%\nmodel(aA = ARIMA(y ~ 0 + pdq(0, 1, 3) + PDQ(0, 0, 2))) %>% \nreport()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSeries: y \nModel: ARIMA(0,1,3)(0,0,2)[24] \n\nCoefficients:\n          ma1      ma2      ma3    sma1    sma2\n      -0.5037  -0.2156  -0.0968  0.5082  0.1355\ns.e.   0.0246   0.0287   0.0252  0.0247  0.0226\n\nsigma^2 estimated as 722.2:  log likelihood=-7796.99\nAIC=15605.98   AICc=15606.03   BIC=15638.45\n```\n:::\n:::\n::: {.cell}\n\n```{.r .cell-code}\nfable_ARIMA_aA <- fit_aA |> forecast(h = 24)\n\narima_fcst_df <- arima_fcst_df |> mutate(ARIMA_aA =  as.vector(fable_ARIMA_aA$.mean) )\n\ncompare_fore(arima_fcst_df)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\nIf you care about the third decimal place in the RMSE calculation, you might consider it to be the best solution so far.\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nRMSE <-  function(m, o){sqrt(mean((m - o)^2))}\nrms_names <- c(\"arima\", \"ARIMA\", \"ARIMA_2\",\"ARIMA_aA\")\nrms_fcst <- array(NA_real_,\n                          dim = 4,\n                          dimnames = list(rms_names))\nrms_fcst[1] <- RMSE(arima_fcst_df$arima, arima_fcst_df$BE_actual)\nrms_fcst[2] <- RMSE(arima_fcst_df$ARIMA, arima_fcst_df$BE_actual)\nrms_fcst[3] <- RMSE(arima_fcst_df$ARIMA_2, arima_fcst_df$BE_actual)\nrms_fcst[4] <- RMSE(arima_fcst_df$ARIMA_aA, arima_fcst_df$BE_actual)\nrms_fcst\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   arima    ARIMA  ARIMA_2 ARIMA_aA \n4.966623 5.041250 4.954435 4.952944 \n```\n:::\n:::\n\n## Question\n\nAll of this leads to a an interesting question: *Given a time series with enough observations to hold out some data on its tail for predictions, a model fit by the auto ARIMA function, and an epsilon: is there a systematic way based on some theory to search for an additional ARIMA model that fits the data with a least squares error from the actual hold out values, that is with epsilon of the auto-ARIMA model?*\n\nWe at R Works would be very happy to publish a post that might illuminate this question.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}