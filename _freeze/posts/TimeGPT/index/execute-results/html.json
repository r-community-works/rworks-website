{
  "hash": "2bf6e59907a08c840e7a6bd430ad7faf",
  "result": {
    "markdown": "---\ntitle: \"A First Look at TimeGPT using nixtlar\"\nauthor: \"Joseph Rickert\"\ndate: 2024-12-13\ndescription: \"\"\nimage: \"\"\nimage-alt: \"\"\ncategories: \"\"\neditor: source\n---\n\n\nThis post is a first look at [Nixtla's](https://docs.nixtla.io/) `TimeGPT` generative, pre-trained transformer for time series forecasting using the `nixtlar` R package.\n\nAs described in [Garza et al. (2021)](https://arxiv.org/abs/2111.04052), TimeGPT is a Transformer-based time series model with self-attention mechanisms. The architecture comprises an encoder-decoder structure with multiple layers, each with residual connections and layer normalization. The encoder, a stack of multi-head self-attention layers followed by a feed-forward neural network, processes the input time series. The decoder, which is similar to the encoder, generates the forecast. The decoder includes an additional multi-head attention layer that takes the encoder’s output as input. The model is trained using a teacher-forcing strategy, where the decoder receives the ground-truth values during training. The model is then used for forecasting by feeding the model’s predictions back as input during inference.\n\n![](timegpt.png){fig-alt=\"TimeGPT architecture\"}\n\n\nNixtla's website provides a considerable amount of explanatory material, documentation, and code examples in Python. The [`nixlar`](https://cran.r-project.org/package=nixtlar) package wraps the Python code to provide an R interface. The package documentation for version 0.6.2 doesn't fully the R functions, but the vignettes provide sufficient code examples to get started.\n\n*Before getting started with TimeGPT you will have to register for an API key. The process is easy enough and is described in this [vignette](https://cran.r-project.org/web/packages/nixtlar/vignettes/setting-up-your-api-key.html).*\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(forecast)\nlibrary(xts)\nlibrary(prophet)\nlibrary(nixtlar)\n```\n:::\n\n\n\n\n## The Data\n\nThe electricity dataset included in the `nixtlar` package contains hourly observations of  electricity consumption generated sourced from the [PJM Interconnection LLC](https://www.pjm.com/), a regional transmission organization that is part of the Eastern Interconnection grid in the United States. There are five different time series with data taken from 2012 to 2018. A look at the data frame shows that the various series do not cover the same time periods.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- nixtlar::electricity\ndf_wide <- df |> \n  pivot_wider(names_from = unique_id, values_from = y) \nhead(df_wide)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 6\n  ds                     BE    DE    FR    NP   PJM\n  <chr>               <dbl> <dbl> <dbl> <dbl> <dbl>\n1 2016-10-22 00:00:00  70      NA  54.7    NA    NA\n2 2016-10-22 01:00:00  37.1    NA  51.2    NA    NA\n3 2016-10-22 02:00:00  37.1    NA  48.9    NA    NA\n4 2016-10-22 03:00:00  44.8    NA  45.9    NA    NA\n5 2016-10-22 04:00:00  37.1    NA  41.2    NA    NA\n6 2016-10-22 05:00:00  35.6    NA  41.4    NA    NA\n```\n:::\n:::\n\n\nPlots indicate that all of the series show periods of considerable volatility. The BE, DE, and FR series appear to be stationary. NP trends upward to the right and the PJM series appears to be nonlinear.\n\n::: {.cell}\n\n```{.r .cell-code}\ndf2 <- df |> mutate( time = as.POSIXct(ds, format = \"%Y-%m-%d %H:%M:%S\")) |> \n                     group_by(unique_id)\np <- df2 |> ggplot(aes(x = time, y = y, color = unique_id)) +\n           geom_line() + facet_wrap(~unique_id, scales = \"free\")\np\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n\n## TimeGPT Forecsts\n\nI'll begin by showing off the `nixtlar` forecasting function which can handle multiple time series by forecasting eight hours ahead using all of the data. The parameter `h` specifies the number of steps ahead to forecast, and `level` specifies parameter that the confidence level for the forecast.\n\nHere is the built-in `nixtlar` plot function\n\n::: {.cell}\n\n```{.r .cell-code}\n#nixtla_client_plot(df, nixtla_client_fcst, max_insample_length = 200)\n```\n:::\n\n\n\n![](fcst.png){fig-alt=\"\"}\n\nThis plot uses`ggplot2`to focus in on the forecasts.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# nixtla_client_fcst <- nixtla_client_forecast(df, h = 8, level = c(80,95))\n# saveRDS(nixtla_client_fcst, \"nixtla_client_fcst.rds\")\nnixtla_client_fcst <- readRDS(\"nixtla_client_fcst.rds\")\n\nncf_df <-  nixtla_client_fcst |> mutate( time = as.POSIXct(ds, format = \"%Y-%m-%d %H:%M:%S\")) |> group_by(unique_id)\nnames(ncf_df) <- c(\"unique_id\", \"ds\", \"TimeGPT\",  \"lon\", \"loe\", \"hie\", \"hin\")\npf <- ncf_df |> ggplot(aes(x = ds, y = TimeGPT, color = unique_id)) +\n           geom_line() +\n           geom_ribbon(aes(ymin=lon, ymax=hin), linetype=2, alpha=0.1) +\n           facet_wrap(~unique_id, scales = \"free\") \npf\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\nFor the rest of this post I'll work only with the BE data and do some simple back testing. I will split the data into training set and a test set containing 24 hours worth of observations. Then I'll fit established time series forecasting models and compare how well they do vis-a-vis the actual data and with each other. Note, I will not attempt any tuning of these models. This will make it a fair, \"out-of-the-box\" comparison.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nNF <- 24\nBE_df_wide <- df |> pivot_wider(names_from = unique_id, values_from = y) |>\n                    select(ds,BE) |> drop_na()\nBE_train_df <- BE_df_wide %>% filter(row_number() <= n()-NF)\nBE_test_df <- tail(BE_df_wide,NF)\nBE_train_df <- BE_train_df |> rename(y = BE) |> mutate(unique_id = \"BE\")\nBE_test_df <- BE_test_df |> rename(y = BE)\n```\n:::\n\n\n\nThe `nixtla_client_forecast()` function is the main `nixtlar` forecasting function. (I have already run this function and saved the results RDS file in order not to make an API call every time the code is run during the blog building process.) \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# nixtla_fcst <- nixtla_client_forecast(BE_train_df, h = NF, level = 95)\n# saveRDS(nixtla_fcst, \"nixtla_fcst.rds\")\nnixtla_fcst <- readRDS(\"nixtla_fcst.rds\")\nnames(nixtla_fcst) <- c(\"unique_id\", \"ds\", \"TimeGPT\", \"lo95\", \"up95\")\n```\n:::\n\n\nHere, I create a data frame to hold the actual and forecast values\n\n::: {.cell}\n\n```{.r .cell-code}\nfcst_df <- tail(nixtla_fcst,NF) |> select(ds,TimeGPT) |> \n           rename(time = ds, tgpt_fcst = TimeGPT) |>\n           mutate(elec_actual = BE_test_df$y)\nhead(fcst_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 time tgpt_fcst elec_actual\n1 2016-12-30 00:00:00  38.82010       44.30\n2 2016-12-30 01:00:00  36.29234       44.30\n3 2016-12-30 02:00:00  34.97838       41.26\n4 2016-12-30 03:00:00  32.99565       40.62\n5 2016-12-30 04:00:00  31.58322       40.07\n6 2016-12-30 05:00:00  33.27422       41.02\n```\n:::\n:::\n\n\n## Some Comparative Forecasts\n\n\n### ARIMA Forecast with `auto.arima()`\n\nThe `auto.arima()` function form the `forecast` package fits an ARIMA(2,1,1) model. This means two autoregressive terms, one difference and one moving average term.\n\n::: {.cell}\n\n```{.r .cell-code}\narima_train <- BE_train_df |> select(-unique_id) |>\n               mutate( time = as.POSIXct(ds, format = \"%Y-%m-%d %H:%M:%S\"))\narima_train <- arima_train |> select(-ds)\n\nelec_ts <- as.xts(arima_train)\n\narima_fcst <- elec_ts |> \n  auto.arima() |>\n  # number of periods to forecast\n  forecast(h =NF , level = 95)\n```\n:::\n\n\n\n### Exponential smoothing forecast with `ets()`\n\nBecause I have provided no guidance, the `ets()` function from the `forecast` package fits an ETS(A,A,N) model with an additive error, an additive trend and no seasonality. All parameters are estimated from the data.\n\n::: {.cell}\n\n```{.r .cell-code}\nets_fcst <- elec_ts |> \n  ets() |>\n  # number of periods to forecast\n  forecast(h = NF)\n```\n:::\n\n\n\n### Prophet Forecast\n\nI also ask the `prophet()` function from the `prophet` package for an automatic fit using the default parameters. Among other things this means a linear growth curve additive seasonality and automatic estimates for daily seasonality. As above, the model is fit to the data in the `BE_train_df` data frame, but here the `make_future_dataframe()` function creates a data frame with the same structure as `BE_train_df` but with the `ds` column extended by `NF` periods.\n\n::: {.cell}\n\n```{.r .cell-code}\nprophet_fit <- prophet(BE_train_df)\nfuture <- make_future_dataframe(prophet_fit, periods = NF,  freq = 3600, include_history = FALSE)\nprophet_fcst <- predict(prophet_fit, future)\n```\n:::\n\n\n\n## Results and Discussion\n\nBefore plotting, Let's have a look at the wide data frame that holds the forecasts.\n\n::: {.cell}\n\n```{.r .cell-code}\nfcst_df2 <- fcst_df |>\n             mutate(arima_fcst = as.vector(arima_fcst$mean),\n                    ets_fcst = as.vector(ets_fcst$mean),\n                    prophet_fcst = prophet_fcst$yhat)\nhead(fcst_df2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 time tgpt_fcst elec_actual arima_fcst ets_fcst prophet_fcst\n1 2016-12-30 00:00:00  38.82010       44.30   46.60606 43.10791     32.05748\n2 2016-12-30 01:00:00  36.29234       44.30   47.83980 42.33006     29.74375\n3 2016-12-30 02:00:00  34.97838       41.26   48.52490 41.70778     22.66588\n4 2016-12-30 03:00:00  32.99565       40.62   48.90180 41.20996     15.85864\n5 2016-12-30 04:00:00  31.58322       40.07   49.10927 40.81171     15.62841\n6 2016-12-30 05:00:00  33.27422       41.02   49.22347 40.49310     23.54824\n```\n:::\n:::\n\n\nThen, shape the data into long format and plot.\n\n::: {.cell}\n\n```{.r .cell-code}\nfcst_dft2_long <- fcst_df2 %>%\n  pivot_longer(!time, names_to = \"method\", values_to = \"mean\")\n\nq <- fcst_dft2_long |> ggplot(aes(x = time, y = mean, group = method, color = method)) +\n                        geom_line() +\n                        geom_point() +\n                        ggtitle(\"TimeGPT vs ARIMA vs ETS vs Prophet vs actual data\") \nq\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nI don't think that there are any big surprises here. Because the Nixtla folks chose the electricity data set to show off their transformer, I was expecting a pretty good fit. However, it is curious that except for one point the TimeGPT forecast is lower than the actual data. It is also interesting that the forecasts that are farther out are a better match to the actual data.\n\nThe \"no thought\" prophet model does a pretty good job, but it does seem to have overacted to the downward trends at the beginning and end of the forecast period. My guess is that with a little thought prophet could do better. \n\nThe ARIMA model is a bit of a disappointment. I would have expected it to follow some of the twists and turns. It is as if the exponential smoothing and ARIMA models flipped a coin at the beginning of the forecast to decide who takes the high road and who gets the low road, but then tracked each other all the way through. The exponential smoothing forecast, on the other hand, is astonishing. It tracks the actual data closely for six forecast points! Are the gods of randomness trolling me again or did time series forecasting really [peak in 1956](https://www.industrydocuments.ucsf.edu/tobacco/docs/#id=jzlc0130)?\n\n\n## Some Final Thoughts\n\nIt is clear that TimeGPT model has upped the game for black-box time series forecasting. It is sure to become a powerful tool for doing exploratory work with large time series and comparing and contrasting multiple time series, and may become the goto baseline forecasting tool for a wide range of time series. Moreover, I expect that time series experts who can fine tune prophet and more traditional time series models will be able to develop some intuition about what TimeGPT is doing by assessing its behavior in relation to these models. \n\nI am aware that this little post may have raised more questions than it answered. If so, please try your hand at elaborating some of the issues raised. We would be very happy to consider your time series posts for publication on R Works.\n\nFinally, for a more sophisticated analysis of these series that deals with their multiseasonality aspects see the [Electricity Load Forecast Tutorial](https://nixtlaverse.nixtla.io/statsforecast/docs/tutorials/electricityloadforecasting.html). And, for some ideas about how to harness \"ordinary\" LLMs for time series forecasting have a look at the second half of the talk that Bryan Lewis gave to [nyhackr](https://bit.ly/4aWxF6P) in April 2024. \n\n\n\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}